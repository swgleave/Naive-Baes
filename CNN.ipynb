{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.fasttext import FastText\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Read In "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finnum/train.csv')\n",
    "x = df1.index\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df = pd.concat([df1,df2], ignore_index = True)\n",
    "df['cat_num'] = df['category'].astype('category').cat.codes\n",
    "labels = pd.get_dummies(df['cat_num'][x])\n",
    "labels2 = pd.get_dummies(df['cat_num'][len(x):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInp = np.load('trainChar.npy')\n",
    "valInp = np.load('valChar.npy')\n",
    "embed = np.load('embedChar.npy')\n",
    "embed = embed.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (trainInp, labels)\n",
    "val = (valInp, labels2)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create validation Dataset and batch it\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val)\n",
    "val_data = val_data.shuffle(10000) # if you want to shuffle your data\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "val_init = iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 547, 8) dtype=float64>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 547, 8, 1) dtype=float64>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [2,3,4,5]\n",
    "embedding_size = embed.shape[1]\n",
    "num_filters = 64\n",
    "max_length = 547\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W,tf.float64),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, tf.cast(b,tf.float64)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu,\n",
    "        ksize=[1, max_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pool:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_1:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_2:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_3:0' shape=(?, 1, 1, 64) dtype=float64>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 256) dtype=float64>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tf.layers.dense(combined_flat, 100, activation = 'relu')\n",
    "conn2 = tf.layers.dense(conn, len(set(df.cat_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Relu:0' shape=(?, 100) dtype=float64>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = conn2)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(conn2)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.3719082586771505\n",
      "Accuracy 0: 0.45714285714285713\n",
      "Average val loss epoch 0: 1.1281594136747493\n",
      "Val_Accuracy 0: 0.5917553191489362\n",
      "Average loss epoch 1: 0.990506347899332\n",
      "Accuracy 1: 0.6572916666666667\n",
      "Average val loss epoch 1: 1.0263348339284404\n",
      "Val_Accuracy 1: 0.6170212765957447\n",
      "Average loss epoch 2: 0.8469634291120312\n",
      "Accuracy 2: 0.7145833333333333\n",
      "Average val loss epoch 2: 0.8511234732981438\n",
      "Val_Accuracy 2: 0.7180851063829787\n",
      "Average loss epoch 3: 0.7615854822863075\n",
      "Accuracy 3: 0.7397321428571428\n",
      "Average val loss epoch 3: 0.8627141441655367\n",
      "Val_Accuracy 3: 0.6901595744680851\n",
      "Average loss epoch 4: 0.6878063878033591\n",
      "Accuracy 4: 0.762797619047619\n",
      "Average val loss epoch 4: 0.7816882893602137\n",
      "Val_Accuracy 4: 0.7154255319148937\n",
      "Average loss epoch 5: 0.6424694950895987\n",
      "Accuracy 5: 0.7732142857142857\n",
      "Average val loss epoch 5: 0.7199134539851327\n",
      "Val_Accuracy 5: 0.7433510638297872\n",
      "Average loss epoch 6: 0.5901797837091161\n",
      "Accuracy 6: 0.7950892857142857\n",
      "Average val loss epoch 6: 0.7252516075951174\n",
      "Val_Accuracy 6: 0.7393617021276596\n",
      "Average loss epoch 7: 0.5438827317384208\n",
      "Accuracy 7: 0.8083333333333333\n",
      "Average val loss epoch 7: 0.7345894874813593\n",
      "Val_Accuracy 7: 0.7539893617021277\n",
      "Average loss epoch 8: 0.4975503288105715\n",
      "Accuracy 8: 0.8239583333333333\n",
      "Average val loss epoch 8: 0.6944945424542923\n",
      "Val_Accuracy 8: 0.7566489361702128\n",
      "Average loss epoch 9: 0.4627173320905488\n",
      "Accuracy 9: 0.8386904761904762\n",
      "Average val loss epoch 9: 0.6596606009063433\n",
      "Val_Accuracy 9: 0.7726063829787234\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        totalvalright = 0\n",
    "        nvalbatches = 0\n",
    "        totalvalloss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(val_init)\n",
    "        try:\n",
    "            while True:\n",
    "                accval,valloss = sess.run([accuracy, loss])\n",
    "                totalvalright += accval\n",
    "                nvalbatches += 1\n",
    "                totalvalloss += valloss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "        print('Average val loss epoch {0}: {1}'.format(i, totalvalloss/nvalbatches))\n",
    "        print('Val_Accuracy {0}: {1}'.format(i, totalvalright/nvalbatches)) \n",
    "    prediction = sess.run(preds, feed_dict={txt: valInp})\n",
    "    prediction = np.asarray(prediction)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728494623655914"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(np.argmax(prediction, 1), labels2.idxmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fooling around with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n",
      "(9, 2)\n",
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "f = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "#f = np.array([[1,2,3], [1,2,3]])\n",
    "#print(f)\n",
    "g = np.expand_dims(f, axis = 1)\n",
    "g = np.reshape(f, [-1,2])\n",
    "print(g)\n",
    "print(g.shape)\n",
    "print(np.squeeze(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
