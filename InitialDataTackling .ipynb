{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in training data\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx</th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>target_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98221616</td>\n",
       "      <td>4976</td>\n",
       "      <td>$ARNA APD334 for Amyotrophic Lateral Sclerosis...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82321187</td>\n",
       "      <td>9839</td>\n",
       "      <td>$OCLR Noob investor that i am, put a 7.38 stop...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>stop loss</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103328840</td>\n",
       "      <td>1455</td>\n",
       "      <td>$ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104840294</td>\n",
       "      <td>1111</td>\n",
       "      <td>$TMUS its acquisition of Layer3 TV The purchas...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>relative</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>forecast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94249158</td>\n",
       "      <td>1372</td>\n",
       "      <td>$SEED L2 Capital deal is real savvy. It takes ...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100979260</td>\n",
       "      <td>505</td>\n",
       "      <td>$BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100775772</td>\n",
       "      <td>1210</td>\n",
       "      <td>$WRN My fav $WRN pattern on my watchlist for 1...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   idx                                              tweet  \\\n",
       "0   98221616  4976  $ARNA APD334 for Amyotrophic Lateral Sclerosis...   \n",
       "1   82321187  9839  $OCLR Noob investor that i am, put a 7.38 stop...   \n",
       "2  103328840  1455  $ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...   \n",
       "3  104840294  1111  $TMUS its acquisition of Layer3 TV The purchas...   \n",
       "4   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "5   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "6   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "7   94249158  1372  $SEED L2 Capital deal is real savvy. It takes ...   \n",
       "8  100979260   505  $BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...   \n",
       "9  100775772  1210  $WRN My fav $WRN pattern on my watchlist for 1...   \n",
       "\n",
       "         category     subcategory target_num  \n",
       "0  Product Number  Product Number        334  \n",
       "1        Monetary       stop loss       7.38  \n",
       "2  Product Number  Product Number          1  \n",
       "3  Product Number  Product Number          5  \n",
       "4      Percentage        relative         14  \n",
       "5        Quantity        Quantity          4  \n",
       "6        Monetary        forecast          5  \n",
       "7        Temporal            date         33  \n",
       "8        Temporal            date          4  \n",
       "9        Temporal            date         11  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 2,
=======
     "execution_count": 3,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'finnum\\train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for encoding categories as numbers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 40,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_num'] = df['category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in fastText model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load('fastText1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing tweets by lowercasing"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 41,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = [x.lower() for x in df.tweet]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 42,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 42,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing target with <num\\>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 43,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to training, making this into a new column"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 44,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mod'] = df.apply(lambda x: flagNum(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 45,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd <num>  for amyotrophic lateral scler...\n",
       "1    $oclr noob investor that i am, put a  <num>  s...\n",
       "2    $es_f $spy bias-2 bearish and the dlt- <num>  ...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up  <num> %  4 time avg ...\n",
       "Name: mod, dtype: object"
      ]
     },
<<<<<<< HEAD
     "execution_count": 9,
=======
     "execution_count": 45,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mod'].head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 46,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
<<<<<<< HEAD
     "execution_count": 10,
=======
     "execution_count": 46,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out numbers and words less than 3 characters long"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 47,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped = [textPuncandNum(text) for text in df['mod']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of unique words from this processed text, excluding <num\\>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 48,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13107"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 48,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = list(stripped)\n",
    "low = ' '.join(low)\n",
    "low = list(set(low.split()))\n",
    "low.remove('<num>')\n",
    "len(low)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bounce&gt;$.,',\n",
       " 'merchants',\n",
       " 'stops',\n",
       " 'unofficially',\n",
       " '....jmo',\n",
       " 'viral',\n",
       " 'outdoor',\n",
       " 'value=$mm,',\n",
       " '$glad',\n",
       " 'dividend',\n",
       " 'adjust',\n",
       " 'afrezza',\n",
       " 'hundred',\n",
       " '(https://stockflare.com/stock/vco)',\n",
       " 'kicked',\n",
       " 'cover.',\n",
       " '$btc',\n",
       " 'away?',\n",
       " 'leg',\n",
       " 'http://pattern-based-trading.com/category/stocks/fitbit-fit/',\n",
       " 'lockup',\n",
       " '$bita',\n",
       " '$egn',\n",
       " 'events.',\n",
       " 'possibly.',\n",
       " 'pick!',\n",
       " 'http://mytradinglicks.com//',\n",
       " 'virginia,',\n",
       " 'property',\n",
       " 'pro',\n",
       " 'rise',\n",
       " 'bitcoins,',\n",
       " 'echnuter.com/channel-news/blackberry-expands-its-channel-ecosystem-with-six-new-partners-in-india.html',\n",
       " 'ups)',\n",
       " 'https://www.youtube.com/watch?v=_tnuslsaaa',\n",
       " '$cgnt',\n",
       " 'replaced',\n",
       " '-weeks',\n",
       " '$app,',\n",
       " 'limp.',\n",
       " 'bucks,',\n",
       " 'synergy&#;s',\n",
       " 'waited.',\n",
       " '@uundastan',\n",
       " 'add!',\n",
       " '$dxlg',\n",
       " 'bucks',\n",
       " '$alny',\n",
       " 'capital',\n",
       " 'passionate-some',\n",
       " 'awww',\n",
       " 'defined',\n",
       " 'briefly',\n",
       " 'https://finance.yahoo.com/news/corporate-news-blog-us-fda-.html',\n",
       " 'best-in-show,',\n",
       " 'uptrend',\n",
       " 'chartists?',\n",
       " 'scene',\n",
       " 'who&#;s',\n",
       " 'alt-coins',\n",
       " 'trending',\n",
       " 'conducting',\n",
       " '$cara',\n",
       " 'respiratory,',\n",
       " '$drio',\n",
       " '&quot;watchers&quot;',\n",
       " 'notified',\n",
       " 'season',\n",
       " '$snap',\n",
       " '$nxpi',\n",
       " 'out,',\n",
       " 'heating',\n",
       " 'dow',\n",
       " 'clearest',\n",
       " 'fiasco',\n",
       " 'blow',\n",
       " 'vol.',\n",
       " 'instruments',\n",
       " '$shop',\n",
       " 'sweat!',\n",
       " 'augmented',\n",
       " 'inventory',\n",
       " 'card',\n",
       " '$cdxc',\n",
       " 'yet?',\n",
       " 'easiest',\n",
       " 'days...just',\n",
       " 'advantage',\n",
       " 'days...news???',\n",
       " 'target',\n",
       " 'updates',\n",
       " 'gild',\n",
       " '$bmo',\n",
       " '$ada.x',\n",
       " '$thld',\n",
       " 'campaign',\n",
       " 'print.',\n",
       " 'walmart&#;s',\n",
       " 'owners.',\n",
       " 'published?',\n",
       " 'dollars',\n",
       " '$opy',\n",
       " 'trucking.',\n",
       " '$apd',\n",
       " 'pope',\n",
       " '$qtrh',\n",
       " 'nabbed',\n",
       " 'few.',\n",
       " 'excellent',\n",
       " 'cliff/first',\n",
       " 'tough',\n",
       " 'kazatomprom',\n",
       " 'x-men',\n",
       " 'boom!!!',\n",
       " 'buds',\n",
       " 'catches',\n",
       " 'wishing',\n",
       " '$kona',\n",
       " 'attached!',\n",
       " 'https://investoralmanac.com////the-us-oversupply-of-oil-is-ending-',\n",
       " '&amp;search-related',\n",
       " '$intc',\n",
       " 'anyone.',\n",
       " 'now!!!',\n",
       " 'dcthd',\n",
       " 'damn',\n",
       " 'hmm',\n",
       " '$vrtx',\n",
       " 'series.',\n",
       " '$nvda',\n",
       " '$tndm',\n",
       " ',big',\n",
       " 'area',\n",
       " 'lool.',\n",
       " '$eep',\n",
       " 'none',\n",
       " 'overpriced',\n",
       " 'dates',\n",
       " 'jbum',\n",
       " 'earnings',\n",
       " 'https://finance.yahoo.com/news/why-himax-technologies-inc-popped-.html',\n",
       " 'list.',\n",
       " 'exaggeration,',\n",
       " 'qatar',\n",
       " 'directors',\n",
       " 'perennial',\n",
       " 'orally',\n",
       " 'faded',\n",
       " 'term,',\n",
       " 'nukes,they&#;d',\n",
       " '$rds.a',\n",
       " '$canf',\n",
       " 'support!',\n",
       " 'consolidating',\n",
       " 'dubious',\n",
       " '$ino',\n",
       " 'currrently',\n",
       " 'overbought.',\n",
       " 'loan',\n",
       " '$exon',\n",
       " 'heiken-ashi',\n",
       " 'supply',\n",
       " 'uctt&#;s',\n",
       " '$gld',\n",
       " 'ltc',\n",
       " '#taxreform',\n",
       " 'shitty/red',\n",
       " 'formation',\n",
       " 'there&#;s',\n",
       " 'gopro',\n",
       " '#blozf',\n",
       " 'retrac',\n",
       " 'gpro',\n",
       " 'close&gt;,',\n",
       " '$hrg',\n",
       " 'tune',\n",
       " 'rev-multi',\n",
       " '$mnkd',\n",
       " '$dwt',\n",
       " 'still',\n",
       " '$mime',\n",
       " 'gine',\n",
       " 'already,',\n",
       " 'lows???',\n",
       " 'pre-mkt',\n",
       " 'might',\n",
       " 'safer',\n",
       " 'weekend!',\n",
       " 'play/cheap',\n",
       " 'comin&#;...',\n",
       " 'boost!!!!',\n",
       " '-year-low',\n",
       " 'entities',\n",
       " 'morning/afternoon',\n",
       " '#madalena',\n",
       " '$suns',\n",
       " 'commute',\n",
       " '$team',\n",
       " 'loans',\n",
       " '$snoa',\n",
       " '#bullboard',\n",
       " 'milly',\n",
       " 'china!',\n",
       " 'contributions.',\n",
       " 'gurus',\n",
       " 'others?',\n",
       " 'updated',\n",
       " '$uctt',\n",
       " 'nzr',\n",
       " 'outside',\n",
       " 'firms,',\n",
       " '$hyg',\n",
       " 'period.',\n",
       " 'benefits.',\n",
       " 'vehicles',\n",
       " 'winner',\n",
       " 'die...',\n",
       " 'eventually)',\n",
       " 'sensex',\n",
       " 'total=',\n",
       " 'class.',\n",
       " '@snowball',\n",
       " 'unable',\n",
       " 'for.',\n",
       " 'tight.',\n",
       " 'average',\n",
       " 'https://investoralmanac.com////greenhaven-road-capital-q',\n",
       " 'title',\n",
       " 'surprise',\n",
       " 'out',\n",
       " '$dea',\n",
       " '$cenx',\n",
       " '$ian',\n",
       " 'geekbench',\n",
       " 'digits',\n",
       " 'green..',\n",
       " 'omen',\n",
       " 'post,',\n",
       " 'importance',\n",
       " 'buyout,',\n",
       " 'gilead',\n",
       " '&quot;crypto&quot;',\n",
       " '$omnt',\n",
       " 'holiday',\n",
       " 'mar',\n",
       " 'solid',\n",
       " 'news!',\n",
       " 'goertek',\n",
       " 'quiram',\n",
       " 'divestiture',\n",
       " '$ppc',\n",
       " 'land,',\n",
       " '$nzdusd',\n",
       " 'trending-pback',\n",
       " '$fnko',\n",
       " 'northern',\n",
       " '$hii',\n",
       " 'showing',\n",
       " 'holdings/share',\n",
       " 'then.....story',\n",
       " 'pre-divi',\n",
       " 'pre-holiday',\n",
       " '$ltc.x,',\n",
       " 'avg,',\n",
       " 'worked',\n",
       " 'superheated',\n",
       " '(yield',\n",
       " 'https://docs.google.com/spreadsheets/d/jx-dvxungrxxgzvnceubfqqzacumx_ci/edit?usp=sharing',\n",
       " 'near',\n",
       " 'line,',\n",
       " 'rose',\n",
       " 'determination',\n",
       " 'loving',\n",
       " 'refund',\n",
       " 'thoughts',\n",
       " 'renal',\n",
       " 'hivinfectious',\n",
       " '$vxx',\n",
       " 'interesting.',\n",
       " 'deal',\n",
       " 'banger',\n",
       " 'week-',\n",
       " 'bull',\n",
       " 'awhile',\n",
       " 'show',\n",
       " '$otel',\n",
       " '$soupq',\n",
       " 'ask.',\n",
       " '$rsls.',\n",
       " 'mergers,',\n",
       " 'output',\n",
       " '@turquoise',\n",
       " 'automation',\n",
       " 'invested',\n",
       " 'bio',\n",
       " 'nice',\n",
       " 'bigly',\n",
       " 'continuing.',\n",
       " 'reservations',\n",
       " '&quot;@ipotweet:',\n",
       " 'shingrix',\n",
       " 'ceo,',\n",
       " 'chasing',\n",
       " 'restricted',\n",
       " 'nvidia.',\n",
       " 'gem.',\n",
       " 'divi!',\n",
       " 'wholesale',\n",
       " '$cei',\n",
       " 'coupons',\n",
       " '(haters',\n",
       " 'wedge',\n",
       " 'care',\n",
       " 'blue',\n",
       " '(closing',\n",
       " '-,where',\n",
       " 'kinglist',\n",
       " 'ceo.',\n",
       " 'pivotal',\n",
       " 'bank!',\n",
       " 'we&#;d',\n",
       " 'shale',\n",
       " 'moviepass',\n",
       " 'price.',\n",
       " 'disconnected',\n",
       " 'patience...esp.',\n",
       " 'projections.',\n",
       " 'tecfidera[with',\n",
       " '$kbwd',\n",
       " 'triangles',\n",
       " '$hibb',\n",
       " 'given',\n",
       " 'estimates',\n",
       " 'reform',\n",
       " 'usual',\n",
       " 'session.',\n",
       " 'aphb',\n",
       " 'launched',\n",
       " 'users.',\n",
       " 'wipes',\n",
       " '$jnpr',\n",
       " '(weekly)',\n",
       " '$hbi',\n",
       " 'richer',\n",
       " 'hand,',\n",
       " 'https://coinmarketcap.com/currencies/bitcoin-cash/',\n",
       " 'anythingin',\n",
       " 'biopsies',\n",
       " 'daily',\n",
       " 'drops.',\n",
       " 'achieves',\n",
       " 'primary',\n",
       " 'width',\n",
       " 'focus.',\n",
       " 'pointclickcare',\n",
       " '-years/',\n",
       " 'days',\n",
       " 'hear',\n",
       " 'structured',\n",
       " 'https://www.youtube.com/watch?v=vgfadvlio',\n",
       " 'leg-up',\n",
       " 'pattern.',\n",
       " 'loses',\n",
       " '$gsum&#;s',\n",
       " 'bofaml',\n",
       " 'journey.',\n",
       " '$ostk',\n",
       " 'boys',\n",
       " 'list!!!',\n",
       " 'agreement,',\n",
       " '$endp',\n",
       " '$edv.ca',\n",
       " 'fl...time',\n",
       " 'amat',\n",
       " '$aveo',\n",
       " '$bzun,',\n",
       " 'vcel',\n",
       " 'net',\n",
       " 'https://stocknews.com/news/flks-cantor-fitzgerald-reiterates-hold-rating-',\n",
       " '..with',\n",
       " '$adus',\n",
       " 'employees',\n",
       " 'https://insiderfinancial.com/canopy-growth-corp-otcmktstwmjf-getting-marijuana-into-the-fortune-',\n",
       " '$wrn',\n",
       " 'overdone..',\n",
       " 'different',\n",
       " 'usd',\n",
       " 'recap:',\n",
       " 'nuts.',\n",
       " 'imminent.',\n",
       " 'assets,',\n",
       " 'called',\n",
       " 'poor',\n",
       " 'ladies',\n",
       " '@mrfrancois',\n",
       " 'yearly',\n",
       " 'prdts.',\n",
       " 'mins',\n",
       " 'baron',\n",
       " 'historic',\n",
       " 'resp.',\n",
       " 'know',\n",
       " 'daily....',\n",
       " '#swingtrade',\n",
       " 'experiment,',\n",
       " 'tsx',\n",
       " 'lbs',\n",
       " 'tho',\n",
       " 'voyage',\n",
       " 'license',\n",
       " 'stay',\n",
       " '$axti',\n",
       " 'xiaomi',\n",
       " 'yeah.',\n",
       " '$cycc',\n",
       " 'basely',\n",
       " '...still',\n",
       " 'customers',\n",
       " 'resurgence',\n",
       " 'booze.',\n",
       " 'tulip',\n",
       " 'holder-but',\n",
       " 'year&#',\n",
       " 'wks,',\n",
       " 'anything,',\n",
       " 'advancements',\n",
       " '$htgm',\n",
       " '$mdgl',\n",
       " 'spells',\n",
       " 'consolidation,',\n",
       " 'halting',\n",
       " 'weeks...stay',\n",
       " '$enph',\n",
       " 'brands',\n",
       " 'cross',\n",
       " 'position.',\n",
       " 'obviously',\n",
       " 'prescriptions.',\n",
       " 'there:',\n",
       " 'oct',\n",
       " 'corp',\n",
       " '.maybe',\n",
       " 'play!!!',\n",
       " '--results-earnings-call-transcript?part=single',\n",
       " 'http://www.chartsmarter.com////chartsmarter-tuesday-game-plan-/',\n",
       " '$rad,',\n",
       " 'low,seeing',\n",
       " 'shield..',\n",
       " 'jefferies',\n",
       " 'spoiled.',\n",
       " '$mzor',\n",
       " 'careful',\n",
       " 'mnk',\n",
       " 'starting...',\n",
       " 'predict',\n",
       " '$swn',\n",
       " 'jeans',\n",
       " 'hold!',\n",
       " '@towman',\n",
       " 'earnings/',\n",
       " '$ipdn,',\n",
       " '$arkw',\n",
       " '&#;users&#;',\n",
       " 'shaka',\n",
       " 'fear.',\n",
       " 'interesting,',\n",
       " 'schwab',\n",
       " 'cafc',\n",
       " 'triggers',\n",
       " 'son!',\n",
       " 'inc.you',\n",
       " 'norway',\n",
       " 'filling',\n",
       " 'shares..',\n",
       " 'dumb',\n",
       " 'tcco',\n",
       " 'lng',\n",
       " 'remission.',\n",
       " '#alternativefacts',\n",
       " 'anyway.',\n",
       " 'chrondrocytes',\n",
       " 'only!',\n",
       " 'solved',\n",
       " 'rip',\n",
       " 'yikes!!!',\n",
       " 'tradingwithzach.com.',\n",
       " 'dumb!',\n",
       " 'only$.',\n",
       " 'tomm',\n",
       " 'dividend...',\n",
       " '$fred',\n",
       " '$cwh',\n",
       " '$abil',\n",
       " 'pinch',\n",
       " 'homebuilders',\n",
       " 'finish!',\n",
       " '$ctix',\n",
       " ',,but',\n",
       " 'third',\n",
       " 'beautiful',\n",
       " '@/near',\n",
       " '$kool',\n",
       " '$nib',\n",
       " 'novartis',\n",
       " 'https://www.youtube.com/watch?v=fhcebffh',\n",
       " 'china.',\n",
       " '$tho,',\n",
       " 'shareprice',\n",
       " '(now',\n",
       " 'parts',\n",
       " 'hand.',\n",
       " '$insy',\n",
       " 'dividend!',\n",
       " 'out:',\n",
       " 'trigger',\n",
       " 'corruption!!',\n",
       " '$xly',\n",
       " '$ssc',\n",
       " '$aqxp@',\n",
       " 'balance',\n",
       " 'headlines.',\n",
       " '@tylerstoner',\n",
       " 'tweeted',\n",
       " 'corrects',\n",
       " 'halves',\n",
       " 'spot-',\n",
       " 'cents,',\n",
       " '$achn',\n",
       " 'moron',\n",
       " '@lizzie',\n",
       " 'sales?',\n",
       " '$lvs',\n",
       " 'suggest',\n",
       " 'for!',\n",
       " 'strategic',\n",
       " 'ytd,,',\n",
       " 'wage',\n",
       " 'sells',\n",
       " 'things',\n",
       " 'bought,',\n",
       " '$ctrn',\n",
       " '$eurcad:',\n",
       " 'construction,',\n",
       " 'ctlt',\n",
       " 'such',\n",
       " 'then????',\n",
       " 'rollover',\n",
       " 'jun',\n",
       " 'assests.',\n",
       " 'www.businessinsider.com/saudi-arabia-gas-prices--',\n",
       " 'dummies,&quot;',\n",
       " 'big',\n",
       " '$mkc',\n",
       " 'neighbor',\n",
       " 'wake',\n",
       " 'model.',\n",
       " 'lineup',\n",
       " '$gpor',\n",
       " 'clf',\n",
       " 'stopped',\n",
       " 'row..',\n",
       " 'shorts..',\n",
       " 'day..hmm.',\n",
       " 'issues,',\n",
       " 'horrible',\n",
       " 'profits,can',\n",
       " 'real',\n",
       " 'smashed',\n",
       " 'sees',\n",
       " 'assassin&#;s',\n",
       " 'lolol',\n",
       " '$labu',\n",
       " '@thelonghi',\n",
       " 'bsishness',\n",
       " 'factory',\n",
       " 'newspaper',\n",
       " 'sen.',\n",
       " 'apple).',\n",
       " 'day....let&#;s',\n",
       " '@alexanderhandleton',\n",
       " 'rating.',\n",
       " '...peeking',\n",
       " 'nasdaq.',\n",
       " 'namaste',\n",
       " 'hitting',\n",
       " 'shifted',\n",
       " 'https://jpratt.wordpress.com////current-carbon-dioxide-level-not-seen-for--years-stopadani-auspol-qldpol/',\n",
       " 'owning',\n",
       " 'fly',\n",
       " '$enbl',\n",
       " 'vs...diesel',\n",
       " 'rent.',\n",
       " 'basing',\n",
       " '$ac.ca',\n",
       " 'booyah',\n",
       " 'nears',\n",
       " 'slammed',\n",
       " 'hrs.',\n",
       " 'hmmm.',\n",
       " 'nvidia....',\n",
       " '$sune',\n",
       " '$uup',\n",
       " 'buyers?',\n",
       " 'forth',\n",
       " 'challenge;',\n",
       " 'plan.',\n",
       " '$tfm',\n",
       " 'low!',\n",
       " '$dal',\n",
       " 'watchlist:',\n",
       " 'http://stockmillionaires.com/stock-watch-list-updated--cdna-ears-plug/',\n",
       " 'chain',\n",
       " 'https://t.co/lmncwyna',\n",
       " 'sites',\n",
       " 'aspects',\n",
       " 'won&#;t',\n",
       " 'mothers',\n",
       " 'pushing',\n",
       " 'winning',\n",
       " 'respond.',\n",
       " 'touch',\n",
       " 'out.stop@$',\n",
       " 'from',\n",
       " 'emerald',\n",
       " '$jks',\n",
       " 'hard!',\n",
       " 'exercise',\n",
       " 'avgs.',\n",
       " 'canna-based',\n",
       " 'bringin',\n",
       " 'alibaba:',\n",
       " 'http://majorleaguestocks.com',\n",
       " 'groupon',\n",
       " 'high/low',\n",
       " 'good,',\n",
       " 'cannabis-crypto',\n",
       " 'exploding!!!',\n",
       " 'dream',\n",
       " '$meli',\n",
       " 'ema(',\n",
       " 'week....hoping',\n",
       " '$cohu',\n",
       " 'skype',\n",
       " '$bch.x$bch.x',\n",
       " '$doge',\n",
       " 'stocks',\n",
       " '$cetx',\n",
       " 'things.',\n",
       " 'icing',\n",
       " 'https://t.co/edafpigm',\n",
       " 'gdx',\n",
       " 'street.&quot;',\n",
       " 'hemophilia',\n",
       " 'longest',\n",
       " 'pause,',\n",
       " 'passes',\n",
       " 'timing.',\n",
       " 'come.expect',\n",
       " 'ltd.,',\n",
       " 'long-term',\n",
       " '$ptn.',\n",
       " 'hill',\n",
       " '@mr_robot',\n",
       " 'shortly',\n",
       " '$au.v',\n",
       " 'benifit',\n",
       " 'friend',\n",
       " '$riot',\n",
       " 'listen',\n",
       " '$cadjpy',\n",
       " '$ntes',\n",
       " 'significant',\n",
       " 'qtr.',\n",
       " '$koru',\n",
       " 'halts',\n",
       " 'close.',\n",
       " 'hills.',\n",
       " 'dreamers?',\n",
       " 'since.',\n",
       " 'best',\n",
       " '-just',\n",
       " '$ashr',\n",
       " 'vitamins',\n",
       " 'fuck',\n",
       " 'stop:',\n",
       " 'evolve',\n",
       " 'thousands',\n",
       " 'http://pattern-based-trading.com////important-market-update---/',\n",
       " 'dma.',\n",
       " 'aapl',\n",
       " 'pip',\n",
       " 'https://www.youtube.com/watch?v=wzhkdjsq',\n",
       " 'ltc.x',\n",
       " 'words.',\n",
       " 'noise.',\n",
       " 'holding!',\n",
       " 'zone!',\n",
       " 'successfully',\n",
       " 'procure',\n",
       " 'magnet!',\n",
       " 'self-driving',\n",
       " '#flexpharma',\n",
       " 'there?',\n",
       " 'quit',\n",
       " '$viacom',\n",
       " 'retail',\n",
       " 'risinger',\n",
       " 'wait,',\n",
       " 'billioniare',\n",
       " 'stockholders',\n",
       " 'scott',\n",
       " '$ubq.x',\n",
       " '$gdx&lt;',\n",
       " '$mmc.',\n",
       " 'dolla-holla!!!',\n",
       " 'camp,',\n",
       " 'sto',\n",
       " '$chkp',\n",
       " 'contago',\n",
       " 'volatile',\n",
       " 'watching!',\n",
       " '$rdus',\n",
       " 'animal.',\n",
       " '@tickertutor',\n",
       " 'arrives',\n",
       " 'wfm',\n",
       " '@unodare',\n",
       " 'jagoffs',\n",
       " 'pop...',\n",
       " 'putting',\n",
       " 'https://finance.yahoo.com/news/forget-amazon-e-commerce-company-.html',\n",
       " '$wlh',\n",
       " '$isrg',\n",
       " 'confident.',\n",
       " 'neo',\n",
       " 'investment;',\n",
       " '$sid',\n",
       " 'action!',\n",
       " 'lung',\n",
       " 'picture)',\n",
       " 'haha.',\n",
       " 'wish',\n",
       " '&#;&#;going',\n",
       " 'dimon,',\n",
       " 'pass',\n",
       " 'revenue,',\n",
       " 'inormal',\n",
       " 'units',\n",
       " 'corporation,',\n",
       " 'following:',\n",
       " 'more;',\n",
       " 'daily.',\n",
       " '$lfin',\n",
       " 'min,',\n",
       " '$viab',\n",
       " 'reinvested',\n",
       " 'unfortunately',\n",
       " 'witness',\n",
       " 'icos',\n",
       " 'texas!',\n",
       " 'dropp',\n",
       " 'poc',\n",
       " 'driver.',\n",
       " 'click',\n",
       " '$ctlt',\n",
       " 'matches',\n",
       " 'conviction',\n",
       " 'booooming',\n",
       " '(opentable)',\n",
       " 'https://www.equities.com/news/cummins-inc-cmi-moves-lower-on-volume-spike-for-november-',\n",
       " 'boxers',\n",
       " '-year',\n",
       " 'four',\n",
       " 'decide',\n",
       " 'shareholders',\n",
       " 'starting',\n",
       " '&quot;majority',\n",
       " 'well',\n",
       " 'here...',\n",
       " '$staf',\n",
       " 'quiet',\n",
       " 'https://www.thecontraaccountant.com/single-post////have-you-forgotten-about-the-',\n",
       " 'device',\n",
       " '@ipotweet',\n",
       " '$.what',\n",
       " 'bet',\n",
       " 'basmati',\n",
       " '@hassriz',\n",
       " '$biib',\n",
       " '$ltc',\n",
       " 'now..',\n",
       " 'meaning',\n",
       " 'enters',\n",
       " 'incredible.',\n",
       " 'site',\n",
       " 'net,',\n",
       " '$ogi.ca',\n",
       " 'night.',\n",
       " 'ride',\n",
       " '$sftby',\n",
       " 'queue,',\n",
       " 'reuters',\n",
       " 'fomo',\n",
       " 'line.',\n",
       " 'seeking',\n",
       " 'easiest,',\n",
       " 'later:&#;&#;n',\n",
       " 'stover',\n",
       " 'vix&#;in',\n",
       " 'uptrending,',\n",
       " 'transcript',\n",
       " '$kbh',\n",
       " 'only',\n",
       " 'drawdown',\n",
       " 'shars',\n",
       " 'disruptions:',\n",
       " 'industries',\n",
       " 'outperformance',\n",
       " 'tell-',\n",
       " 'justice',\n",
       " 'hearing!',\n",
       " '$ekso,',\n",
       " 'holder',\n",
       " 'no-brainer',\n",
       " 'doesn;t',\n",
       " '$sftbf,',\n",
       " 'weekends,',\n",
       " 'early.',\n",
       " '$shos',\n",
       " 'robert',\n",
       " 'annihilated',\n",
       " 'tip,',\n",
       " 'ordere',\n",
       " 'caution',\n",
       " 'days..',\n",
       " 'nics',\n",
       " 'cdk',\n",
       " 'workspace',\n",
       " 'wow!!',\n",
       " 'alert',\n",
       " 'er+missed',\n",
       " 'bright',\n",
       " 'partnerships,',\n",
       " 'ath&#;d',\n",
       " 'ladies&amp;gentlemen,',\n",
       " '#luckylou',\n",
       " 'aside',\n",
       " 'all...',\n",
       " 'everyone-there',\n",
       " 'hurricanes',\n",
       " '$eth.x,',\n",
       " 'ideas?',\n",
       " '$btc.x.',\n",
       " '$cemp',\n",
       " 'pc&#;s.',\n",
       " 'sniffing',\n",
       " 'greatest',\n",
       " '$uwt',\n",
       " '#tesla',\n",
       " 'hdge',\n",
       " 'people.shows',\n",
       " '$stripe',\n",
       " '$leds',\n",
       " '-space//',\n",
       " 'weird.',\n",
       " '$dxtr',\n",
       " 'musk',\n",
       " 'correcting',\n",
       " 'group)',\n",
       " 'purposes',\n",
       " 'satisfy',\n",
       " '@jkallstar',\n",
       " '$brt:',\n",
       " 'stinking',\n",
       " 'closures.',\n",
       " 'sentiment----',\n",
       " '$socl',\n",
       " 'paying..',\n",
       " 'website',\n",
       " 'strategy',\n",
       " '$rick',\n",
       " '$dlr',\n",
       " 'one)',\n",
       " 'filled',\n",
       " 'masterbato',\n",
       " 'moment,',\n",
       " 'exercising',\n",
       " 'upgrade',\n",
       " '$asm',\n",
       " 'headline,',\n",
       " 'drama',\n",
       " 'obvious,',\n",
       " 'distributed',\n",
       " 'buybacks,',\n",
       " '$ttm',\n",
       " 'las',\n",
       " 'etf',\n",
       " 'names',\n",
       " 'looooooooong!',\n",
       " 'discounts.',\n",
       " '(swing)',\n",
       " '$lode',\n",
       " 'portfolio...',\n",
       " 'stays',\n",
       " 'mode.',\n",
       " 'yoube',\n",
       " 'tea',\n",
       " 'guided',\n",
       " 'theater',\n",
       " 'on&quot;...',\n",
       " 'ratio',\n",
       " '$vuzi',\n",
       " 'media',\n",
       " 'passthrough',\n",
       " 'cheaper',\n",
       " 'pixel',\n",
       " 'dpw',\n",
       " 'titanic',\n",
       " 'madalena',\n",
       " 'january',\n",
       " '$vti',\n",
       " 'bio..',\n",
       " 'trend',\n",
       " 'gross',\n",
       " 'conference?',\n",
       " '-dividend-stocks-expected-to-increase-distributions-in-/',\n",
       " '$fuapf',\n",
       " 'said....this',\n",
       " 'closer...',\n",
       " 'need:',\n",
       " 'patents',\n",
       " 'sometimes',\n",
       " 'nightmare',\n",
       " 'above,',\n",
       " '$arwr,',\n",
       " '@reez',\n",
       " 'bad,',\n",
       " 'off!',\n",
       " '$iop.x',\n",
       " 'normal?',\n",
       " '@jkessel',\n",
       " '$nuro',\n",
       " '@robertmoreira',\n",
       " 'surprising.',\n",
       " 'tip',\n",
       " 'too',\n",
       " 'fort',\n",
       " 'appetizing!',\n",
       " '$wins',\n",
       " 'tired.',\n",
       " 'wkly&lt;',\n",
       " 'https://www.yahoo.com/amphtml/finance/news/vuzix-m-programs-top--.html',\n",
       " 'shared',\n",
       " '$acia',\n",
       " '#long',\n",
       " 'off',\n",
       " 'gents.',\n",
       " '$gsv',\n",
       " 'again,',\n",
       " 'times.',\n",
       " 'averts-',\n",
       " 'industry,',\n",
       " '$ddd',\n",
       " 'cable',\n",
       " 'spartanstrade',\n",
       " 'indexes',\n",
       " 'calls/shares,',\n",
       " 'synthetic',\n",
       " '(just',\n",
       " 'out....',\n",
       " 'https://www.youtube.com/watch?v=dbxixxikw',\n",
       " 'loved',\n",
       " 'speculative',\n",
       " '$ipdn...',\n",
       " 'bags',\n",
       " 'bagga.',\n",
       " 'gekkos',\n",
       " 'fcking',\n",
       " '$ncr',\n",
       " 'https://twitter.com/andybiotech/status/',\n",
       " 'quantx',\n",
       " 'profits',\n",
       " 'entry!',\n",
       " 'ahead!',\n",
       " 'https://finance.yahoo.com/news/-things-kratos-defense-security-.html',\n",
       " 'possible!!',\n",
       " 'result',\n",
       " 'red.',\n",
       " 'support...',\n",
       " '$dsx',\n",
       " '-dmas',\n",
       " '$tif',\n",
       " '$nxtd',\n",
       " 'dont.',\n",
       " 'hour',\n",
       " 'dude',\n",
       " '#twmjf.',\n",
       " 'own!',\n",
       " 'fri.',\n",
       " '%.....almost',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to get embeddingss for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.wv[low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13107, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing these unqique words in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {k: v for v, k in enumerate(low)}\n",
    "label_dict['<num>'] = len(label_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding <num\\> back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13107"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict['<num>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding filler word to keep all tweets the same length. Then replacing all words with their dictionary equivalent. This is for tenssorflows matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufferIndex = len(label_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "modifiedText = [[label_dict[word] for word in text.split()]for text in stripped]\n",
    "maxLen = max(map(len, modifiedText))\n",
    "for item in modifiedText:                # for each item in the list\n",
    "    while len(item) < maxLen:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex) \n",
    "numpyInp = np.asarray(modifiedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
=======
   "execution_count": 49,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[ 6242,  3203, 13107, ..., 13108, 13108, 13108],\n",
       "       [11717,  2068,  9415, ..., 13108, 13108, 13108],\n",
       "       [ 9748, 12082,  6560, ..., 13108, 13108, 13108],\n",
       "       ...,\n",
       "       [10546,  4721,   883, ..., 13108, 13108, 13108],\n",
       "       [ 2046,  2158, 12260, ..., 13108, 13108, 13108],\n",
       "       [13107,  2046,  2158, ..., 13108, 13108, 13108]])"
      ]
     },
     "execution_count": 20,
=======
       "['popped',\n",
       " 'light',\n",
       " 'okay',\n",
       " '@thenewsguy',\n",
       " 'sessions.',\n",
       " 'micron',\n",
       " 'exhaust',\n",
       " '$intc',\n",
       " 'controls',\n",
       " 'falls',\n",
       " 'ending',\n",
       " '$trxc',\n",
       " 'horizontals',\n",
       " '$gluu,',\n",
       " 'responded',\n",
       " 'buyer@',\n",
       " 'increase',\n",
       " 'res',\n",
       " '$mdr',\n",
       " 'fib',\n",
       " '&quot;pullback&quot;',\n",
       " '$cbl',\n",
       " 'hurt-but',\n",
       " 'anywhere',\n",
       " '$mro',\n",
       " 'vix',\n",
       " 'loss',\n",
       " '(especially',\n",
       " 'accumulation.',\n",
       " 'ridge',\n",
       " '{weekly}',\n",
       " '$ge--&gt;&#;s',\n",
       " 'jagx',\n",
       " '$acst$acst',\n",
       " '$czr',\n",
       " 'science',\n",
       " '$gene',\n",
       " 'shareholder',\n",
       " 'opinion',\n",
       " '$kite',\n",
       " 'rising',\n",
       " '-year-low',\n",
       " 'millions',\n",
       " 'maturities',\n",
       " 'https://www.fda.gov/newsevents/newsroom/pressannouncements/ucm.htm',\n",
       " '$hsgx',\n",
       " 'total?',\n",
       " 'autonomous',\n",
       " 'coming?',\n",
       " 'char',\n",
       " 'gets.',\n",
       " 'patience...',\n",
       " 'hackers',\n",
       " 'expectation.',\n",
       " '$dgly',\n",
       " 'buying,',\n",
       " 'achieve',\n",
       " 'forecasts',\n",
       " 'shorts,bearish',\n",
       " '$surry',\n",
       " 'back?',\n",
       " 'bitcoin-',\n",
       " 'lower..',\n",
       " 'first-in-human',\n",
       " 'storage',\n",
       " '$ebio.',\n",
       " 'assistance,',\n",
       " '@hardassets',\n",
       " 'haven&#;t',\n",
       " 'september.might',\n",
       " 'cleared',\n",
       " '$adx',\n",
       " 'savvy.',\n",
       " 'disconnected',\n",
       " 'hike,',\n",
       " '$cohr',\n",
       " 'lines',\n",
       " '$lscc',\n",
       " '$gbsn',\n",
       " '$carg',\n",
       " '$bil',\n",
       " 'models!',\n",
       " '#algotrading',\n",
       " 'assets,',\n",
       " '$mfa',\n",
       " 'highly',\n",
       " '$prelf/$hive',\n",
       " 'follows',\n",
       " 'rising,',\n",
       " 'battle',\n",
       " 'important',\n",
       " 'fine!',\n",
       " '$plx.',\n",
       " 'indicators',\n",
       " 'alibaba',\n",
       " '...non-combustible',\n",
       " '-,where',\n",
       " 'shine',\n",
       " 'leg-up',\n",
       " '$aph',\n",
       " '(depends',\n",
       " 'down...',\n",
       " '$gasl',\n",
       " 'taking.',\n",
       " '$cytx',\n",
       " '#samsung',\n",
       " 'less.',\n",
       " 'exaggerating',\n",
       " 'screaming',\n",
       " 'today....the',\n",
       " 'needham.',\n",
       " 'witness',\n",
       " '$axon',\n",
       " '@antishock',\n",
       " '#eth',\n",
       " '$tplm,',\n",
       " 'colitis;',\n",
       " '$hlth',\n",
       " 'everyone',\n",
       " 'starts',\n",
       " 'suicide',\n",
       " '$baba',\n",
       " 'keller(athlon,ryzen,apple',\n",
       " 'conformation',\n",
       " 'time&#;s',\n",
       " 'chairman',\n",
       " '$delt.',\n",
       " 'that?idiots,look',\n",
       " '$cnx',\n",
       " 'gawn!',\n",
       " 'tight.',\n",
       " 'chartsmarter',\n",
       " 'depletion',\n",
       " 'these',\n",
       " 'bears,',\n",
       " 'pos',\n",
       " '(swing)',\n",
       " 'ebitda',\n",
       " '$cei',\n",
       " 'down!',\n",
       " 'selling.',\n",
       " 'elliot',\n",
       " 'trials',\n",
       " 'wanting',\n",
       " 'funny.',\n",
       " '$cutr',\n",
       " 'times...',\n",
       " 'fell',\n",
       " 'rylty',\n",
       " 'lod',\n",
       " 'baaang!',\n",
       " 'red',\n",
       " '$ears,',\n",
       " 'peeps',\n",
       " 'guidance.',\n",
       " 'stake',\n",
       " 'without',\n",
       " '$regn',\n",
       " '$bmo',\n",
       " 'mid-term:',\n",
       " 'channel',\n",
       " 'science,',\n",
       " 'tnx',\n",
       " '#longterm',\n",
       " '$shw',\n",
       " 'debt+',\n",
       " 'changed',\n",
       " 'financial',\n",
       " 'park',\n",
       " '@yearoftrump',\n",
       " 'struggling',\n",
       " 'little',\n",
       " '$gnca',\n",
       " '-but-could-it-break-/?utm_source=yahoo&amp;utm_medium=partner&amp;utm_campaign=yahootix&amp;partner=yahootix&amp;yptr=yahoo#ef',\n",
       " 'chances',\n",
       " 'roof.',\n",
       " 'wrecked',\n",
       " '$lpi',\n",
       " '-etsy-fcau-lmb-rkn-trip-evi/',\n",
       " 'earnings.pension?',\n",
       " '$sskn',\n",
       " 'friday.',\n",
       " 'fast.',\n",
       " 'drowning.',\n",
       " 'respected',\n",
       " 'https://www.youtube.com/watch?v=ykrvlqywm',\n",
       " '$mara',\n",
       " 'boundaries...',\n",
       " 'csnider',\n",
       " 'scary',\n",
       " 'reacted',\n",
       " 'apple.',\n",
       " 'acting',\n",
       " 'october:',\n",
       " 'touched',\n",
       " 'cme&#;s',\n",
       " 'wouldnt',\n",
       " 'we&#;d',\n",
       " '[amex]',\n",
       " 'brokeout',\n",
       " 'stent',\n",
       " 'tanker',\n",
       " 'depot',\n",
       " 'lol)',\n",
       " '$apvo',\n",
       " 'policy',\n",
       " 'nhl',\n",
       " 'intc',\n",
       " 'tobacco',\n",
       " 'pets.',\n",
       " 'eventually)',\n",
       " 'mind',\n",
       " 'describes',\n",
       " 'hiv',\n",
       " 'top....',\n",
       " 'coming,',\n",
       " 'watched',\n",
       " 'starter.',\n",
       " 'mornin.',\n",
       " 'give',\n",
       " 'innat',\n",
       " 'quarterly',\n",
       " '(monthly)',\n",
       " 'pokemon',\n",
       " 'https://www.als.net/docs/uploads/why_might_gilenya_be_helpful_for_als.pdf',\n",
       " 'exodus',\n",
       " '$bne.ca',\n",
       " '$sphs',\n",
       " '$evok',\n",
       " 'rally.',\n",
       " '$sbgl',\n",
       " 'crude?',\n",
       " 'trrw',\n",
       " 'oversupply',\n",
       " 'information',\n",
       " '$dpz',\n",
       " 'reached?',\n",
       " 'closures.',\n",
       " 'richard',\n",
       " 'batch',\n",
       " '$sohu',\n",
       " '@jayrome',\n",
       " '$hbm',\n",
       " 'in-depth',\n",
       " 'cj&#;s',\n",
       " 'titanic',\n",
       " 'trend...',\n",
       " 'thanksgiving.',\n",
       " '#thebigone',\n",
       " 'away',\n",
       " 'bak',\n",
       " 'cleaner.',\n",
       " 'trigger.stop@..',\n",
       " 'makings',\n",
       " '$uup',\n",
       " '@natesnotes',\n",
       " 'effort',\n",
       " 'trend.',\n",
       " '$gss.',\n",
       " 'caps',\n",
       " '$arkw',\n",
       " 'roughly',\n",
       " 'roche',\n",
       " 'can',\n",
       " '@stockauthority',\n",
       " 'abnormal',\n",
       " 'sails',\n",
       " 'res,',\n",
       " 'everyone&#;s',\n",
       " 'w/big',\n",
       " 'retail',\n",
       " 'day.&quot;',\n",
       " 'dilly',\n",
       " 'https://stocknewstimes.com////canaccord-genuity-reiterates-buy-rating-for-kratos-defense-security-solutions-inc-ktos.html',\n",
       " 'please?!',\n",
       " 'watt.',\n",
       " 'western',\n",
       " 'road',\n",
       " 'prescription,',\n",
       " '-letter-adp-chipotle-fannie-and-freddie-herbalife-short-mondelez/',\n",
       " 'bloom',\n",
       " 'extension...maybe',\n",
       " 'know,',\n",
       " 'adf',\n",
       " 'line..',\n",
       " 'grows.',\n",
       " '$xgti,',\n",
       " '$htnm',\n",
       " 'cme',\n",
       " 'device.next',\n",
       " '$eca',\n",
       " '(snyr)',\n",
       " '$xco',\n",
       " 'coins.',\n",
       " 'kfg',\n",
       " '(amd)',\n",
       " 'aphb',\n",
       " '$aphqf',\n",
       " 'eats',\n",
       " 'coming!!!',\n",
       " 'bugs',\n",
       " 'acquired',\n",
       " 'monero.',\n",
       " 'close',\n",
       " '$himx',\n",
       " 'underperformer',\n",
       " 'birthday',\n",
       " 'amd:',\n",
       " 'aug--;',\n",
       " '$mux',\n",
       " '$icui',\n",
       " 'bulls,',\n",
       " 'position.',\n",
       " 'fly.',\n",
       " '-programs-top--.html',\n",
       " '$eyeg',\n",
       " 'thurs',\n",
       " 'bad.',\n",
       " 'https://tinyurl.com/yznxe',\n",
       " '$ibm**.',\n",
       " '-now',\n",
       " '$btc.x',\n",
       " 'technical',\n",
       " '$pzrx',\n",
       " 'shorts,',\n",
       " '(new',\n",
       " 'basically,',\n",
       " '$obsv',\n",
       " 'addition',\n",
       " 'themselves',\n",
       " '$mpx,',\n",
       " 'tecfidera',\n",
       " 'watchlist:',\n",
       " 'sale..we',\n",
       " 'see;',\n",
       " 'nvidia',\n",
       " '(opentable)',\n",
       " '$abil',\n",
       " 'traffic',\n",
       " 'doubtful',\n",
       " 'smoking.',\n",
       " 'hate.',\n",
       " 'virtual',\n",
       " 'north.',\n",
       " 'link:',\n",
       " 'shares)',\n",
       " 'hiding',\n",
       " 'renaissance',\n",
       " '%.the',\n",
       " 'beer.',\n",
       " 'true,',\n",
       " 'https://www.youtube.com/watch?v=dbxixxikw',\n",
       " 'night,',\n",
       " 'position?',\n",
       " '$lode',\n",
       " 'at/near',\n",
       " 'genius.',\n",
       " 'wtf?',\n",
       " 'trade',\n",
       " 'h*ll',\n",
       " 'dividend',\n",
       " 'las',\n",
       " '$scon',\n",
       " 'holly',\n",
       " 'well..',\n",
       " 'opposite',\n",
       " '@lizzie',\n",
       " 'massacre',\n",
       " 'http://manyratings.com/mro',\n",
       " 'leveled',\n",
       " 'beat!',\n",
       " 'qrhc',\n",
       " 'innovative',\n",
       " 'ave',\n",
       " 'box',\n",
       " 'store',\n",
       " 'offering?',\n",
       " 'wash',\n",
       " 'promotions',\n",
       " 'throughout',\n",
       " 'earn',\n",
       " '$stmp',\n",
       " '(prts),',\n",
       " 'galgt',\n",
       " ',profit',\n",
       " 'https://www.crunchbase.com/organization/iconix-brand-group',\n",
       " 'bn/yr,',\n",
       " '$mark',\n",
       " 'wanted',\n",
       " 'outpacing',\n",
       " 'cybersecurity',\n",
       " '$aker',\n",
       " 'attention.',\n",
       " 'bash',\n",
       " 'resolving',\n",
       " 'only!?!',\n",
       " 'giggles.',\n",
       " 'upset',\n",
       " 'magnet!',\n",
       " 'end!!',\n",
       " 'craze',\n",
       " 'https://firstoinvest.com/melinta-therapeutics-case-study/',\n",
       " 'tuck',\n",
       " 'dangdang',\n",
       " 'reveal:-)',\n",
       " '-energy',\n",
       " 'chinese',\n",
       " 'incy,',\n",
       " 'portfolios',\n",
       " 'https://free.hadeplatform.com/feed.php',\n",
       " '$fnma',\n",
       " 'rises',\n",
       " '$ccj',\n",
       " 'resistance',\n",
       " 'money~didn&#;t',\n",
       " 'units',\n",
       " 'hear/fear',\n",
       " 'fortunes',\n",
       " '$dps',\n",
       " 'home.',\n",
       " 'inc',\n",
       " 'sense',\n",
       " 'cousin',\n",
       " 'dtl',\n",
       " '$ifon',\n",
       " 'standpoint,',\n",
       " '$rdus',\n",
       " 'wording',\n",
       " '&quot;shorty&quot;',\n",
       " '(semiconductor',\n",
       " 'scsmper...',\n",
       " 'down!!!!',\n",
       " 'e.o.d.',\n",
       " 'stll',\n",
       " 'shrs.extr',\n",
       " 'promissing',\n",
       " 'then.',\n",
       " 'pipeline.',\n",
       " 'ipos',\n",
       " '$fuv',\n",
       " '@april',\n",
       " 'rips',\n",
       " 'meanwhile,',\n",
       " 'things',\n",
       " 'yeee',\n",
       " 'prepare',\n",
       " 'anyway.',\n",
       " 'agreement.',\n",
       " 'brainer',\n",
       " 'jetted',\n",
       " '^buy',\n",
       " 'approved,until',\n",
       " 'india,',\n",
       " '(mlnt)',\n",
       " 'enjoyed',\n",
       " 'exhibit',\n",
       " 'shars',\n",
       " 'pick!',\n",
       " 'fan',\n",
       " 'hope',\n",
       " '!radar',\n",
       " 'mnk',\n",
       " '$robo',\n",
       " 'https://www.sec.gov/archives/edgar/data///m.htm',\n",
       " '..for',\n",
       " 'tutes',\n",
       " 'usd/jpy',\n",
       " 'microcenter',\n",
       " 'warmed',\n",
       " '$vtl',\n",
       " 'vote',\n",
       " 'load',\n",
       " 'dilution.',\n",
       " 'isrg',\n",
       " 'experiment,',\n",
       " '$gpwh',\n",
       " 'plus,',\n",
       " 'deliveries',\n",
       " 'ipad',\n",
       " 'penny.',\n",
       " 'bullish',\n",
       " 'satisfy',\n",
       " '&quot;pennystocks',\n",
       " 'report.',\n",
       " '$...........then',\n",
       " 'arc',\n",
       " '//tvo-market-barometer-',\n",
       " '$bta.ca',\n",
       " 'bigly',\n",
       " 'trades?',\n",
       " 'pullbacks.',\n",
       " 'effect-',\n",
       " 'back,',\n",
       " 'est-...',\n",
       " 'flash',\n",
       " '$btceur',\n",
       " 'highs)',\n",
       " 'upped',\n",
       " '$zsan',\n",
       " 'ii)...last',\n",
       " 'http://artificialinvestor.blogspot.com///leveraged-etfs-looking-out--days-using.html',\n",
       " 'http://bit.ly/zsrpkz',\n",
       " '@months',\n",
       " 'shelf',\n",
       " 'ow.ly/fxgqg',\n",
       " 'corporation,',\n",
       " 'folks......this',\n",
       " '$bdr',\n",
       " '$gdot',\n",
       " '$btcs',\n",
       " '$eman',\n",
       " 'longer.',\n",
       " 'scratched',\n",
       " '$airg',\n",
       " 'boy,',\n",
       " 'msft.',\n",
       " 'threw',\n",
       " '$cleg',\n",
       " 'k-cup',\n",
       " 'follower',\n",
       " 'warriors',\n",
       " 'its',\n",
       " 'thing,',\n",
       " 'http://markets.businessinsider.com/news/interestrates/brazil-lawmaker-in-charge-of-pension-reform-bill-expects-house-vote-by-december-',\n",
       " 'runs.',\n",
       " 'ripple',\n",
       " 'point?',\n",
       " 'closely',\n",
       " '(monday)',\n",
       " 'trucks',\n",
       " '$bioa',\n",
       " 'groups',\n",
       " 'year??',\n",
       " 'catalyst.',\n",
       " 'tomorrow....capisce?',\n",
       " 'overbought',\n",
       " 'raises',\n",
       " 'self',\n",
       " 'banks',\n",
       " 'risky',\n",
       " '$.cents',\n",
       " '$ivv',\n",
       " 'holders!',\n",
       " 'again&quot;.',\n",
       " 'stfr',\n",
       " '$hmc',\n",
       " 'ceiling',\n",
       " 'equities,',\n",
       " 'group',\n",
       " '$kmb',\n",
       " 'aapl,',\n",
       " 'izea',\n",
       " 'tumbled',\n",
       " '(winter',\n",
       " 'gifted',\n",
       " 'only$',\n",
       " 'score',\n",
       " 'q&amp;a',\n",
       " 'aftr=',\n",
       " 'carts',\n",
       " '$cafd',\n",
       " 'receive',\n",
       " 'triangle..',\n",
       " 'btfds',\n",
       " 'necessary,',\n",
       " 'coiled.',\n",
       " 'relative',\n",
       " 'concern',\n",
       " 'here&#;s',\n",
       " 'conferences',\n",
       " 'divvy,',\n",
       " 'integrated',\n",
       " 'reclaiming',\n",
       " 'imo...deutsche',\n",
       " 'proof',\n",
       " 'trading.',\n",
       " 'submitted',\n",
       " 'panic',\n",
       " 'generated',\n",
       " '#stock',\n",
       " '(robotics',\n",
       " '$rjf,',\n",
       " 'something-rather...lol!',\n",
       " 'holidays',\n",
       " 'trash',\n",
       " 'thur.',\n",
       " 'difficult',\n",
       " 'hat',\n",
       " '$ome,$bemo,$rxi,',\n",
       " '(chapter)',\n",
       " 'bath',\n",
       " '&amp;&amp;',\n",
       " 'pension',\n",
       " 'fitzgerald',\n",
       " '$jack',\n",
       " '@stockcat',\n",
       " 'flag,',\n",
       " '#mnkdsecinvestigation',\n",
       " 'worse',\n",
       " 'trump&#;s',\n",
       " 'opioid',\n",
       " '$jmei',\n",
       " 'refundable',\n",
       " 'throwing',\n",
       " 'launched.',\n",
       " '&quot;pump',\n",
       " '$rmti',\n",
       " 'adus-',\n",
       " 'fish',\n",
       " '@cdfriedrich',\n",
       " 'change?',\n",
       " 'more',\n",
       " 'saw',\n",
       " 'plan,',\n",
       " 'apple&#;s',\n",
       " 'repatriation.',\n",
       " 'grandma',\n",
       " 'billion!',\n",
       " 'more?',\n",
       " 'sits',\n",
       " 'tom',\n",
       " 'bank.',\n",
       " 'one!',\n",
       " '$gddy',\n",
       " 'generation',\n",
       " 'best-case',\n",
       " 'forward,',\n",
       " 'pair',\n",
       " 'trade.',\n",
       " 'price!',\n",
       " '$gogl',\n",
       " 'drinking',\n",
       " '$jjg',\n",
       " 'again....let&#;s',\n",
       " 'hig.',\n",
       " '$gsv',\n",
       " 'fuck',\n",
       " 'thx',\n",
       " 'semester',\n",
       " '$gc_f',\n",
       " '$ete',\n",
       " 'blatant',\n",
       " '$mmyt',\n",
       " 'european',\n",
       " '#pattern',\n",
       " 'flagged',\n",
       " 'unpolished',\n",
       " 'perform.',\n",
       " 'facts.',\n",
       " 'programs',\n",
       " 'moment,',\n",
       " 'prs',\n",
       " 'dependent',\n",
       " 'implying',\n",
       " 'buy/selling',\n",
       " 'heard',\n",
       " 'discusses',\n",
       " '$als',\n",
       " 'macys',\n",
       " 'runway',\n",
       " 'indication',\n",
       " 'cl_f',\n",
       " '$tph',\n",
       " '$rvlt',\n",
       " 'https://www.forbes.com/sites/kenkam////rex-jacobsens-target-price-for-nvidia-is-',\n",
       " 'impressed',\n",
       " 'hear',\n",
       " 'same',\n",
       " '@thelonghi',\n",
       " 'amigos',\n",
       " 'etc)',\n",
       " 'genocea',\n",
       " 'include',\n",
       " '$iyt',\n",
       " '$$managers',\n",
       " '$ltc.x',\n",
       " '$amid',\n",
       " 'yep',\n",
       " 'bail',\n",
       " 'relief',\n",
       " '$afg',\n",
       " 'x=ipad,',\n",
       " 'leveraged',\n",
       " 'smas,',\n",
       " 'however.',\n",
       " '$wwe',\n",
       " 'pending.',\n",
       " 'ptn',\n",
       " 'people...that&#;s',\n",
       " 'already...can&#;t',\n",
       " 'invests',\n",
       " '-.html',\n",
       " 'ohh',\n",
       " 'settle....keep',\n",
       " 'declaration',\n",
       " 'heats',\n",
       " 'layer',\n",
       " 'others?',\n",
       " 'downtrend.',\n",
       " 'mccain',\n",
       " '$ipo',\n",
       " '$me.to',\n",
       " 'https://diyquantfund.blogspot.sg///buy-nasdaqluna-luna-innovations.html',\n",
       " 'forum',\n",
       " 'high',\n",
       " '(pump)',\n",
       " 'pan',\n",
       " 'spike!',\n",
       " '$vlo.',\n",
       " 'tinder..',\n",
       " 'cycle',\n",
       " '#homebuilders',\n",
       " '$toca',\n",
       " 'northern',\n",
       " 'around.',\n",
       " 'holidays?',\n",
       " 'jhh',\n",
       " '$osur',\n",
       " 'corrections',\n",
       " 'review',\n",
       " 'interest',\n",
       " 'glbs',\n",
       " '$tlp',\n",
       " 'confirmation,',\n",
       " 'out....',\n",
       " '&gt;ma,',\n",
       " 'hospitals',\n",
       " '$lpl',\n",
       " '$fhn',\n",
       " 'profits,',\n",
       " 'euphoria',\n",
       " 'where',\n",
       " '(oclaro)',\n",
       " 'juego',\n",
       " 'end!',\n",
       " '$fpg',\n",
       " '$ctix',\n",
       " 'measures...',\n",
       " 'https://seekingalpha.com/article/-integrated-biosci-research-protalix-top-biosci-catalyst-',\n",
       " 'marketwatch???',\n",
       " 'tens',\n",
       " 'presents',\n",
       " '$aveo',\n",
       " 'prick',\n",
       " 'hong',\n",
       " 'congress.',\n",
       " 'divi,',\n",
       " 'musk.',\n",
       " 'equity,the',\n",
       " 'trxc',\n",
       " 'lung',\n",
       " 'default.',\n",
       " 'wide',\n",
       " '$gc_f,',\n",
       " 'yet)',\n",
       " 'par',\n",
       " 'sure..',\n",
       " '$ulta',\n",
       " 'lawsuit',\n",
       " 'red,',\n",
       " '$vstm',\n",
       " '$vvus',\n",
       " 'selling,',\n",
       " 'results.here',\n",
       " '$zb_f',\n",
       " 'sell?',\n",
       " '@court',\n",
       " 'q-etsy-fcau-lmb-rkn-trip-evi/',\n",
       " 'genius',\n",
       " 'beautiful',\n",
       " 'https://www.thestreet.com/story///westfield-being-bought-out-by-unibail-rodamco.html',\n",
       " 'two.',\n",
       " '$arwr',\n",
       " '$scco',\n",
       " 'favor.hopefully',\n",
       " '-things-kratos-defense-security-.html',\n",
       " 'investments',\n",
       " 'consolidation.',\n",
       " 'upside',\n",
       " 'cannabis',\n",
       " 'gurus',\n",
       " 'specs,',\n",
       " 'opentable',\n",
       " '$sgyp',\n",
       " 'mining',\n",
       " 'reach',\n",
       " 'feb',\n",
       " 'space?',\n",
       " 'cautious',\n",
       " 'bull',\n",
       " 'sht,',\n",
       " '$sqbg',\n",
       " '$akca',\n",
       " 'fortune&#;s',\n",
       " 'crisis.',\n",
       " 'dependent/foreign',\n",
       " 'reason',\n",
       " 'http://www.mercurynews.com////four-silicon-valley-executives-charged-in-plot-to-steal-semiconductor-technology/',\n",
       " 'notable',\n",
       " '$frsh',\n",
       " 'january.',\n",
       " 'stwits!',\n",
       " 'through,',\n",
       " 'rotation,',\n",
       " 'awesomeness',\n",
       " 'diddle',\n",
       " 'looks',\n",
       " 'ride.',\n",
       " 'yup!',\n",
       " '$gdi',\n",
       " 'wrong!',\n",
       " 'graphic',\n",
       " 'nordstrom.',\n",
       " '(ltl)',\n",
       " 'violent',\n",
       " 'kim',\n",
       " 'snap',\n",
       " '$utmd',\n",
       " 'https://www.yahoo.com/amphtml/finance/news/vuzix-m-programs-top-',\n",
       " 'payment',\n",
       " 'rio',\n",
       " 'released.',\n",
       " 'remind',\n",
       " '&quot;extreme&quot;',\n",
       " 'winning',\n",
       " 'fundamental',\n",
       " 'rip',\n",
       " 'brokers!',\n",
       " 'hodl!!',\n",
       " 'policies',\n",
       " 'http://bit.ly/ghmyw',\n",
       " '$cia,',\n",
       " 'tension',\n",
       " 'wants',\n",
       " 'result',\n",
       " 'gigafactory',\n",
       " 'news?',\n",
       " 'groceries',\n",
       " '$ebio,',\n",
       " 'meal',\n",
       " 'loser',\n",
       " 'gets',\n",
       " 'sales,margins,subs,',\n",
       " 'progress......',\n",
       " 'counted:',\n",
       " 'momentum',\n",
       " 'ramp',\n",
       " 'germans',\n",
       " 'impressive!',\n",
       " 'took',\n",
       " 'https://apps.newyorkfed.org/markets/autorates/tomo-results-display?showmore=true&amp;startdate=//&amp;enddate=//',\n",
       " 'wks,',\n",
       " 'dear',\n",
       " 'along.',\n",
       " 'learned',\n",
       " 'opp',\n",
       " '$dust',\n",
       " 'faces',\n",
       " 'kandi',\n",
       " 'cutting,',\n",
       " 'basket',\n",
       " 'respecting',\n",
       " 'earnest',\n",
       " 'call:',\n",
       " 'someone',\n",
       " '$oncy',\n",
       " 'something!',\n",
       " '$ogi...',\n",
       " 'tnbc',\n",
       " 'read-',\n",
       " 'usually,',\n",
       " 'nation',\n",
       " 'sleeping',\n",
       " 'meaning',\n",
       " 'somebody',\n",
       " 'more&quot;',\n",
       " 'dubious',\n",
       " 'https://qz.com//an-innovative-natural-gas-power-plant-could-be-the-future-of-hurricane-proof-electricity/',\n",
       " 'environment',\n",
       " 'revenue.',\n",
       " '$cvi',\n",
       " 'group)',\n",
       " 'http://stks.co/aab',\n",
       " 'trendy,then',\n",
       " 'outta',\n",
       " 'happen',\n",
       " 'trip',\n",
       " 'www.businessinsider.com/saudi-arabia-gas-prices-',\n",
       " 'gambling,',\n",
       " '$tap',\n",
       " '$glpg',\n",
       " 'good',\n",
       " '$ttwo',\n",
       " 'ugh.',\n",
       " 'catalyst',\n",
       " 'job',\n",
       " 'bang',\n",
       " 'spoiled.',\n",
       " 'fortune',\n",
       " '$vix',\n",
       " 'debt?',\n",
       " 'already.',\n",
       " 'merger/buyer',\n",
       " 'cheap.',\n",
       " '$adms',\n",
       " 'https://seekingalpha.com/pr/-tracon-pharmaceuticals-presents-updated-data-phase-b--study-trc',\n",
       " '-days-using.html',\n",
       " '$djia',\n",
       " 'booked',\n",
       " '$unit',\n",
       " '$djia,',\n",
       " 'holdings.',\n",
       " 'due.',\n",
       " 'states',\n",
       " 'herev',\n",
       " 'lows',\n",
       " 'care',\n",
       " 'salty',\n",
       " 'harvesting?',\n",
       " 'today.',\n",
       " 'ordere',\n",
       " 'jpmorgan,',\n",
       " 'dirty',\n",
       " 'discount',\n",
       " '$rad,',\n",
       " 'analysis@https://goo.gl/crw',\n",
       " 'computing..shouldve',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'trxc,rnn,',\n",
       " '$mict,',\n",
       " 'bank!',\n",
       " 'products&quot;',\n",
       " 'high:',\n",
       " 'affect',\n",
       " 'undervalued',\n",
       " 'cheapies',\n",
       " 'omeros&#;',\n",
       " 'sec?',\n",
       " 'destroy',\n",
       " '$prelf',\n",
       " 'monthly..',\n",
       " 'sub-$',\n",
       " '#cbis',\n",
       " 'tomrrow.',\n",
       " 'ordered',\n",
       " 'buy!',\n",
       " '$jpm',\n",
       " 'wow!',\n",
       " 'preliminarily,',\n",
       " 'place',\n",
       " 'pension.',\n",
       " 'projections.',\n",
       " '$tsro',\n",
       " 'approaching',\n",
       " '$otic',\n",
       " 'switching',\n",
       " '$imh.ca',\n",
       " '-worked',\n",
       " 'countries',\n",
       " 'strongly',\n",
       " 'isr',\n",
       " 'combine',\n",
       " 'surprising.',\n",
       " '@godzillalizard',\n",
       " 'need.',\n",
       " 'scandal',\n",
       " '$else',\n",
       " '$hg_f',\n",
       " 'tells',\n",
       " '$nwl',\n",
       " 'hawk!',\n",
       " '.avg',\n",
       " 'sports',\n",
       " 'vertex',\n",
       " 'early-stage',\n",
       " 'http://www.marketwatch.com/story/',\n",
       " 'cares..',\n",
       " 'minor',\n",
       " 'after-hours....',\n",
       " ',which',\n",
       " 'goodwill',\n",
       " 'old?',\n",
       " 'time',\n",
       " '$rds.a',\n",
       " '$grow,',\n",
       " 'issue',\n",
       " 'famous',\n",
       " 'technlogy.',\n",
       " 'knows.',\n",
       " 'who',\n",
       " 'revs/money.',\n",
       " 'today...',\n",
       " 'revenue',\n",
       " 'only!',\n",
       " 'mover',\n",
       " '$sage.',\n",
       " 'blowout.',\n",
       " '$tmus.',\n",
       " '$$apc',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpyInp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in unique embeddings for <num\\> and filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.vstack((embed, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13109, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layer"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to get embeddingss for these words"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 50,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13107, 100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix for any input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing these unqique words in a dictionary "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 108,
=======
   "execution_count": 52,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.get_dummies(df['cat_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding <num\\> back in "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (numpyInp, labels)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "markdown",
=======
   "execution_count": 53,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "source": [
    "What does enumerate do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 3)\n",
      "(2, 4)\n"
     ]
=======
     "data": {
      "text/plain": [
       "13107"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
    }
   ],
   "source": [
    "filter_sizes = [0,3,4]\n",
    "for i in enumerate(filter_sizes):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "metadata": {},
   "source": [
    "Adding filler word to keep all tweets the same length. Then replacing all words with their dictionary equivalent. This is for tenssorflows matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "source": [
    "Failed attempt using predefined filter\n",
    "Update: Works now, not using tho"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 55,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WINDOW_SIZE = 100\n",
    "STRIDE = int(WINDOW_SIZE/2)\n",
    "#embedding2 = tf.expand_dims(embedding, axis = 1)\n",
    "conv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \n",
    "               strides=1, padding='SAME') \n",
    "conv = tf.nn.relu(conv)   \n",
    "words = flatten(conv)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 56,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 26, 100) dtype=float64>"
      ]
     },
     "execution_count": 31,
=======
       "array([[10826,  9214, 13107, ..., 13108, 13108, 13108],\n",
       "       [12526,  5153,  3347, ..., 13108, 13108, 13108],\n",
       "       [ 1416, 12376,  4108, ..., 13108, 13108, 13108],\n",
       "       ...,\n",
       "       [  849, 10790,  4043, ..., 13108, 13108, 13108],\n",
       "       [ 7649,  9443,  4842, ..., 13108, 13108, 13108],\n",
       "       [13107,  7649,  9443, ..., 13108, 13108, 13108]])"
      ]
     },
     "execution_count": 56,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 26, 100, 1) dtype=float64>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in unique embeddings for <num\\> and filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [2, 3, 5]\n",
    "embedding_size = 100\n",
    "num_filters = 2\n",
    "max_length = 26\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W,tf.float64),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, tf.cast(b,tf.float64)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu,\n",
    "        ksize=[1, max_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 58,
   "metadata": {},
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[<tf.Tensor 'pool:0' shape=(?, 1, 1, 2) dtype=float64>,\n",
       " <tf.Tensor 'pool_1:0' shape=(?, 1, 1, 2) dtype=float64>,\n",
       " <tf.Tensor 'pool_2:0' shape=(?, 1, 1, 2) dtype=float64>]"
      ]
     },
     "execution_count": 93,
=======
       "(13109, 100)"
      ]
     },
     "execution_count": 58,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 6) dtype=float64>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layer"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 115,
=======
   "execution_count": 59,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tf.layers.dense(combined_flat, 100, activation = 'relu')\n",
    "conn2 = tf.layers.dense(conn, len(set(df.cat_num)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 116,
=======
   "execution_count": 60,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Relu:0' shape=(?, 100) dtype=float64>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix for any input\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = conn2)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
=======
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(conn2)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
=======
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (numpyInp, df['cat_num'])\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 151,
=======
   "execution_count": 80,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does enumerate do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.8918759177186593\n",
      "Average accuracy 0: 0.40535714285714286\n",
      "Average loss epoch 1: 1.4448999256021\n",
      "Average accuracy 1: 0.44107142857142856\n",
      "Average loss epoch 2: 1.3877455143136561\n",
      "Average accuracy 2: 0.4662202380952381\n",
      "Average loss epoch 3: 1.3331219561384777\n",
      "Average accuracy 3: 0.4816964285714286\n",
      "Average loss epoch 4: 1.287438232884526\n",
      "Average accuracy 4: 0.49657738095238096\n",
      "Average loss epoch 5: 1.2448388037318345\n",
      "Average accuracy 5: 0.5148809523809523\n",
      "Average loss epoch 6: 1.1989496318891915\n",
      "Average accuracy 6: 0.5446428571428571\n",
      "Average loss epoch 7: 1.1683561015490522\n",
      "Average accuracy 7: 0.5572916666666666\n",
      "Average loss epoch 8: 1.1445011464997248\n",
      "Average accuracy 8: 0.555952380952381\n",
      "Average loss epoch 9: 1.1222883953033431\n",
      "Average accuracy 9: 0.5678571428571428\n",
      "Average loss epoch 10: 1.1030813209136456\n",
      "Average accuracy 10: 0.577827380952381\n",
      "Average loss epoch 11: 1.0839651231061516\n",
      "Average accuracy 11: 0.5919642857142857\n",
      "Average loss epoch 12: 1.066617419801618\n",
      "Average accuracy 12: 0.5922619047619048\n",
      "Average loss epoch 13: 1.0531140694014878\n",
      "Average accuracy 13: 0.5986607142857143\n",
      "Average loss epoch 14: 1.0316889353276268\n",
      "Average accuracy 14: 0.6144345238095238\n",
      "Average loss epoch 15: 1.0263957791652318\n",
      "Average accuracy 15: 0.6078869047619048\n",
      "Average loss epoch 16: 1.008980995963534\n",
      "Average accuracy 16: 0.6223214285714286\n",
      "Average loss epoch 17: 0.9958700570332106\n",
      "Average accuracy 17: 0.6293154761904762\n",
      "Average loss epoch 18: 0.984319140251745\n",
      "Average accuracy 18: 0.6313988095238096\n",
      "Average loss epoch 19: 0.9812934933440453\n",
      "Average accuracy 19: 0.6287202380952381\n",
      "Average loss epoch 20: 0.9645202269242805\n",
      "Average accuracy 20: 0.643452380952381\n",
      "Average loss epoch 21: 0.9633537132309554\n",
      "Average accuracy 21: 0.6369047619047619\n",
      "Average loss epoch 22: 0.9541203247178139\n",
      "Average accuracy 22: 0.6422619047619048\n",
      "Average loss epoch 23: 0.9458645729557088\n",
      "Average accuracy 23: 0.6520833333333333\n",
      "Average loss epoch 24: 0.9501036742982196\n",
      "Average accuracy 24: 0.6464285714285715\n",
      "Average loss epoch 25: 0.9335041786476722\n",
      "Average accuracy 25: 0.6526785714285714\n",
      "Average loss epoch 26: 0.9223789109398897\n",
      "Average accuracy 26: 0.6510416666666666\n",
      "Average loss epoch 27: 0.9202392873243713\n",
      "Average accuracy 27: 0.659375\n",
      "Average loss epoch 28: 0.9090766054530847\n",
      "Average accuracy 28: 0.6638392857142857\n",
      "Average loss epoch 29: 0.9113229404866712\n",
      "Average accuracy 29: 0.6650297619047619\n",
      "Average loss epoch 30: 0.9039870176713376\n",
      "Average accuracy 30: 0.6674107142857143\n",
      "Average loss epoch 31: 0.9044658287602061\n",
      "Average accuracy 31: 0.6681547619047619\n",
      "Average loss epoch 32: 0.8905262381458091\n",
      "Average accuracy 32: 0.6697916666666667\n",
      "Average loss epoch 33: 0.8803860050857792\n",
      "Average accuracy 33: 0.6724702380952381\n",
      "Average loss epoch 34: 0.8720691973329326\n",
      "Average accuracy 34: 0.6742559523809524\n",
      "Average loss epoch 35: 0.880238346123151\n",
      "Average accuracy 35: 0.6674107142857143\n",
      "Average loss epoch 36: 0.8655131133188075\n",
      "Average accuracy 36: 0.68125\n",
      "Average loss epoch 37: 0.8649477479211418\n",
      "Average accuracy 37: 0.68125\n",
      "Average loss epoch 38: 0.8641463455858255\n",
      "Average accuracy 38: 0.680952380952381\n",
      "Average loss epoch 39: 0.858203622371233\n",
      "Average accuracy 39: 0.6860119047619048\n",
      "Average loss epoch 40: 0.8525763799401267\n",
      "Average accuracy 40: 0.6837797619047619\n",
      "Average loss epoch 41: 0.8455229293695358\n",
      "Average accuracy 41: 0.6866071428571429\n",
      "Average loss epoch 42: 0.8447356910414513\n",
      "Average accuracy 42: 0.6866071428571429\n",
      "Average loss epoch 43: 0.8394918994811054\n",
      "Average accuracy 43: 0.6907738095238095\n",
      "Average loss epoch 44: 0.8414417048007475\n",
      "Average accuracy 44: 0.6941964285714286\n",
      "Average loss epoch 45: 0.8303253357004245\n",
      "Average accuracy 45: 0.6915178571428572\n",
      "Average loss epoch 46: 0.8329033948974904\n",
      "Average accuracy 46: 0.6879464285714286\n",
      "Average loss epoch 47: 0.8214419614481338\n",
      "Average accuracy 47: 0.6980654761904762\n",
      "Average loss epoch 48: 0.823716479094961\n",
      "Average accuracy 48: 0.6971726190476191\n",
      "Average loss epoch 49: 0.8091884260714467\n",
      "Average accuracy 49: 0.7035714285714286\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Average accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "    prediction = sess.run(preds, feed_dict={txt: numpyInp})\n",
    "    prediction = np.asarray(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
=======
   "metadata": {},
   "source": [
    "Failed attempt using predefined filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WINDOW_SIZE = 100\\nSTRIDE = int(WINDOW_SIZE/2)\\n#embedding = tf.reshape(embedding, [53,1,26,100])\\n#conv = tf.layers.conv2d(embedding, 2, [2,WINDOW_SIZE], \\n#               strides=1, padding='SAME') \\nfilter1 = np.array([1,2,100]).astype(np.float64)\\nconv = tf.nn.conv2d(embedded_chars_expanded, [1,2,100,1], strides = [1,2,2,1], padding = 'SAME')\\nconv = tf.nn.relu(conv)   \\nwords = tf.squeeze(conv, [2]) \""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''WINDOW_SIZE = 100\n",
    "STRIDE = int(WINDOW_SIZE/2)\n",
    "#embedding = tf.reshape(embedding, [53,1,26,100])\n",
    "#conv = tf.layers.conv2d(embedding, 2, [2,WINDOW_SIZE], \n",
    "#               strides=1, padding='SAME') \n",
    "filter1 = np.array([1,2,100]).astype(np.float64)\n",
    "conv = tf.nn.conv2d(embedded_chars_expanded, [1,2,100,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "conv = tf.nn.relu(conv)   \n",
    "words = tf.squeeze(conv, [2]) '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 2\n",
    "embedding_size = 100\n",
    "num_filters = 2\n",
    "filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "conv = tf.nn.conv2d(\n",
    "    embedded_chars_expanded,\n",
    "    tf.cast(W,tf.float64),\n",
    "    strides=[1, 1, 1, 1],\n",
    "    padding='VALID',\n",
    "    name='conv')\n",
    "conv = tf.nn.relu(conv)   \n",
    "words = tf.squeeze(conv, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = tf.reshape(t, [1, -1])\n",
    "    t = tf.squeeze(t)\n",
    "    return t"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_preds = np.equal(np.argmax(prediction, 1), labels.idxmax(axis = 1))\n",
    "acc = np.mean(out_preds)"
=======
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = flatten(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_3:0' shape=(?, 25, 2) dtype=float64>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 153,
=======
   "execution_count": 90,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.7272591708917387"
      ]
     },
     "execution_count": 153,
=======
       "<tf.Tensor 'Relu_3:0' shape=(?, 25, 1, 2) dtype=float64>"
      ]
     },
     "execution_count": 90,
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fooling around with numpy"
=======
    "conv"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n",
      "(9, 2)\n",
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "f = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "#f = np.array([[1,2,3], [1,2,3]])\n",
    "#print(f)\n",
    "g = np.expand_dims(f, axis = 1)\n",
    "g = np.reshape(f, [-1,2])\n",
    "print(g)\n",
    "print(g.shape)\n",
    "print(np.squeeze(g))"
=======
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_4:0' shape=<unknown> dtype=float64>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened"
>>>>>>> 41852ad2389710bf35bbb31e506c1be0f9ad47fb
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
