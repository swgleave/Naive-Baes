{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx</th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>target_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98221616</td>\n",
       "      <td>4976</td>\n",
       "      <td>$ARNA APD334 for Amyotrophic Lateral Sclerosis...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82321187</td>\n",
       "      <td>9839</td>\n",
       "      <td>$OCLR Noob investor that i am, put a 7.38 stop...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>stop loss</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103328840</td>\n",
       "      <td>1455</td>\n",
       "      <td>$ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104840294</td>\n",
       "      <td>1111</td>\n",
       "      <td>$TMUS its acquisition of Layer3 TV The purchas...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>relative</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>forecast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94249158</td>\n",
       "      <td>1372</td>\n",
       "      <td>$SEED L2 Capital deal is real savvy. It takes ...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100979260</td>\n",
       "      <td>505</td>\n",
       "      <td>$BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100775772</td>\n",
       "      <td>1210</td>\n",
       "      <td>$WRN My fav $WRN pattern on my watchlist for 1...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   idx                                              tweet  \\\n",
       "0   98221616  4976  $ARNA APD334 for Amyotrophic Lateral Sclerosis...   \n",
       "1   82321187  9839  $OCLR Noob investor that i am, put a 7.38 stop...   \n",
       "2  103328840  1455  $ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...   \n",
       "3  104840294  1111  $TMUS its acquisition of Layer3 TV The purchas...   \n",
       "4   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "5   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "6   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "7   94249158  1372  $SEED L2 Capital deal is real savvy. It takes ...   \n",
       "8  100979260   505  $BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...   \n",
       "9  100775772  1210  $WRN My fav $WRN pattern on my watchlist for 1...   \n",
       "\n",
       "         category     subcategory target_num  \n",
       "0  Product Number  Product Number        334  \n",
       "1        Monetary       stop loss       7.38  \n",
       "2  Product Number  Product Number          1  \n",
       "3  Product Number  Product Number          5  \n",
       "4      Percentage        relative         14  \n",
       "5        Quantity        Quantity          4  \n",
       "6        Monetary        forecast          5  \n",
       "7        Temporal            date         33  \n",
       "8        Temporal            date          4  \n",
       "9        Temporal            date         11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('finnum/train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for encoding categories as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_num'] = df['category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load('Gensim FastText Vectors/fastText1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing tweets by lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = [x.lower() for x in df.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing target with <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to training, making this into a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mod'] = df.apply(lambda x: flagNum(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd <num>  for amyotrophic lateral scler...\n",
       "1    $oclr noob investor that i am, put a  <num>  s...\n",
       "2    $es_f $spy bias-2 bearish and the dlt- <num>  ...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up  <num> %  4 time avg ...\n",
       "Name: mod, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mod'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out numbers and words less than 3 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped = [textPuncandNum(text) for text in df['mod']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of unique words from this processed text, excluding <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13107"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = list(stripped)\n",
    "low = ' '.join(low)\n",
    "low = list(set(low.split()))\n",
    "low.remove('<num>')\n",
    "len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space',\n",
       " 'happens,',\n",
       " 'variants',\n",
       " 'lows',\n",
       " 'ebay',\n",
       " 'jhh',\n",
       " '.when',\n",
       " 'breakeven',\n",
       " 'two',\n",
       " 'impact',\n",
       " 'daughter',\n",
       " 'vetr',\n",
       " 'rode',\n",
       " 'letter:',\n",
       " 'david',\n",
       " 'week&#;s',\n",
       " 'makes',\n",
       " 'whatnot.',\n",
       " 'dilute',\n",
       " 'tanked',\n",
       " 'fut,',\n",
       " '$mitk',\n",
       " 'coco',\n",
       " 'hello?',\n",
       " 'doc',\n",
       " 'deposited',\n",
       " '$cei',\n",
       " '$cmcsa',\n",
       " 'turd',\n",
       " '$hsgx',\n",
       " 'rebound!',\n",
       " '$vstm',\n",
       " 'plunger',\n",
       " 'break.',\n",
       " 'ascent',\n",
       " 'charted,',\n",
       " 'deal$',\n",
       " 'swings.',\n",
       " 'soldiers',\n",
       " 'blackjack',\n",
       " 'rent.',\n",
       " 'scratched',\n",
       " 'pos',\n",
       " 'multiply',\n",
       " 'pt,the',\n",
       " 'somebody',\n",
       " 'stocktwits',\n",
       " 'show',\n",
       " 'con,',\n",
       " 'algo.',\n",
       " '@sardines',\n",
       " 'texas',\n",
       " '$waves.x',\n",
       " '$optt',\n",
       " 'weeks!!',\n",
       " 'shorter&#;s',\n",
       " 'soon.',\n",
       " '$fosl',\n",
       " 'olympics',\n",
       " 'successfully',\n",
       " 'ets.',\n",
       " 'impaired',\n",
       " 'stock!',\n",
       " 'morgan',\n",
       " '(prts)',\n",
       " '$syn',\n",
       " 'rally.',\n",
       " '$rdd.x',\n",
       " 'then',\n",
       " 'looming.',\n",
       " 'more?',\n",
       " 'drag,',\n",
       " 'pharma',\n",
       " '$kc_f',\n",
       " 'basket:',\n",
       " 'evil',\n",
       " '$pey.ca',\n",
       " '$...........then',\n",
       " 'wash',\n",
       " 'naive',\n",
       " 'bit.',\n",
       " 'contact,',\n",
       " 'lol....here',\n",
       " 'channel.',\n",
       " '$wdr',\n",
       " 'relief.',\n",
       " 'rarely',\n",
       " '$msft',\n",
       " 'pushed',\n",
       " 'nov-',\n",
       " 'spike',\n",
       " '$xlu',\n",
       " 'sexually',\n",
       " 'ceiling',\n",
       " 'mon',\n",
       " '..$amd',\n",
       " 'breached.',\n",
       " 'cash..',\n",
       " 'profit',\n",
       " 'recovers',\n",
       " 'four',\n",
       " 'van',\n",
       " 'er,this',\n",
       " 'signal...',\n",
       " 'yrs.',\n",
       " 'trauma',\n",
       " 'such',\n",
       " 'still,',\n",
       " 'eoy..',\n",
       " 'worrying,',\n",
       " '$bidu',\n",
       " 'perform.',\n",
       " 'levels',\n",
       " 'biotech',\n",
       " 'mania',\n",
       " 'amaz',\n",
       " 'point?',\n",
       " '$mbi',\n",
       " 'philip',\n",
       " 'two-month',\n",
       " 'non-earning',\n",
       " 'unblocked',\n",
       " 'commentary.',\n",
       " 'upside!',\n",
       " 'sply?',\n",
       " '$sohu',\n",
       " 'labu',\n",
       " '$vips,',\n",
       " 'worry,',\n",
       " 'taylor',\n",
       " 'iphone.',\n",
       " 'ah&#;s',\n",
       " 'mgti',\n",
       " 'occurs',\n",
       " '$xauusd',\n",
       " 'fears',\n",
       " 'jewel=lots',\n",
       " 'horrible',\n",
       " 'unspoken',\n",
       " '$car',\n",
       " '$aqn',\n",
       " 'driver?',\n",
       " 'experiences.',\n",
       " 'https://goo.gl/aidphr',\n",
       " 'rule&#;--',\n",
       " '#pmcb,',\n",
       " '(tomorrow)',\n",
       " 'cnn!',\n",
       " 'nvdia',\n",
       " '$nvcr',\n",
       " 'sell?',\n",
       " 'back-in',\n",
       " 'gds?',\n",
       " 'want',\n",
       " 'throwing',\n",
       " 'cresy',\n",
       " 'math.',\n",
       " 'viekira',\n",
       " 'predictable',\n",
       " '.avg.',\n",
       " 'life',\n",
       " 'alone',\n",
       " '$tsg',\n",
       " '$rsge',\n",
       " 'shorter',\n",
       " 'aspen',\n",
       " 'pig',\n",
       " 'lovein.',\n",
       " 'charts,',\n",
       " 'crisis',\n",
       " 'programs',\n",
       " 'kaiser:',\n",
       " 'mcfly,',\n",
       " '$nfx',\n",
       " '$gogl',\n",
       " 'spells',\n",
       " 'trading',\n",
       " 'pony',\n",
       " 'playing,will',\n",
       " 'uptrend.',\n",
       " 'what/',\n",
       " 'distribution',\n",
       " 'played',\n",
       " 'turnaround.',\n",
       " 'mates.',\n",
       " '$faz',\n",
       " '$incy',\n",
       " '$tsri',\n",
       " 'amat',\n",
       " '$./share',\n",
       " 'ron',\n",
       " '$esv',\n",
       " 'als',\n",
       " 'bristol-meyers',\n",
       " 'predicted,',\n",
       " 'topic',\n",
       " 'https://youtu.be/eaubcuw',\n",
       " 'filled',\n",
       " 'sweat!',\n",
       " 'secondary',\n",
       " '$rig',\n",
       " 'eurgbp',\n",
       " 'output',\n",
       " 'scalp,',\n",
       " 'dip,',\n",
       " 'builds',\n",
       " 'loooaded',\n",
       " 'eglt',\n",
       " 'damp',\n",
       " 'inventory?',\n",
       " 'accumulates',\n",
       " '#amfe',\n",
       " '$htnm',\n",
       " 'talk;',\n",
       " 'people,',\n",
       " 'profits!',\n",
       " 'bubble.',\n",
       " 'congratulations',\n",
       " '@biggercapital',\n",
       " 'order?',\n",
       " 'results',\n",
       " 'www.businessinsider.com/saudi-arabia-gas-prices--',\n",
       " 'sub-$,',\n",
       " '@suhdude',\n",
       " '@lexephraim',\n",
       " 'resubmission.',\n",
       " 'nack',\n",
       " 'crytocurrency',\n",
       " 'isolation',\n",
       " 'mnkd',\n",
       " 'rates.',\n",
       " 'undervalued!',\n",
       " '%.&quot;',\n",
       " 'rip...',\n",
       " 'https://www.biopharmcatalyst.com/news//ash-biotech-stocks-on-watch-week-in-review-november--',\n",
       " 'rio',\n",
       " '$gbsn',\n",
       " 'faded',\n",
       " '$okta',\n",
       " 'flipflopped',\n",
       " 'eyal',\n",
       " 'microcap',\n",
       " 'will,',\n",
       " 'myo',\n",
       " 'blockbuster',\n",
       " 'love',\n",
       " '$izea',\n",
       " '-earnings-release',\n",
       " 'transactions',\n",
       " 'averages',\n",
       " 'weekend,',\n",
       " 'shenzhen',\n",
       " '#etfs',\n",
       " 'dream,',\n",
       " 'ratio',\n",
       " 'incy',\n",
       " 'crashes',\n",
       " '@liutrades',\n",
       " 'already?',\n",
       " '#hsi',\n",
       " 'temporary',\n",
       " 'upped',\n",
       " 'search',\n",
       " 'mil,',\n",
       " 'full',\n",
       " 'recession',\n",
       " 'treat',\n",
       " 'staple',\n",
       " 'earn',\n",
       " 'slightly',\n",
       " '$kmb',\n",
       " 'martini',\n",
       " 'cap,',\n",
       " 'run.almost',\n",
       " '$ssw',\n",
       " 'brings.',\n",
       " 'hand.....lmao!',\n",
       " '$pzrx',\n",
       " '$pacd',\n",
       " 'sting',\n",
       " 'correlation',\n",
       " 'hrs.',\n",
       " '$panl',\n",
       " '$flt',\n",
       " 'off!!',\n",
       " 'popular',\n",
       " 'hitched',\n",
       " 'apr',\n",
       " 'gpu',\n",
       " 'pre-orders)...',\n",
       " '$ifon',\n",
       " 'verizon',\n",
       " 'fresh',\n",
       " 'http://fortune.com////tencent-just-reminded-us-what-a-blowout-tech-ipo-looks-like/',\n",
       " 'intervene.',\n",
       " 'ideas?',\n",
       " 'burst.',\n",
       " 'pip',\n",
       " 'consecutive',\n",
       " '-days-using.html',\n",
       " 'synergies',\n",
       " ',rest',\n",
       " 'res',\n",
       " 'longs!',\n",
       " 'leaned',\n",
       " '$cnsl',\n",
       " 'also',\n",
       " 'higher,on',\n",
       " '$hvn.x',\n",
       " '$blfs',\n",
       " '$abt',\n",
       " 'ends',\n",
       " 'value-focused',\n",
       " 'adding!',\n",
       " 'own)...by',\n",
       " 'purchases',\n",
       " 'years..',\n",
       " 'viewing',\n",
       " '&quot;buying',\n",
       " 'reviewing',\n",
       " 'nws.',\n",
       " 'sucker',\n",
       " 'started',\n",
       " 'that&#;s',\n",
       " 'expose',\n",
       " 'hardcore',\n",
       " 'sell,',\n",
       " 'again....',\n",
       " '$qtrh',\n",
       " 'rd-party',\n",
       " 'period',\n",
       " '$oclr',\n",
       " '(hrs',\n",
       " 'her',\n",
       " 'straight',\n",
       " '$gogo',\n",
       " 'noticed.',\n",
       " 'q-etsy-fcau-lmb-rkn-trip-evi/',\n",
       " 'https://finance.yahoo.com/news/credit-suisse-ag-announces-reverse-.html',\n",
       " 'microled',\n",
       " '$evh',\n",
       " 'tinder..',\n",
       " 'they',\n",
       " 'china.',\n",
       " 'sensor',\n",
       " 'karma',\n",
       " 'repricing',\n",
       " 'suppose.',\n",
       " 'recovery',\n",
       " '#earnings',\n",
       " 'knock',\n",
       " '...ends',\n",
       " '@da_cheif',\n",
       " 'downgrade.',\n",
       " 'breaks?',\n",
       " 'calls)',\n",
       " 'shouting',\n",
       " 'pllbk,',\n",
       " 'targets',\n",
       " '&#;sell',\n",
       " 'harvest',\n",
       " 'dec.',\n",
       " 'rollin',\n",
       " 'continuation',\n",
       " '$nep',\n",
       " 'restricted',\n",
       " '..just',\n",
       " '$bbg',\n",
       " 'press',\n",
       " '$veco',\n",
       " 'growing,',\n",
       " 'totally',\n",
       " 'businesses',\n",
       " 'myron',\n",
       " 'instrument',\n",
       " 'imo-',\n",
       " 'settled',\n",
       " 'share,when',\n",
       " 'dealer,',\n",
       " 'drawdown',\n",
       " 'tie',\n",
       " 'mid-next',\n",
       " 'week!',\n",
       " 'predicts',\n",
       " 'capex',\n",
       " 'away...',\n",
       " 'resistance,',\n",
       " 'pumpers',\n",
       " 'morning!!',\n",
       " 'knows',\n",
       " '$srg',\n",
       " '..game',\n",
       " '-contradictions-plaguing-the-industry-peak-demand-really/',\n",
       " 'during',\n",
       " 'trash!',\n",
       " '$idt',\n",
       " 'corporate',\n",
       " 'aphria',\n",
       " 'more',\n",
       " 'toss',\n",
       " '$tlgt',\n",
       " '$lnco',\n",
       " 'blog',\n",
       " 'technologies',\n",
       " '-programs-top--.html',\n",
       " '$wtw',\n",
       " 'fri.',\n",
       " '$vicl',\n",
       " 'eagles,',\n",
       " 'foreign',\n",
       " '$hii',\n",
       " 'flag',\n",
       " '$camp',\n",
       " 'ending',\n",
       " '(rate',\n",
       " 'imageshawnhollenbach',\n",
       " 'said,',\n",
       " '$bery',\n",
       " 'expectation.',\n",
       " 'you&#;ll',\n",
       " 'half',\n",
       " 'change',\n",
       " 'bonus.',\n",
       " 'brent',\n",
       " 'confident...',\n",
       " 'great!',\n",
       " 'rectangle',\n",
       " '$gds',\n",
       " 'reports.',\n",
       " 'pill',\n",
       " 'fighting',\n",
       " 'mobile-app',\n",
       " 'http://performance.morningstar.com/stock/performance-return.action?p=dividend_split_page&amp;t=t&amp;region=usa&amp;culture=en-us',\n",
       " 'note:the',\n",
       " 'opps.',\n",
       " '%.....almost',\n",
       " 'crazy',\n",
       " '$etc.x',\n",
       " 'offs..',\n",
       " 'hangover.',\n",
       " 'loyal',\n",
       " 'investors,',\n",
       " 'accepted',\n",
       " 'commodity',\n",
       " 'takes,',\n",
       " 'unsure',\n",
       " 'combo',\n",
       " 'breakdown',\n",
       " 'should...bought',\n",
       " 'valeant',\n",
       " 'action',\n",
       " '$glnnf',\n",
       " 'kidney',\n",
       " 'afternoon...thought',\n",
       " 'month!',\n",
       " 'space.',\n",
       " '(monday)',\n",
       " 'bzun',\n",
       " 'who',\n",
       " 'slr',\n",
       " 'talked',\n",
       " 'insts',\n",
       " '$mtbc',\n",
       " 'https://www.youtube.com/watch?v=rcpkrdpeuu',\n",
       " 'out!!!',\n",
       " 'economy',\n",
       " '$glng',\n",
       " 'pdufa',\n",
       " 'ownership',\n",
       " 'ferrari&#;s',\n",
       " 'bounce?',\n",
       " 'correction',\n",
       " 'thailand',\n",
       " 'exhausted.',\n",
       " 'buyback',\n",
       " 'ain&#;t',\n",
       " 'eps',\n",
       " 'saying',\n",
       " 'ish',\n",
       " 'properly',\n",
       " 'avgs.',\n",
       " 'downloads.',\n",
       " 'symbols',\n",
       " 'regrest',\n",
       " 'nicely.',\n",
       " 'rebalancing',\n",
       " '$cie',\n",
       " '$mrvl.',\n",
       " 'non',\n",
       " 'in-line,',\n",
       " '$gme.',\n",
       " 'ore',\n",
       " 'sound?',\n",
       " 'reputable!',\n",
       " 'oprah',\n",
       " 'listing',\n",
       " 'rebound',\n",
       " 'criminals',\n",
       " '@robertmoreira',\n",
       " 'concert',\n",
       " 'got',\n",
       " 'litecoin.',\n",
       " 'richard',\n",
       " 'alexa',\n",
       " '$jets',\n",
       " 'coordinated',\n",
       " '$algn',\n",
       " 'lmao',\n",
       " 'stooges',\n",
       " '#gogo',\n",
       " 'current-',\n",
       " 'beauty,',\n",
       " '@kingcrypto',\n",
       " '/bbl.',\n",
       " 'penetrated',\n",
       " 'month',\n",
       " 'kept',\n",
       " 'partnerships',\n",
       " '$exas',\n",
       " 'interview',\n",
       " 'blood',\n",
       " 'brokers!',\n",
       " 'quicker',\n",
       " 'desperate.',\n",
       " 'almost',\n",
       " 'and/or',\n",
       " 'broadcom',\n",
       " 'scootch',\n",
       " 'translate',\n",
       " 'confident',\n",
       " 're-enter',\n",
       " '$eglt,',\n",
       " 'idiot.',\n",
       " '$twou',\n",
       " 'yours.',\n",
       " 'breakout..',\n",
       " '$spx',\n",
       " 'upon',\n",
       " 'secondary.',\n",
       " 'approx',\n",
       " 'following:',\n",
       " 'ev/rev',\n",
       " 'ahr&#;s',\n",
       " 'again.',\n",
       " 'memes.',\n",
       " 'hotel',\n",
       " 'overtook',\n",
       " 'catalyzed',\n",
       " 'mine.',\n",
       " 'bailout.',\n",
       " 'mistakes',\n",
       " 'finalized',\n",
       " 'ruy',\n",
       " '-lrx',\n",
       " 'wedge.',\n",
       " '$rick',\n",
       " 'profit!',\n",
       " 'hint?',\n",
       " 'impeach**',\n",
       " 'lot',\n",
       " 'pop!',\n",
       " '$ryb',\n",
       " 'premium.',\n",
       " '&quot;hold&quot;',\n",
       " 'techs',\n",
       " 'rich,',\n",
       " 'gilead',\n",
       " '$ions',\n",
       " 'er/cc',\n",
       " 'msg.',\n",
       " 'zone!',\n",
       " 'warren',\n",
       " 'shorts..',\n",
       " 'silver',\n",
       " 'kill',\n",
       " 'fuel',\n",
       " 'damn!',\n",
       " 'believe.',\n",
       " '$kite',\n",
       " 'teens/$s',\n",
       " 'investment;',\n",
       " 'gltu',\n",
       " 'ulcerative',\n",
       " 'wake',\n",
       " '$xlv',\n",
       " 'tech.',\n",
       " 'retailer',\n",
       " 'deal.',\n",
       " 'range',\n",
       " 'exclusively',\n",
       " 'read-',\n",
       " 'miller',\n",
       " 'morning....k',\n",
       " 'ormp',\n",
       " 'first.',\n",
       " 'hour',\n",
       " '$indlmeter',\n",
       " 'come',\n",
       " 'again!',\n",
       " 'dcth',\n",
       " '$b...bidu',\n",
       " 'warm',\n",
       " 'halted',\n",
       " 'losses',\n",
       " '$line',\n",
       " 'rounds.',\n",
       " '$extr',\n",
       " 'reason.',\n",
       " 'rich',\n",
       " 'advance',\n",
       " 'moose',\n",
       " '$eth',\n",
       " '@curioustock',\n",
       " '$etrm',\n",
       " 'feedback',\n",
       " '#amd',\n",
       " 'ignored',\n",
       " '$nee',\n",
       " 'https://finance.yahoo.com/news/madalena-announces-investor-conference-call-.html?soc_src=social-sh&amp;soc_trk=tw',\n",
       " 'dorkey',\n",
       " 'ticketops',\n",
       " '@cjgross',\n",
       " 'way',\n",
       " 'tremendous',\n",
       " 'contacting',\n",
       " 'shown',\n",
       " 'thoughts',\n",
       " 'relax',\n",
       " '$tou.ca',\n",
       " 'debacle.',\n",
       " 'international',\n",
       " 'thankful',\n",
       " '$pgh',\n",
       " 'myopic/stupid',\n",
       " 'guessing',\n",
       " 'playin',\n",
       " 'ethereum',\n",
       " 'jetted',\n",
       " '$rsys',\n",
       " '#trendfollowing',\n",
       " '$rkda',\n",
       " 'levie',\n",
       " 'tracking',\n",
       " 'launch',\n",
       " 'inst',\n",
       " 'best',\n",
       " 'eia',\n",
       " 'slowing',\n",
       " '/was',\n",
       " '$ptx',\n",
       " 'exposure,',\n",
       " 'surge',\n",
       " 'scoped',\n",
       " '$trtn',\n",
       " '$$cash',\n",
       " 'roi',\n",
       " 'volanesorsen',\n",
       " 'year-end',\n",
       " 'because',\n",
       " 'fool',\n",
       " 'shares..cant',\n",
       " 'now,',\n",
       " '$bne.ca',\n",
       " 'rotation',\n",
       " '$bkbef',\n",
       " 'sncr',\n",
       " '/tvo-market-barometer--',\n",
       " 'lesson',\n",
       " 'cut',\n",
       " 'shazam.',\n",
       " 'ema.',\n",
       " 'mannkind',\n",
       " 'exploded',\n",
       " 'process',\n",
       " 'inertia',\n",
       " 'virtue,patience',\n",
       " 'nailing',\n",
       " 'fortunes',\n",
       " 'difficulty',\n",
       " '$teck',\n",
       " 'detail',\n",
       " 'proxy.',\n",
       " 'actelion',\n",
       " 'will',\n",
       " 'blind',\n",
       " 'snap.',\n",
       " 'info.',\n",
       " 'printers',\n",
       " '$umc',\n",
       " '&quot;premium&quot;',\n",
       " 'campaign',\n",
       " 'q-contradictions-plaguing-the-industry-peak-demand-really/',\n",
       " 'that?',\n",
       " 'reportedly',\n",
       " '$bhge',\n",
       " '$xxii.',\n",
       " 'scurrying',\n",
       " '@rocklaunches',\n",
       " '@ericvondr',\n",
       " 'track',\n",
       " '$mgm',\n",
       " 'right.',\n",
       " 'almost;',\n",
       " 'hype,',\n",
       " '$airi',\n",
       " 'within',\n",
       " 'teens',\n",
       " 'child',\n",
       " 'fibo',\n",
       " 'inventories&quot;',\n",
       " 'reasons',\n",
       " 'ops',\n",
       " 'day',\n",
       " 'simultaneously.',\n",
       " 'ira',\n",
       " 'bargain',\n",
       " 'spark',\n",
       " 'cuts?',\n",
       " '$jks',\n",
       " 'cooler',\n",
       " '@monster_calls',\n",
       " 'chip',\n",
       " '$jwn',\n",
       " 'consortiums,',\n",
       " 'portion',\n",
       " 'izea',\n",
       " '$ons',\n",
       " '$uec',\n",
       " '$pbyi',\n",
       " 'ema(',\n",
       " '$dia...compared',\n",
       " '...aaaand',\n",
       " 'savvy',\n",
       " 'catherine',\n",
       " 'games',\n",
       " '@vitaislade',\n",
       " 'lists',\n",
       " '$tfm',\n",
       " 'incredible',\n",
       " 'wonder',\n",
       " 'latest.',\n",
       " 'exaggerate',\n",
       " '#silver',\n",
       " '$itot',\n",
       " '$glad',\n",
       " '$daio',\n",
       " 'order',\n",
       " 'holiday.',\n",
       " 'spy',\n",
       " 'code',\n",
       " '$slrc',\n",
       " 'thoughts?',\n",
       " 'no-brainer',\n",
       " '$cbli.',\n",
       " 'were',\n",
       " 'gold.',\n",
       " 'leads',\n",
       " '$cl_f.',\n",
       " 'canna-based',\n",
       " 'money!!',\n",
       " 'longing',\n",
       " 'lol.',\n",
       " '$nue',\n",
       " 'realm',\n",
       " 'gains.',\n",
       " 'amaze!',\n",
       " 'stem',\n",
       " '$acia',\n",
       " 'listen?',\n",
       " ',let&#;s',\n",
       " 'able',\n",
       " 'sale.',\n",
       " 'inevitable.',\n",
       " '$fprx',\n",
       " 'gdp',\n",
       " '$dis',\n",
       " '$cog',\n",
       " 'earning',\n",
       " 'upside.',\n",
       " '&lt;&gt;',\n",
       " 'sidelines',\n",
       " 'https://www.yahoo.com/amphtml/finance/news/vuzix-m',\n",
       " 'musk.',\n",
       " 'all,',\n",
       " 'not...',\n",
       " 'portfolio:',\n",
       " 'decisively',\n",
       " '$weed&#;s',\n",
       " 'bitcoins/',\n",
       " 'when?',\n",
       " 'pls.',\n",
       " 'esports',\n",
       " '$ahh',\n",
       " 'officially',\n",
       " 'holding,',\n",
       " 'wife,',\n",
       " 'boyar',\n",
       " '$ish.',\n",
       " 'worry',\n",
       " 'nuts',\n",
       " 'cable',\n",
       " 'unpaid',\n",
       " 'century',\n",
       " 'loses',\n",
       " 'talks,..',\n",
       " 'transportation/automotive,',\n",
       " '$ibb,',\n",
       " 'customers,',\n",
       " 'price!!',\n",
       " 'agree!',\n",
       " 'trader',\n",
       " 'aquired',\n",
       " 'old,',\n",
       " 'events.',\n",
       " 'approval,we',\n",
       " 'low,',\n",
       " 'pumping.',\n",
       " '$blcm',\n",
       " 'they&#;ll..',\n",
       " 'fade',\n",
       " '$fnsr',\n",
       " 'bn/yr,',\n",
       " 'wmlp',\n",
       " '#cannabis',\n",
       " '$espr',\n",
       " '$sogo',\n",
       " 'ideas',\n",
       " 'education.',\n",
       " 'https://finance.yahoo.com/news/young-consumers-fueled-alibabas-',\n",
       " 'repatriation.',\n",
       " 'for.,',\n",
       " 'chardan',\n",
       " 'eom-march',\n",
       " 'fibonacci',\n",
       " '$qrhc',\n",
       " '$htgm',\n",
       " '$atvi:',\n",
       " 'takeoff',\n",
       " 'statements',\n",
       " 'name.',\n",
       " '$ogi.ca',\n",
       " '$aks',\n",
       " '-minute',\n",
       " 'steele...',\n",
       " 'ico',\n",
       " '$btg',\n",
       " 'dma.',\n",
       " 'reaches',\n",
       " 'jumanji',\n",
       " 'dad',\n",
       " 'addition',\n",
       " 'https://lnkd.in/ejgvqhe',\n",
       " 'https://wccftech.com/apple-iphone-x-first-benchmarks-show-results/',\n",
       " '@johnny_greenjeans',\n",
       " '@bmaine',\n",
       " 'gets',\n",
       " '$spar',\n",
       " 'gawn!',\n",
       " 'nanoflu',\n",
       " 'shares(',\n",
       " 'dick&#;s',\n",
       " '$cmls',\n",
       " '@jessicacolorado',\n",
       " 'divvy',\n",
       " 'https://www.washingtonpost.com/news/wonk/wp////cvs-agrees-to-buy-aetna-in-',\n",
       " 'or...do',\n",
       " 'npd',\n",
       " 'etf)',\n",
       " 'retreat',\n",
       " 'relax.',\n",
       " 'completion,dsma',\n",
       " 'avgo',\n",
       " '%..hoping',\n",
       " '$twlo',\n",
       " '#uranium',\n",
       " 'shares',\n",
       " 'lifetime.',\n",
       " '$spnc',\n",
       " 'safely',\n",
       " 'educated.',\n",
       " 'logical',\n",
       " 'nutanix',\n",
       " '$aumn',\n",
       " 'rip.',\n",
       " 'today..',\n",
       " 'correction,',\n",
       " 'continually',\n",
       " '#retail',\n",
       " 'orphan',\n",
       " 'calls,',\n",
       " 'value?',\n",
       " '$csco',\n",
       " '-good',\n",
       " 'yet?',\n",
       " 'hate.',\n",
       " 'pension',\n",
       " 'secured)communications;',\n",
       " 'abc',\n",
       " '$wpcs',\n",
       " 'open',\n",
       " 'something',\n",
       " 'investor.',\n",
       " '$bund',\n",
       " 'averts-',\n",
       " 'selling,',\n",
       " 'spending',\n",
       " 'campus',\n",
       " 'increase,',\n",
       " 'formed',\n",
       " 'basically',\n",
       " 'moves!',\n",
       " 'outperforms',\n",
       " 'valued',\n",
       " 'fam.',\n",
       " '..good',\n",
       " 'was,',\n",
       " 'stockhouse',\n",
       " 'papa',\n",
       " 'havent',\n",
       " 'opp',\n",
       " 'changed',\n",
       " '$arlz',\n",
       " '$nvda',\n",
       " '$wti',\n",
       " 'wayyy',\n",
       " 'morning!',\n",
       " 'lock',\n",
       " 'gambled',\n",
       " 'hod.',\n",
       " 'tanking',\n",
       " 'have',\n",
       " 'can,',\n",
       " '$sncr',\n",
       " 'ftc',\n",
       " 'publicly',\n",
       " 'passionate-some',\n",
       " 'luck.',\n",
       " 'car',\n",
       " 'ancient',\n",
       " 'sight.',\n",
       " '#homebuilders',\n",
       " 'grpn',\n",
       " 'zone',\n",
       " '$vltc',\n",
       " 'yet)',\n",
       " '$dcr.x',\n",
       " 'derivatives....',\n",
       " 'any',\n",
       " 'vote,',\n",
       " 'debt',\n",
       " 'fly',\n",
       " 'pfizer',\n",
       " 'title',\n",
       " '@shekelcrusader',\n",
       " 'boughtbdhares',\n",
       " 'kyle,',\n",
       " 'rnva,',\n",
       " 'thousands',\n",
       " 'violation',\n",
       " 'skyrocket',\n",
       " 'carbon',\n",
       " 'bsishness',\n",
       " '$mon.',\n",
       " 'jung?',\n",
       " 'canceled',\n",
       " 'btc.',\n",
       " 'reached.',\n",
       " 'save',\n",
       " 'yellow',\n",
       " '$ast',\n",
       " '...still',\n",
       " 'amendment',\n",
       " 'usual',\n",
       " 'brokers,',\n",
       " 'grab:',\n",
       " 'fine.',\n",
       " 'false',\n",
       " 'deposits.',\n",
       " 'everything',\n",
       " 'winner',\n",
       " 'finished',\n",
       " 'seladelpar',\n",
       " 'soon,',\n",
       " '$tjx',\n",
       " 'brb',\n",
       " 'note:',\n",
       " 'retired,',\n",
       " 'nothin',\n",
       " 'dev',\n",
       " '$cgnx',\n",
       " 'smashed',\n",
       " 'arrived',\n",
       " 'quote',\n",
       " 'levin',\n",
       " 'sma',\n",
       " 'hey',\n",
       " 'ptn?',\n",
       " 'ice',\n",
       " 'billioniare',\n",
       " 'eod.',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to get embeddingss for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.wv[low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13107, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing these unqique words in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {k: v for v, k in enumerate(low)}\n",
    "label_dict['<num>'] = len(label_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding <num\\> back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13107"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict['<num>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding filler word to keep all tweets the same length. Then replacing all words with their dictionary equivalent. This is for tenssorflows matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufferIndex = len(label_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "modifiedText = [[label_dict[word] for word in text.split()]for text in stripped]\n",
    "maxLen = max(map(len, modifiedText))\n",
    "for item in modifiedText:                # for each item in the list\n",
    "    while len(item) < maxLen:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex) \n",
    "numpyInp = np.asarray(modifiedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6284,  9175, 13107, ..., 13108, 13108, 13108],\n",
       "       [  331,  3463, 12225, ..., 13108, 13108, 13108],\n",
       "       [11264,  3953,  4594, ..., 13108, 13108, 13108],\n",
       "       ...,\n",
       "       [ 3119,  8674,   197, ..., 13108, 13108, 13108],\n",
       "       [ 7623,  3166,  3175, ..., 13108, 13108, 13108],\n",
       "       [13107,  7623,  3166, ..., 13108, 13108, 13108]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpyInp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in unique embeddings for <num\\> and filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.vstack((embed, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13109, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do again for validation set\n",
    "\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df2['cat_num'] = df2['category'].astype('category').cat.codes\n",
    "model2 = FastText.load('Gensim FastText Vectors/fastText1')\n",
    "df2['lower'] = [x.lower() for x in df2.tweet]\n",
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out\n",
    "df2['mod'] = df2.apply(lambda x: flagNum(x), axis = 1)\n",
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped2 = [textPuncandNum(text) for text in df2['mod']]\n",
    "low2 = list(stripped2)\n",
    "low2 = ' '.join(low2)\n",
    "low2 = list(set(low2.split()))\n",
    "low2.remove('<num>')\n",
    "embed2 = model2.wv[low2]\n",
    "label_dict2 = {k: v for v, k in enumerate(low2)}\n",
    "label_dict2['<num>'] = len(label_dict2) \n",
    "label_dict2['<num>']\n",
    "bufferIndex2 = len(label_dict2) \n",
    "import numpy as np\n",
    "modifiedText2 = [[label_dict2[word] for word in text.split()]for text in stripped2]\n",
    "maxLen2 = max(map(len, modifiedText2))\n",
    "for item in modifiedText2:                # for each item in the list\n",
    "    while len(item) < maxLen2:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex2) \n",
    "numpyInp2 = np.asarray(modifiedText2)\n",
    "embed2 = np.vstack((embed2, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2952, 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix for any input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.get_dummies(df['cat_num'])\n",
    "labels2 = pd.get_dummies(df2['cat_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (numpyInp, labels)\n",
    "val = (numpyInp2, labels2)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create validation Dataset and batch it\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val)\n",
    "val_data = val_data.shuffle(10000) # if you want to shuffle your data\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "val_init = iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does enumerate do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 3)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [0,3,4]\n",
    "for i in enumerate(filter_sizes):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed attempt using predefined filter\n",
    "Update: Works now, not using tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWINDOW_SIZE = 100\\nSTRIDE = int(WINDOW_SIZE/2)\\n#embedding2 = tf.expand_dims(embedding, axis = 1)\\nconv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \\n               strides=1, padding='SAME') \\nconv = tf.nn.relu(conv)   \\nwords = flatten(conv)\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "WINDOW_SIZE = 100\n",
    "STRIDE = int(WINDOW_SIZE/2)\n",
    "#embedding2 = tf.expand_dims(embedding, axis = 1)\n",
    "conv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \n",
    "               strides=1, padding='SAME') \n",
    "conv = tf.nn.relu(conv)   \n",
    "words = flatten(conv)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1:0' shape=(?, 26, 100) dtype=float64>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_1:0' shape=(?, 26, 100, 1) dtype=float64>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [2, 3, 5]\n",
    "embedding_size = 100\n",
    "num_filters = 2\n",
    "max_length = 26\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W,tf.float64),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, tf.cast(b,tf.float64)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu,\n",
    "        ksize=[1, max_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pool_3:0' shape=(?, 1, 1, 2) dtype=float64>,\n",
       " <tf.Tensor 'pool_4:0' shape=(?, 1, 1, 2) dtype=float64>,\n",
       " <tf.Tensor 'pool_5:0' shape=(?, 1, 1, 2) dtype=float64>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 6) dtype=float64>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tf.layers.dense(combined_flat, 100, activation = 'relu')\n",
    "conn2 = tf.layers.dense(conn, len(set(df.cat_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/Relu:0' shape=(?, 100) dtype=float64>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = conn2)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(conn2)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.6168136879367743\n",
      "Accuracy 0: 0.3586309523809524\n",
      "Average val loss epoch 0: 1.5331933481641733\n",
      "Val_Accuracy 0: 0.37367021276595747\n",
      "Average loss epoch 1: 1.4523220857028236\n",
      "Accuracy 1: 0.4239583333333333\n",
      "Average val loss epoch 1: 1.5434028384173304\n",
      "Val_Accuracy 1: 0.3683510638297872\n",
      "Average loss epoch 2: 1.3629796513262582\n",
      "Accuracy 2: 0.48735119047619047\n",
      "Average val loss epoch 2: 1.585013277622928\n",
      "Val_Accuracy 2: 0.37101063829787234\n",
      "Average loss epoch 3: 1.2897668706834249\n",
      "Accuracy 3: 0.5190476190476191\n",
      "Average val loss epoch 3: 1.6033073083270097\n",
      "Val_Accuracy 3: 0.34574468085106386\n",
      "Average loss epoch 4: 1.2241665976164304\n",
      "Accuracy 4: 0.5425595238095238\n",
      "Average val loss epoch 4: 1.6614739401343372\n",
      "Val_Accuracy 4: 0.34175531914893614\n",
      "Average loss epoch 5: 1.193822955186069\n",
      "Accuracy 5: 0.5586309523809524\n",
      "Average val loss epoch 5: 1.635164749333593\n",
      "Val_Accuracy 5: 0.3470744680851064\n",
      "Average loss epoch 6: 1.1530101472749095\n",
      "Accuracy 6: 0.5711309523809524\n",
      "Average val loss epoch 6: 1.671473951748027\n",
      "Val_Accuracy 6: 0.35771276595744683\n",
      "Average loss epoch 7: 1.1309523485328024\n",
      "Accuracy 7: 0.575595238095238\n",
      "Average val loss epoch 7: 1.6822100676861056\n",
      "Val_Accuracy 7: 0.34308510638297873\n",
      "Average loss epoch 8: 1.1021392430461763\n",
      "Accuracy 8: 0.5892857142857143\n",
      "Average val loss epoch 8: 1.7239810440497316\n",
      "Val_Accuracy 8: 0.3324468085106383\n",
      "Average loss epoch 9: 1.0710590906738258\n",
      "Accuracy 9: 0.6007440476190476\n",
      "Average val loss epoch 9: 1.7407655067933183\n",
      "Val_Accuracy 9: 0.36436170212765956\n",
      "Average loss epoch 10: 1.0571640409290193\n",
      "Accuracy 10: 0.6074404761904761\n",
      "Average val loss epoch 10: 1.7767179940605788\n",
      "Val_Accuracy 10: 0.3271276595744681\n",
      "Average loss epoch 11: 1.0346401168397128\n",
      "Accuracy 11: 0.615922619047619\n",
      "Average val loss epoch 11: 1.7470545741746417\n",
      "Val_Accuracy 11: 0.3390957446808511\n",
      "Average loss epoch 12: 1.0144833852216149\n",
      "Accuracy 12: 0.6197916666666666\n",
      "Average val loss epoch 12: 1.807696194785047\n",
      "Val_Accuracy 12: 0.3390957446808511\n",
      "Average loss epoch 13: 1.0055440220543086\n",
      "Accuracy 13: 0.6294642857142857\n",
      "Average val loss epoch 13: 1.8187735815125852\n",
      "Val_Accuracy 13: 0.324468085106383\n",
      "Average loss epoch 14: 0.9947969765119064\n",
      "Accuracy 14: 0.6306547619047619\n",
      "Average val loss epoch 14: 1.8383064649685184\n",
      "Val_Accuracy 14: 0.3404255319148936\n",
      "Average loss epoch 15: 0.9815753234764966\n",
      "Accuracy 15: 0.6386904761904761\n",
      "Average val loss epoch 15: 1.8600929395305439\n",
      "Val_Accuracy 15: 0.3351063829787234\n",
      "Average loss epoch 16: 0.9683657619124945\n",
      "Accuracy 16: 0.6404761904761904\n",
      "Average val loss epoch 16: 1.9217252162292906\n",
      "Val_Accuracy 16: 0.35904255319148937\n",
      "Average loss epoch 17: 0.9565457622525891\n",
      "Accuracy 17: 0.646875\n",
      "Average val loss epoch 17: 1.9043656467506378\n",
      "Val_Accuracy 17: 0.3271276595744681\n",
      "Average loss epoch 18: 0.9472717965240002\n",
      "Accuracy 18: 0.6492559523809524\n",
      "Average val loss epoch 18: 1.9541640479558995\n",
      "Val_Accuracy 18: 0.324468085106383\n",
      "Average loss epoch 19: 0.9367718541444885\n",
      "Accuracy 19: 0.653422619047619\n",
      "Average val loss epoch 19: 1.8928756125342652\n",
      "Val_Accuracy 19: 0.3337765957446808\n",
      "Average loss epoch 20: 0.9219313718652031\n",
      "Accuracy 20: 0.6592261904761905\n",
      "Average val loss epoch 20: 2.0266622763954656\n",
      "Val_Accuracy 20: 0.3523936170212766\n",
      "Average loss epoch 21: 0.9287031071761221\n",
      "Accuracy 21: 0.6586309523809524\n",
      "Average val loss epoch 21: 1.969141597580625\n",
      "Val_Accuracy 21: 0.3390957446808511\n",
      "Average loss epoch 22: 0.9062406921095769\n",
      "Accuracy 22: 0.6636904761904762\n",
      "Average val loss epoch 22: 1.992176430959812\n",
      "Val_Accuracy 22: 0.30186170212765956\n",
      "Average loss epoch 23: 0.9063347270718233\n",
      "Accuracy 23: 0.6604166666666667\n",
      "Average val loss epoch 23: 2.0061288638503907\n",
      "Val_Accuracy 23: 0.33643617021276595\n",
      "Average loss epoch 24: 0.906556401489181\n",
      "Accuracy 24: 0.6644345238095238\n",
      "Average val loss epoch 24: 2.029078566346851\n",
      "Val_Accuracy 24: 0.3257978723404255\n",
      "Average loss epoch 25: 0.896009173169496\n",
      "Accuracy 25: 0.6660714285714285\n",
      "Average val loss epoch 25: 2.0568238246662154\n",
      "Val_Accuracy 25: 0.35771276595744683\n",
      "Average loss epoch 26: 0.8936823559059702\n",
      "Accuracy 26: 0.6654761904761904\n",
      "Average val loss epoch 26: 2.037478366453669\n",
      "Val_Accuracy 26: 0.3178191489361702\n",
      "Average loss epoch 27: 0.8849643282957517\n",
      "Accuracy 27: 0.6726190476190477\n",
      "Average val loss epoch 27: 2.0465194321707947\n",
      "Val_Accuracy 27: 0.31382978723404253\n",
      "Average loss epoch 28: 0.8780047739804959\n",
      "Accuracy 28: 0.6741071428571429\n",
      "Average val loss epoch 28: 2.0403764117749033\n",
      "Val_Accuracy 28: 0.3470744680851064\n",
      "Average loss epoch 29: 0.8691534585485294\n",
      "Accuracy 29: 0.6772321428571428\n",
      "Average val loss epoch 29: 2.0758882697198215\n",
      "Val_Accuracy 29: 0.30851063829787234\n",
      "Average loss epoch 30: 0.8634155996138105\n",
      "Accuracy 30: 0.6819940476190476\n",
      "Average val loss epoch 30: 2.047250418585762\n",
      "Val_Accuracy 30: 0.31382978723404253\n",
      "Average loss epoch 31: 0.8599112140697185\n",
      "Accuracy 31: 0.6824404761904762\n",
      "Average val loss epoch 31: 2.17173383657397\n",
      "Val_Accuracy 31: 0.32978723404255317\n",
      "Average loss epoch 32: 0.8601125189646595\n",
      "Accuracy 32: 0.6767857142857143\n",
      "Average val loss epoch 32: 2.076795635313431\n",
      "Val_Accuracy 32: 0.31117021276595747\n",
      "Average loss epoch 33: 0.8478039524834091\n",
      "Accuracy 33: 0.6825892857142857\n",
      "Average val loss epoch 33: 2.2372469702460513\n",
      "Val_Accuracy 33: 0.30186170212765956\n",
      "Average loss epoch 34: 0.8681724300525601\n",
      "Accuracy 34: 0.6772321428571428\n",
      "Average val loss epoch 34: 2.152495669577983\n",
      "Val_Accuracy 34: 0.2992021276595745\n",
      "Average loss epoch 35: 0.8444420516052976\n",
      "Accuracy 35: 0.6895833333333333\n",
      "Average val loss epoch 35: 2.147243392582478\n",
      "Val_Accuracy 35: 0.2912234042553192\n",
      "Average loss epoch 36: 0.8470394369035615\n",
      "Accuracy 36: 0.6845238095238095\n",
      "Average val loss epoch 36: 2.251567224499341\n",
      "Val_Accuracy 36: 0.3404255319148936\n",
      "Average loss epoch 37: 0.8377758272252132\n",
      "Accuracy 37: 0.687797619047619\n",
      "Average val loss epoch 37: 2.2181966752708315\n",
      "Val_Accuracy 37: 0.32313829787234044\n",
      "Average loss epoch 38: 0.8330894967397875\n",
      "Accuracy 38: 0.6950892857142857\n",
      "Average val loss epoch 38: 2.2755971767766\n",
      "Val_Accuracy 38: 0.31117021276595747\n",
      "Average loss epoch 39: 0.8229911474627587\n",
      "Accuracy 39: 0.6950892857142857\n",
      "Average val loss epoch 39: 2.17692651077189\n",
      "Val_Accuracy 39: 0.29388297872340424\n",
      "Average loss epoch 40: 0.8266937245858774\n",
      "Accuracy 40: 0.6916666666666667\n",
      "Average val loss epoch 40: 2.3249949355380006\n",
      "Val_Accuracy 40: 0.3191489361702128\n",
      "Average loss epoch 41: 0.8122961819993656\n",
      "Accuracy 41: 0.6988095238095238\n",
      "Average val loss epoch 41: 2.2051875942896264\n",
      "Val_Accuracy 41: 0.324468085106383\n",
      "Average loss epoch 42: 0.8176180412479465\n",
      "Accuracy 42: 0.6985119047619047\n",
      "Average val loss epoch 42: 2.2429744818437136\n",
      "Val_Accuracy 42: 0.3191489361702128\n",
      "Average loss epoch 43: 0.8138115010262275\n",
      "Accuracy 43: 0.7037202380952381\n",
      "Average val loss epoch 43: 2.2560736233482035\n",
      "Val_Accuracy 43: 0.3178191489361702\n",
      "Average loss epoch 44: 0.8149753740353459\n",
      "Accuracy 44: 0.6985119047619047\n",
      "Average val loss epoch 44: 2.314495567458327\n",
      "Val_Accuracy 44: 0.3204787234042553\n",
      "Average loss epoch 45: 0.8100031120711904\n",
      "Accuracy 45: 0.7020833333333333\n",
      "Average val loss epoch 45: 2.300749164984161\n",
      "Val_Accuracy 45: 0.3337765957446808\n",
      "Average loss epoch 46: 0.8186329208142519\n",
      "Accuracy 46: 0.6983630952380953\n",
      "Average val loss epoch 46: 2.2424028697334855\n",
      "Val_Accuracy 46: 0.3151595744680851\n",
      "Average loss epoch 47: 0.7968814533755095\n",
      "Accuracy 47: 0.7050595238095239\n",
      "Average val loss epoch 47: 2.306957251941965\n",
      "Val_Accuracy 47: 0.3337765957446808\n",
      "Average loss epoch 48: 0.7934118893297998\n",
      "Accuracy 48: 0.7084821428571428\n",
      "Average val loss epoch 48: 2.2704282618201126\n",
      "Val_Accuracy 48: 0.3257978723404255\n",
      "Average loss epoch 49: 0.799594194855224\n",
      "Accuracy 49: 0.7053571428571429\n",
      "Average val loss epoch 49: 2.417227972973961\n",
      "Val_Accuracy 49: 0.31117021276595747\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        totalvalright = 0\n",
    "        nvalbatches = 0\n",
    "        totalvalloss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(val_init)\n",
    "        try:\n",
    "            while True:\n",
    "                accval,valloss = sess.run([accuracy, loss])\n",
    "                totalvalright += accval\n",
    "                nvalbatches += 1\n",
    "                totalvalloss += valloss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "        print('Average val loss epoch {0}: {1}'.format(i, totalvalloss/nvalbatches))\n",
    "        print('Val_Accuracy {0}: {1}'.format(i, totalvalright/nvalbatches)) \n",
    "    prediction = sess.run(preds, feed_dict={txt: numpyInp})\n",
    "    prediction = np.asarray(prediction)\n",
    "    \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_preds = np.equal(np.argmax(prediction, 1), labels.idxmax(axis = 1))\n",
    "acc = np.mean(out_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fooling around with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "#f = np.array([[1,2,3], [1,2,3]])\n",
    "#print(f)\n",
    "g = np.expand_dims(f, axis = 1)\n",
    "g = np.reshape(f, [-1,2])\n",
    "print(g)\n",
    "print(g.shape)\n",
    "print(np.squeeze(g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
