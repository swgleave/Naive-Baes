{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx</th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>target_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98221616</td>\n",
       "      <td>4976</td>\n",
       "      <td>$ARNA APD334 for Amyotrophic Lateral Sclerosis...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82321187</td>\n",
       "      <td>9839</td>\n",
       "      <td>$OCLR Noob investor that i am, put a 7.38 stop...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>stop loss</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103328840</td>\n",
       "      <td>1455</td>\n",
       "      <td>$ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104840294</td>\n",
       "      <td>1111</td>\n",
       "      <td>$TMUS its acquisition of Layer3 TV The purchas...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>relative</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>forecast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94249158</td>\n",
       "      <td>1372</td>\n",
       "      <td>$SEED L2 Capital deal is real savvy. It takes ...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100979260</td>\n",
       "      <td>505</td>\n",
       "      <td>$BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100775772</td>\n",
       "      <td>1210</td>\n",
       "      <td>$WRN My fav $WRN pattern on my watchlist for 1...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   idx                                              tweet  \\\n",
       "0   98221616  4976  $ARNA APD334 for Amyotrophic Lateral Sclerosis...   \n",
       "1   82321187  9839  $OCLR Noob investor that i am, put a 7.38 stop...   \n",
       "2  103328840  1455  $ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...   \n",
       "3  104840294  1111  $TMUS its acquisition of Layer3 TV The purchas...   \n",
       "4   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "5   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "6   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "7   94249158  1372  $SEED L2 Capital deal is real savvy. It takes ...   \n",
       "8  100979260   505  $BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...   \n",
       "9  100775772  1210  $WRN My fav $WRN pattern on my watchlist for 1...   \n",
       "\n",
       "         category     subcategory target_num  \n",
       "0  Product Number  Product Number        334  \n",
       "1        Monetary       stop loss       7.38  \n",
       "2  Product Number  Product Number          1  \n",
       "3  Product Number  Product Number          5  \n",
       "4      Percentage        relative         14  \n",
       "5        Quantity        Quantity          4  \n",
       "6        Monetary        forecast          5  \n",
       "7        Temporal            date         33  \n",
       "8        Temporal            date          4  \n",
       "9        Temporal            date         11  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('finnum/train.csv')\n",
    "x = df1.index\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df = pd.concat([df1,df2], ignore_index = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for encoding categories as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_num'] = df['category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load('fastText1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing tweets by lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = [x.lower() for x in df.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing target with <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to training, making this into a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mod'] = df.apply(lambda x: flagNum(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd <num>  for amyotrophic lateral scler...\n",
       "1    $oclr noob investor that i am, put a  <num>  s...\n",
       "2    $es_f $spy bias-2 bearish and the dlt- <num>  ...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up  <num> %  4 time avg ...\n",
       "Name: mod, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mod'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out numbers and words less than 3 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped = [textPuncandNum(text) for text in df['mod']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of unique words from this processed text, excluding <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14064"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = list(stripped)\n",
    "low = ' '.join(low)\n",
    "low = list(set(low.split()))\n",
    "low.remove('<num>')\n",
    "len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['correction:',\n",
       " 'robotic',\n",
       " 'lmao!',\n",
       " 'expansion',\n",
       " 'uptake',\n",
       " 'held.',\n",
       " 'hit.',\n",
       " 'why.',\n",
       " '$bur',\n",
       " 'https://finance.yahoo.com/news/credit-suisse-ag-announces-reverse-.html',\n",
       " 'cryptocurrency',\n",
       " 'c-suite',\n",
       " 'slv',\n",
       " 'you?',\n",
       " 'http://stks.co/tbnu',\n",
       " '$nxtd',\n",
       " '$snss',\n",
       " 'erinn...looks',\n",
       " 'japan.',\n",
       " 'wasn&#;t',\n",
       " 'metrics',\n",
       " 'days...',\n",
       " 'delux,',\n",
       " '$tfm',\n",
       " '$gpor',\n",
       " '$amrs-',\n",
       " 'force.',\n",
       " 'gtav',\n",
       " '@tfnn',\n",
       " 'cpu',\n",
       " 'lunch.',\n",
       " 'doubted',\n",
       " '$cmm.v',\n",
       " '$omgbtc',\n",
       " '$sgoc',\n",
       " '$exk',\n",
       " 'mainstream',\n",
       " 'ipdn',\n",
       " '$viab',\n",
       " 'capita',\n",
       " 'instant',\n",
       " 'cheaper',\n",
       " 'needham.',\n",
       " 'traffic',\n",
       " 'casino.',\n",
       " 'would',\n",
       " 'goiing',\n",
       " 'gdpr',\n",
       " '$fmsa',\n",
       " 'establish',\n",
       " 'agen',\n",
       " 'agrawals',\n",
       " 'price.',\n",
       " 'quickly?',\n",
       " 'gauge',\n",
       " '$jwn.',\n",
       " 'pricey',\n",
       " 'limits',\n",
       " 'confident.',\n",
       " 'launch.',\n",
       " 'amzn.',\n",
       " 'double+',\n",
       " 'https://www.thecontraaccountant.com/single-post////have-you-forgotten-about-the-',\n",
       " 'fool.',\n",
       " 'reforms.',\n",
       " '$million',\n",
       " 'sears',\n",
       " '@njdevilsinseven',\n",
       " 'cycle',\n",
       " 'reach',\n",
       " 'excellent',\n",
       " 'tomorrow....capisce?',\n",
       " '$aprn',\n",
       " '&quot;office',\n",
       " 'achieved',\n",
       " 'something!',\n",
       " 'cap.',\n",
       " 'posts:&#;',\n",
       " 'spin,',\n",
       " 'laughing',\n",
       " 'since.',\n",
       " 'folks,',\n",
       " 'like,',\n",
       " 'midnight',\n",
       " 'current',\n",
       " 'white',\n",
       " 'nyse',\n",
       " 'open!!',\n",
       " 'prices',\n",
       " 'mine',\n",
       " 'films.',\n",
       " 'cdk/',\n",
       " 'lambert',\n",
       " '$pxs',\n",
       " 'through,',\n",
       " '$bont',\n",
       " 'strs',\n",
       " 'https://www.fda.gov/newsevents/newsroom/pressannouncements/ucm.htm',\n",
       " 'driver.',\n",
       " 'confirm.',\n",
       " '$apps',\n",
       " 'screenshot.',\n",
       " '$vips,',\n",
       " '#djia',\n",
       " 'okay',\n",
       " 'loss.....lololololol',\n",
       " 'concluded,',\n",
       " '$cadc',\n",
       " 'news!',\n",
       " 'pop,',\n",
       " '$rkda',\n",
       " 'lots',\n",
       " 'uranium.',\n",
       " 'newfield',\n",
       " 'juice.',\n",
       " 'afrezza.',\n",
       " '$dea',\n",
       " '$amzn?',\n",
       " 'weakness.',\n",
       " 'carg',\n",
       " 'farfetched',\n",
       " '#fork',\n",
       " '&quot;suffered&quot;',\n",
       " 'loser',\n",
       " 'swn',\n",
       " 'end!',\n",
       " 'generally',\n",
       " 'pick',\n",
       " 'gmv',\n",
       " 'tango',\n",
       " 'regarding',\n",
       " '$fnb',\n",
       " '$cann.x',\n",
       " '$cafd',\n",
       " '-mil',\n",
       " 'cod',\n",
       " 'hung',\n",
       " 'premarket',\n",
       " 'baycity,',\n",
       " 'kinda',\n",
       " '$mnk',\n",
       " 'rico',\n",
       " 'aug/sept',\n",
       " 'sharks!',\n",
       " 'hemi',\n",
       " 'triangles',\n",
       " 'golden!',\n",
       " 'vetr',\n",
       " 'lord',\n",
       " 'turn',\n",
       " 'bet',\n",
       " 'hour!',\n",
       " 'con,',\n",
       " 'coming,',\n",
       " 'ready',\n",
       " 'ignored',\n",
       " '.squeeze',\n",
       " 'cook',\n",
       " '$edc,',\n",
       " 'consistency',\n",
       " 'hodlers',\n",
       " 'money',\n",
       " 'https://www.youtube.com/channel/ucb',\n",
       " 'coming.svxy',\n",
       " 'hour',\n",
       " '#massive',\n",
       " 'dxtr',\n",
       " 'downloads.',\n",
       " 'http://www.swingstocktraders.com/performance.html',\n",
       " 'summary,',\n",
       " 'cnbc',\n",
       " 'up..holding',\n",
       " '$mmc',\n",
       " 'brexit.',\n",
       " 'w/china&#;s',\n",
       " 'low',\n",
       " 'shareholders.',\n",
       " '$b...bidu',\n",
       " 'fine!',\n",
       " 'imminent.',\n",
       " 'holidays!',\n",
       " 'appeared',\n",
       " '$alxn',\n",
       " 'cvv.ca.',\n",
       " 'scalped',\n",
       " 'two-month',\n",
       " 'incompetent',\n",
       " '$emr',\n",
       " 'bigger',\n",
       " 'buddy.',\n",
       " 'squat',\n",
       " 'damn!!',\n",
       " 'tutes',\n",
       " 'watt.',\n",
       " 'geforce',\n",
       " 'options&quot;',\n",
       " 'calendar,',\n",
       " '-new',\n",
       " 'bday',\n",
       " 'day,more',\n",
       " '$gme.',\n",
       " 'setups',\n",
       " 'louis',\n",
       " 'shits',\n",
       " '$xrp.x',\n",
       " '(only',\n",
       " 'network',\n",
       " 'everything',\n",
       " 'thousands',\n",
       " 'board.',\n",
       " 'average.',\n",
       " 'roi',\n",
       " 'bias-',\n",
       " 'got',\n",
       " 'udc',\n",
       " 'e-cigarettes',\n",
       " '..just',\n",
       " '$mynd',\n",
       " 'here',\n",
       " 'intc',\n",
       " 'yatch',\n",
       " 'comes.',\n",
       " '#editing',\n",
       " 'ago)',\n",
       " 'november',\n",
       " '&quot;short',\n",
       " 'globally.k',\n",
       " 'bag',\n",
       " 'aggressive',\n",
       " 'offing',\n",
       " 'maybe...just',\n",
       " 'lovein.',\n",
       " 'complicated&quot;.',\n",
       " '.com/story/-doomed-stocks-to-sell-in-may-',\n",
       " 'curis',\n",
       " 'clobbered,',\n",
       " 'imbalance',\n",
       " 'back?',\n",
       " '$alo',\n",
       " '$expr',\n",
       " 'avg.',\n",
       " 'bod?',\n",
       " '$paas',\n",
       " 'extended',\n",
       " 'w/coal',\n",
       " '$itus',\n",
       " 'pete',\n",
       " 'surprise',\n",
       " 'stmicroelectronics',\n",
       " 'mode',\n",
       " '(sold',\n",
       " 'multiply',\n",
       " 'sales@ascentsolar.com',\n",
       " 'beholding',\n",
       " '$mrk.',\n",
       " '$bts.x',\n",
       " '$vstm',\n",
       " 'panic.',\n",
       " 'campaign',\n",
       " '$gds',\n",
       " 'nowhere',\n",
       " 'unbelievable',\n",
       " 'indication',\n",
       " '$googl,',\n",
       " '$etp',\n",
       " 'zone,',\n",
       " 'winner!',\n",
       " 'time&#;s',\n",
       " 'possibility',\n",
       " 'capped',\n",
       " 'netflix.',\n",
       " '$dac',\n",
       " '$gme',\n",
       " 'additional',\n",
       " 'jump',\n",
       " 'celulose',\n",
       " 'sh!t',\n",
       " 'years.management',\n",
       " '.avg..going',\n",
       " 'https://investoralmanac.com////boyar-value-group-',\n",
       " 'worm',\n",
       " '$ubnt',\n",
       " 'fast;',\n",
       " '$giii',\n",
       " '$bnft',\n",
       " 'bastards',\n",
       " '$cqqq',\n",
       " 'meant',\n",
       " '#luckylou',\n",
       " 'profitable..',\n",
       " 'oct-',\n",
       " '{triangle}',\n",
       " 'bln',\n",
       " 'soar',\n",
       " '@illuminatiinvestments',\n",
       " 'https://www.linkedin.com/pulse/trade-idea-performance-review-buying-tax-loss-allan-parks-cfa?trk=pulse_spock-articles',\n",
       " 'secured)communications;',\n",
       " 'much',\n",
       " 'lift',\n",
       " 'rule&#;--',\n",
       " 'new?',\n",
       " 'downtrend.',\n",
       " 'opportunity.',\n",
       " 'normal?',\n",
       " '$delt.',\n",
       " 'shorties.',\n",
       " 'ath',\n",
       " 'worthwhile--&gt;',\n",
       " 'cannabis',\n",
       " '-ranked-stocks-inside-exodus-/',\n",
       " 'tuned',\n",
       " '$cj.ca',\n",
       " '$ipgp',\n",
       " 'dependent/foreign',\n",
       " 'semester',\n",
       " 'squabbles',\n",
       " 'gtt.ca',\n",
       " 'counted:',\n",
       " 'crypts',\n",
       " 'ecommerce,',\n",
       " 'listen',\n",
       " 'mcl',\n",
       " 'broken',\n",
       " 'console.',\n",
       " 'yrs..lol',\n",
       " 'scam,',\n",
       " 'supply-dem-temps',\n",
       " 'wording',\n",
       " 'chunks',\n",
       " '$cgix',\n",
       " 'technology',\n",
       " 'left',\n",
       " '$vtl',\n",
       " 'mania',\n",
       " 'broken...',\n",
       " 'spdr',\n",
       " 'gdx',\n",
       " '#observation',\n",
       " 'patience...',\n",
       " 'careful.',\n",
       " '-cent',\n",
       " '$sune',\n",
       " 'pre',\n",
       " 'https://www.inquisitr.com//red-dead-redemption-',\n",
       " 'ride.',\n",
       " '$fdn',\n",
       " 'concert',\n",
       " 'jpvest',\n",
       " 'buyin',\n",
       " 'wow,repeat',\n",
       " 'impending',\n",
       " 'rule',\n",
       " 'trillion',\n",
       " 'asia.',\n",
       " 'data',\n",
       " 'perhaps.',\n",
       " 'weeks...not',\n",
       " '@fmcasado',\n",
       " '$akca',\n",
       " 'unique',\n",
       " '&lt;---my',\n",
       " 'mofos.',\n",
       " 'basement',\n",
       " 'vulnerabilities,',\n",
       " '?there',\n",
       " 'issue.',\n",
       " 'guy',\n",
       " 'unlimited',\n",
       " 'kim',\n",
       " 'list)',\n",
       " 'financing',\n",
       " 'mystras?',\n",
       " 'nhod-move',\n",
       " 'snatched',\n",
       " 'literally',\n",
       " 'cents,',\n",
       " 'augmented',\n",
       " 'operating',\n",
       " '$tnk',\n",
       " 'easiest',\n",
       " 'holidays.',\n",
       " 'england',\n",
       " 'saas',\n",
       " 'maui',\n",
       " '$ntnx',\n",
       " '..biotech',\n",
       " 'superior',\n",
       " 'ford',\n",
       " 'trading.',\n",
       " 'january',\n",
       " '$pkg',\n",
       " 'triangle.$vstm',\n",
       " '$xxii.',\n",
       " 'participate',\n",
       " 'week&#;s',\n",
       " 'stick',\n",
       " 'mid',\n",
       " 'nvidia,',\n",
       " 'qualcomm',\n",
       " 'courts',\n",
       " '$wstl',\n",
       " '$ptn.',\n",
       " 'ow.ly/fxgqg',\n",
       " '$cik',\n",
       " '$sino',\n",
       " 'sec?',\n",
       " '$btc',\n",
       " 'trouble',\n",
       " 'signal',\n",
       " 'thanks.',\n",
       " 'chase,',\n",
       " 'price:',\n",
       " 'lottos',\n",
       " 'shorters,',\n",
       " 'lying',\n",
       " 'rates',\n",
       " 'indicates',\n",
       " 'snap',\n",
       " 'ii)...last',\n",
       " '$airi',\n",
       " 'https://youtu.be/mwbswifoae',\n",
       " '$hwo.ca',\n",
       " 'tea',\n",
       " '$cco.ca',\n",
       " '$dia...compared',\n",
       " 'loaned',\n",
       " 'businesses',\n",
       " 'crazy,',\n",
       " 'ladies&amp;gentlemen,',\n",
       " 'sabby',\n",
       " 'sick',\n",
       " 'father',\n",
       " '(new',\n",
       " 'officially',\n",
       " 'scary',\n",
       " 'poops',\n",
       " 'kraken...super',\n",
       " '$ita',\n",
       " 'setups.{c&amp;h}',\n",
       " 'cocaines',\n",
       " 'billionaire',\n",
       " 'nbix',\n",
       " 'labeled',\n",
       " 'daily,',\n",
       " 'pharms',\n",
       " 'useless/repeated',\n",
       " 'moon)',\n",
       " 'just',\n",
       " 'shoulda',\n",
       " '$cree',\n",
       " 'b...bidu',\n",
       " 'cares..',\n",
       " 'https://www.youtube.com/watch?v=azdaxmaqj',\n",
       " 'hopefully',\n",
       " '$vsar,$cycc,$nlst,$cycc,$sq,$cycc,$otic,$amzn,$cycc,$cycc....these',\n",
       " '#altcoins',\n",
       " 'coin',\n",
       " 'caca',\n",
       " '$yumc',\n",
       " 'collusion',\n",
       " '$spxsop',\n",
       " '$chk',\n",
       " 'whenever',\n",
       " 'dumb.',\n",
       " 'days,if',\n",
       " 'cranked-up',\n",
       " 'notepads.',\n",
       " 'relations',\n",
       " 'present',\n",
       " 'hasn&#;t',\n",
       " '$uvxy...dow',\n",
       " 'naeve',\n",
       " '$ric',\n",
       " 'lower?',\n",
       " 'doses.',\n",
       " 'uncle',\n",
       " 'nak..',\n",
       " 'hivinfectious',\n",
       " 'radar.',\n",
       " 'maj',\n",
       " 'month).',\n",
       " 'announced#payerssniffin',\n",
       " 'models',\n",
       " 'checkout',\n",
       " 'increments.',\n",
       " 'this,',\n",
       " '$rgc',\n",
       " 'outperformance',\n",
       " 'closer',\n",
       " 'reving',\n",
       " 'consumer',\n",
       " '$nakhit',\n",
       " 'reference',\n",
       " 'denied.',\n",
       " 'shares?',\n",
       " 'chief',\n",
       " 'download',\n",
       " '...fully',\n",
       " '$mrdn',\n",
       " 'compared',\n",
       " 're-test',\n",
       " 'timing',\n",
       " '$lulu',\n",
       " 'diluting',\n",
       " 'articles...too',\n",
       " 'music.',\n",
       " 'sun',\n",
       " 'mgti',\n",
       " 'fitzgerald,',\n",
       " 'gta.',\n",
       " '$oild',\n",
       " 'brick',\n",
       " 'https://ledgergazette.com////ritter-pharmaceuticals-inc-rttr-given-consensus-recommendation-of-buy-by-analysts.html',\n",
       " '#cnbc',\n",
       " 'screw',\n",
       " 'assets...',\n",
       " 'analysis@https://goo.gl/crw',\n",
       " 'puke',\n",
       " 'play.',\n",
       " 'one.',\n",
       " 'directors',\n",
       " 'apex',\n",
       " '$athx',\n",
       " 'facts.',\n",
       " 'https://weather.com/maps/tendayforecast',\n",
       " 'was....nothing',\n",
       " 'sold.',\n",
       " 'after.back',\n",
       " '$anip',\n",
       " 'opent',\n",
       " 'writing',\n",
       " 'idiot',\n",
       " 'sensitive',\n",
       " 'slaughtered',\n",
       " 'offering/backstop',\n",
       " 'htgm',\n",
       " '$atnm.',\n",
       " 'pixar',\n",
       " '$jju.',\n",
       " 'disney',\n",
       " 'bofaml',\n",
       " 'monthly.',\n",
       " 'unsustainable',\n",
       " 'perils',\n",
       " '$ions',\n",
       " 'tom.',\n",
       " 'fekola',\n",
       " ',what',\n",
       " 'due',\n",
       " 'voted',\n",
       " 'shars',\n",
       " 'see;',\n",
       " 'statement',\n",
       " 'recap:today&#;s',\n",
       " 'quantum',\n",
       " 'damp',\n",
       " 'went',\n",
       " 'https://www.forbes.com/sites/blakemorgan////',\n",
       " 'outdoor',\n",
       " 'weeks!!',\n",
       " '(hrs',\n",
       " 'bearishness',\n",
       " 'focus:',\n",
       " 'people.shows',\n",
       " 'horn',\n",
       " 'tank',\n",
       " '$brcm',\n",
       " 'samsung',\n",
       " '$aumn',\n",
       " 'dog.',\n",
       " 'royalties',\n",
       " 'bail',\n",
       " 'headed.',\n",
       " '$syn',\n",
       " 'investor.',\n",
       " 'factores',\n",
       " 'kinglist',\n",
       " 'dhl',\n",
       " 'firm',\n",
       " '$gevo',\n",
       " '$sfs',\n",
       " 'early.',\n",
       " 'j&amp;j',\n",
       " 'market',\n",
       " '$dtb.x',\n",
       " 'appears',\n",
       " '$nomd',\n",
       " 'englander)',\n",
       " 'want.',\n",
       " 'gratz',\n",
       " 'team!',\n",
       " '$ibm**.',\n",
       " 'followers,',\n",
       " 'flippers!',\n",
       " 'contracted',\n",
       " 'official',\n",
       " 'divi!',\n",
       " 'scsmper...',\n",
       " 'btc',\n",
       " '$crb',\n",
       " 'https://investoralmanac.com////bill-ackmans-q',\n",
       " 'days...just',\n",
       " 'first,',\n",
       " 'bcx',\n",
       " 'currencies',\n",
       " 'taxes',\n",
       " 'lol,',\n",
       " 'violation',\n",
       " '&quot;it',\n",
       " 'stocks.',\n",
       " 'cell',\n",
       " 'plays.',\n",
       " 'https://venturebeat.com////superdata-october-digital-game-revenue-up--percent-year-over-year-to-',\n",
       " '$agi',\n",
       " 'covering',\n",
       " '$aieq',\n",
       " 'rate,fiat',\n",
       " 'strike.',\n",
       " 'safe',\n",
       " 'http://stks.co/cgp',\n",
       " 'http://roadmapretire.com/',\n",
       " 'http://thefly.com/landingpagenews.php?id=',\n",
       " 'gregory',\n",
       " 'echnuter.com/channel-news/blackberry-expands-its-channel-ecosystem-with-six-new-partners-in-india.html',\n",
       " 'lol!',\n",
       " 'that!',\n",
       " 'leveled',\n",
       " 'restructuring',\n",
       " '$hyg',\n",
       " '$xin.',\n",
       " 'fed',\n",
       " 'compare',\n",
       " '$ngas',\n",
       " 'kong',\n",
       " 'escape.',\n",
       " 'awake.',\n",
       " 'floaters,',\n",
       " 'reportedly',\n",
       " 'boom.',\n",
       " 'wish',\n",
       " 'cloud:',\n",
       " '$dxc',\n",
       " 'away...',\n",
       " 'heads',\n",
       " 'repurchases',\n",
       " 'come!',\n",
       " 'only!?!',\n",
       " 'varubi',\n",
       " 'anti',\n",
       " '@reformedtrader',\n",
       " 'desire',\n",
       " 'https://www.linkedin.com/pulse/yr-yields-lead-financials-outperform-allan-parks-cfa?trk=prof-post',\n",
       " 'sucess,',\n",
       " '@freddy',\n",
       " 'closer.',\n",
       " 'gltu',\n",
       " 'from',\n",
       " 'evans',\n",
       " '(per',\n",
       " '$vygr',\n",
       " 'jamie',\n",
       " 'actress',\n",
       " 'liabilities',\n",
       " 'conference.',\n",
       " '$gpk',\n",
       " 'mil.',\n",
       " '$wpx',\n",
       " 'recurring',\n",
       " 'inventory?',\n",
       " 'www.tradespoon.com',\n",
       " 'long.stop@$',\n",
       " 'loses',\n",
       " '$spotgold',\n",
       " '$lac.ca',\n",
       " 'makes',\n",
       " 'changed.+',\n",
       " 'vitaliks',\n",
       " 'on&quot;...',\n",
       " 'comp.',\n",
       " 'arc',\n",
       " '$cvm.',\n",
       " 'super',\n",
       " '#tradesmart',\n",
       " 'accounting',\n",
       " 'hqy',\n",
       " '...aaaaand',\n",
       " 'increases,',\n",
       " '(adr)&#;s',\n",
       " 'day....let&#;s',\n",
       " 'called',\n",
       " 'longer',\n",
       " 'abcup',\n",
       " 'crew',\n",
       " '(low',\n",
       " 'euphoria',\n",
       " 'all!',\n",
       " '$box....patience',\n",
       " 'lolololol.',\n",
       " 'emerging',\n",
       " 'buying',\n",
       " 'combo',\n",
       " 'top)',\n",
       " 'evercore',\n",
       " '$tsri',\n",
       " '$snps,',\n",
       " '@bunkerbombs',\n",
       " 'https://apps.newyorkfed.org/markets/autorates/tomo-results-display?showmore=true&amp;startdate=//&amp;enddate=//',\n",
       " 'bitshit,',\n",
       " 'shaer',\n",
       " '$vii',\n",
       " '$osx',\n",
       " 'bubble',\n",
       " 'longing',\n",
       " 'highs?',\n",
       " '$geely',\n",
       " 'gotten',\n",
       " 'twitter',\n",
       " 'holding,',\n",
       " 'your',\n",
       " 'formation',\n",
       " '$pbr',\n",
       " 'ships,',\n",
       " 'change?',\n",
       " 'movement',\n",
       " 'bogus',\n",
       " 'long;',\n",
       " 'weekend.',\n",
       " 'viewers',\n",
       " 'asia',\n",
       " 'know',\n",
       " 'tops',\n",
       " 'theaterdebut.',\n",
       " 'inception',\n",
       " 'accounts',\n",
       " 'promotion',\n",
       " 'btw',\n",
       " 'superhero',\n",
       " 'capable',\n",
       " 'relied',\n",
       " 'shares..',\n",
       " 'dropping',\n",
       " 'debt?',\n",
       " '&quot;test&quot;',\n",
       " 'http://www.wingcharts.com/?symbol=ung',\n",
       " 'behind',\n",
       " 'passes.',\n",
       " 'issuance',\n",
       " 'forward.',\n",
       " 'ago',\n",
       " 'follow.$mara',\n",
       " 'sheet,',\n",
       " 'correction.',\n",
       " '....gna',\n",
       " 'rocket,',\n",
       " 'riding',\n",
       " '....these',\n",
       " 'trash',\n",
       " 'japan',\n",
       " '...then',\n",
       " 'brain',\n",
       " 'changing',\n",
       " 'october.',\n",
       " '$kaly',\n",
       " '$akg',\n",
       " 'divs.',\n",
       " 'risinger',\n",
       " 'mil:',\n",
       " '$abio',\n",
       " 'lower,',\n",
       " '$vicl',\n",
       " '=iphone,',\n",
       " '$epe',\n",
       " 'biggest',\n",
       " 'files',\n",
       " 'tightening',\n",
       " '$audcad',\n",
       " 'low,',\n",
       " 'enough,',\n",
       " '$ears,',\n",
       " 'record.',\n",
       " 'bearsish',\n",
       " 'buys.',\n",
       " '$wfc',\n",
       " '$chw',\n",
       " '$slca',\n",
       " 'cracks',\n",
       " 'resistance.',\n",
       " '$btk',\n",
       " 'belt',\n",
       " 'breakout.',\n",
       " 'otc...,',\n",
       " 'alot.',\n",
       " 'noob',\n",
       " 'experiences.',\n",
       " '$peteinsd',\n",
       " 'seasoned',\n",
       " '.,what',\n",
       " 'digits',\n",
       " '$amsc',\n",
       " 'this--&gt;',\n",
       " '...very',\n",
       " '-month',\n",
       " 'bil.',\n",
       " 'began',\n",
       " 'puts,',\n",
       " 'whales',\n",
       " '=ipod,',\n",
       " 'steele...',\n",
       " 'ritter',\n",
       " 'delay,',\n",
       " 'longer.',\n",
       " 'boundaries...',\n",
       " 'day,reversal,',\n",
       " 'felt',\n",
       " 'imo...been',\n",
       " 'rlog&#;s',\n",
       " 'impostors',\n",
       " 'longs!!!',\n",
       " 'savings...',\n",
       " '..that',\n",
       " 'linden',\n",
       " '@least',\n",
       " '@bullionsean',\n",
       " 'https://seekingalpha.com/article/-big-',\n",
       " 'wma.',\n",
       " 'damn',\n",
       " 'hong',\n",
       " 'storms',\n",
       " 'stands..',\n",
       " 'part',\n",
       " '$nakd',\n",
       " 'days....',\n",
       " '@usamolonlabe',\n",
       " 'jail',\n",
       " 'https://seekingalpha.com/article/-blackberrys-bbry-ceo-john-chen-q',\n",
       " 'http://majorleaguestocks.com',\n",
       " '$wynn',\n",
       " 'foreign',\n",
       " 'range.',\n",
       " 'utility',\n",
       " 'sphs',\n",
       " 'timeframe',\n",
       " 'bak',\n",
       " 'performance',\n",
       " 'control.',\n",
       " '(http://www.otcmarkets.com/stock/fuapf/quote)',\n",
       " 'citing',\n",
       " 'queue',\n",
       " 'wal-mart',\n",
       " 'radar',\n",
       " '(swing)',\n",
       " 'halt!',\n",
       " 'volume=good',\n",
       " '$swks',\n",
       " 'weak',\n",
       " 'leg.',\n",
       " '@wallstreetopgun',\n",
       " '$ovas',\n",
       " 'energy,',\n",
       " 'everyday',\n",
       " 'walls-',\n",
       " 'joint',\n",
       " 'approved,until',\n",
       " '$ras',\n",
       " 'remember',\n",
       " '$insy',\n",
       " 'zuma',\n",
       " 'shows',\n",
       " '-energy',\n",
       " 'xxii.',\n",
       " '$tsm',\n",
       " '$adro',\n",
       " '&#;&#;going',\n",
       " '$cpg',\n",
       " 'signet&#;s',\n",
       " 'token)',\n",
       " 'thanks!!',\n",
       " 'dick&#;s',\n",
       " 'makes.',\n",
       " 'ftr',\n",
       " 'worry',\n",
       " '@mkahn',\n",
       " 'notepads',\n",
       " 'attempts',\n",
       " 'w/big',\n",
       " 'china!',\n",
       " 'borrow,',\n",
       " '(it&#;s',\n",
       " 'partnerships!',\n",
       " 'step',\n",
       " '$atvi',\n",
       " 'running',\n",
       " 'decreased',\n",
       " 'gain!',\n",
       " 'addicted',\n",
       " 'inflection',\n",
       " 'i&#;ll',\n",
       " 'https://www.tradingview.com/x/qzfwphyl/',\n",
       " 'this&#;ll',\n",
       " 'shares-i',\n",
       " 'goof',\n",
       " 'special',\n",
       " '$enph',\n",
       " 'resurgence',\n",
       " '$asti',\n",
       " 'drinking',\n",
       " 'partnership',\n",
       " '$ltc',\n",
       " '$doge',\n",
       " '$aimt',\n",
       " 'https://investoralmanac.com////baron-energy-and-resources-',\n",
       " 'limelight!',\n",
       " '(called',\n",
       " 'xiaomi',\n",
       " 'goimg',\n",
       " 'hover',\n",
       " '$msft.',\n",
       " 'rice,',\n",
       " 'jaffrey',\n",
       " '$srci',\n",
       " 'fam.',\n",
       " 'maths',\n",
       " 'day&#;s',\n",
       " 'csco',\n",
       " 'riot',\n",
       " 'short)',\n",
       " 'years;',\n",
       " '$tdoc',\n",
       " 'tulip',\n",
       " '$amd',\n",
       " 'tomorro',\n",
       " 'firms,',\n",
       " 'lolllll',\n",
       " '$intc',\n",
       " 'something!!!!!!',\n",
       " '$strat.x',\n",
       " 'trucking.',\n",
       " 'halt',\n",
       " 'proof',\n",
       " 'fast.',\n",
       " 'topic',\n",
       " '@claytrader',\n",
       " '$fnko',\n",
       " 'yestrday,watch',\n",
       " '$cmls',\n",
       " '-stay',\n",
       " 'f-it',\n",
       " '$mime',\n",
       " 'friday.',\n",
       " 'steam?',\n",
       " 'spot',\n",
       " 'thick',\n",
       " 'add',\n",
       " 'america',\n",
       " 'while',\n",
       " 'brief',\n",
       " 'awww',\n",
       " 'weekend...i',\n",
       " 'support,but',\n",
       " 'upwards',\n",
       " 'well.',\n",
       " 'agribusiness',\n",
       " 'big.',\n",
       " 'greece',\n",
       " 'ranging',\n",
       " 'hope,',\n",
       " 'powerhouse',\n",
       " 'cold',\n",
       " 'industrials',\n",
       " '^^^side',\n",
       " 'oooops',\n",
       " 'nearing',\n",
       " 'float,',\n",
       " 'rotation.',\n",
       " 'manipulated',\n",
       " 'extreme',\n",
       " 'closely',\n",
       " 'jinping&#;s',\n",
       " 'them!',\n",
       " '-contradictions-plaguing-the-industry-peak-demand-really/',\n",
       " 'lips.',\n",
       " '$mygn',\n",
       " 'wanting',\n",
       " 'fly',\n",
       " 'fut',\n",
       " 'horizon.',\n",
       " 'falc..',\n",
       " 'positons)',\n",
       " 'and',\n",
       " 'material',\n",
       " 'senhance',\n",
       " 'out!',\n",
       " 'ending-',\n",
       " 'potential',\n",
       " 'all-inclusive',\n",
       " 'nears',\n",
       " 'pts',\n",
       " '$sndx',\n",
       " 'him',\n",
       " 'boy',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to get embeddingss for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.wv[low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14064, 100)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing these unqique words in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {k: v for v, k in enumerate(low)}\n",
    "label_dict['<num>'] = len(label_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding <num\\> back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14064"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict['<num>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding filler word to keep all tweets the same length. Then replacing all words with their dictionary equivalent. This is for tenssorflows matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufferIndex = len(label_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "modifiedText = [[label_dict[word] for word in text.split()]for text in stripped]\n",
    "maxLen = max(map(len, modifiedText))\n",
    "for item in modifiedText:                # for each item in the list\n",
    "    while len(item) < maxLen:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex) \n",
    "numpyInp = np.asarray(modifiedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14052,  9307, 14064, ..., 14065, 14065, 14065],\n",
       "       [12214,   792, 10889, ..., 14065, 14065, 14065],\n",
       "       [ 4081,  2743,   212, ..., 14065, 14065, 14065],\n",
       "       ...,\n",
       "       [ 7248, 12383, 13982, ..., 14065, 14065, 14065],\n",
       "       [ 9059, 13030,  4020, ..., 14065, 14065, 14065],\n",
       "       [ 7050,  3231, 12104, ..., 14065, 14065, 14065]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpyInp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in unique embeddings for <num\\> and filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.vstack((embed, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14066, 100)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = numpyInp[x,:]\n",
    "val = numpyInp[len(x):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7450"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape[0] + train.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#do again for validation set\n",
    "\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df2['cat_num'] = df2['category'].astype('category').cat.codes\n",
    "model2 = FastText.load('Gensim FastText Vectors/fastText1')\n",
    "df2['lower'] = [x.lower() for x in df2.tweet]\n",
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out\n",
    "df2['mod'] = df2.apply(lambda x: flagNum(x), axis = 1)\n",
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped2 = [textPuncandNum(text) for text in df2['mod']]\n",
    "low2 = list(stripped2)\n",
    "low2 = ' '.join(low2)\n",
    "low2 = list(set(low2.split()))\n",
    "low2.remove('<num>')\n",
    "embed2 = model2.wv[low2]\n",
    "label_dict2 = {k: v for v, k in enumerate(low2)}\n",
    "label_dict2['<num>'] = len(label_dict2) \n",
    "label_dict2['<num>']\n",
    "bufferIndex2 = len(label_dict2) \n",
    "import numpy as np\n",
    "modifiedText2 = [[label_dict2[word] for word in text.split()]for text in stripped2]\n",
    "maxLen2 = max(map(len, modifiedText2))\n",
    "for item in modifiedText2:                # for each item in the list\n",
    "    while len(item) < maxLen2:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex2) \n",
    "numpyInp2 = np.asarray(modifiedText2)\n",
    "embed2 = np.vstack((embed2, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2952, 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix for any input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInp = numpyInp[x,:]\n",
    "valInp = numpyInp[len(x):,:]\n",
    "\n",
    "labels = pd.get_dummies(df['cat_num'][x])\n",
    "labels2 = pd.get_dummies(df['cat_num'][len(x):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (trainInp, labels)\n",
    "val = (valInp, labels2)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create validation Dataset and batch it\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val)\n",
    "val_data = val_data.shuffle(10000) # if you want to shuffle your data\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "val_init = iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does enumerate do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 3)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [0,3,4]\n",
    "for i in enumerate(filter_sizes):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed attempt using predefined filter\n",
    "Update: Works now, not using tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWINDOW_SIZE = 100\\nSTRIDE = int(WINDOW_SIZE/2)\\n#embedding2 = tf.expand_dims(embedding, axis = 1)\\nconv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \\n               strides=1, padding='SAME') \\nconv = tf.nn.relu(conv)   \\nwords = flatten(conv)\\n\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "WINDOW_SIZE = 100\n",
    "STRIDE = int(WINDOW_SIZE/2)\n",
    "#embedding2 = tf.expand_dims(embedding, axis = 1)\n",
    "conv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \n",
    "               strides=1, padding='SAME') \n",
    "conv = tf.nn.relu(conv)   \n",
    "words = flatten(conv)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 26, 100) dtype=float64>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 26, 100, 1) dtype=float64>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [2,3,4,5]\n",
    "embedding_size = 100\n",
    "num_filters = 64\n",
    "max_length = 26\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W,tf.float64),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, tf.cast(b,tf.float64)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu,\n",
    "        ksize=[1, max_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pool:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_1:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_2:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_3:0' shape=(?, 1, 1, 64) dtype=float64>]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 256) dtype=float64>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tf.layers.dense(combined_flat, 100, activation = 'relu')\n",
    "conn2 = tf.layers.dense(conn, len(set(df.cat_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Relu:0' shape=(?, 100) dtype=float64>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = conn2)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(conn2)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 4.007726843413243\n",
      "Accuracy 0: 0.3325892857142857\n",
      "Average val loss epoch 0: 2.783648899751975\n",
      "Val_Accuracy 0: 0.28856382978723405\n",
      "Average loss epoch 1: 2.384174376319691\n",
      "Accuracy 1: 0.41101190476190474\n",
      "Average val loss epoch 1: 3.0805691838945912\n",
      "Val_Accuracy 1: 0.4002659574468085\n",
      "Average loss epoch 2: 1.888114726271596\n",
      "Accuracy 2: 0.46383928571428573\n",
      "Average val loss epoch 2: 1.8698339953724206\n",
      "Val_Accuracy 2: 0.4773936170212766\n",
      "Average loss epoch 3: 1.7440483381382665\n",
      "Accuracy 3: 0.49404761904761907\n",
      "Average val loss epoch 3: 2.265401900711219\n",
      "Val_Accuracy 3: 0.37632978723404253\n",
      "Average loss epoch 4: 1.5518345099943864\n",
      "Accuracy 4: 0.5300595238095238\n",
      "Average val loss epoch 4: 2.243899756778446\n",
      "Val_Accuracy 4: 0.4601063829787234\n",
      "Average loss epoch 5: 1.4226784007160802\n",
      "Accuracy 5: 0.546875\n",
      "Average val loss epoch 5: 2.191701245306981\n",
      "Val_Accuracy 5: 0.46941489361702127\n",
      "Average loss epoch 6: 1.3149245895284956\n",
      "Accuracy 6: 0.578125\n",
      "Average val loss epoch 6: 1.7494351599340383\n",
      "Val_Accuracy 6: 0.45345744680851063\n",
      "Average loss epoch 7: 1.2675735236799104\n",
      "Accuracy 7: 0.5766369047619048\n",
      "Average val loss epoch 7: 1.62521764892254\n",
      "Val_Accuracy 7: 0.5106382978723404\n",
      "Average loss epoch 8: 1.2102848526796992\n",
      "Accuracy 8: 0.6078869047619048\n",
      "Average val loss epoch 8: 1.6527538901780614\n",
      "Val_Accuracy 8: 0.5372340425531915\n",
      "Average loss epoch 9: 1.0770162597763668\n",
      "Accuracy 9: 0.6333333333333333\n",
      "Average val loss epoch 9: 1.8928024110695347\n",
      "Val_Accuracy 9: 0.5398936170212766\n",
      "Average loss epoch 10: 1.0681537340365566\n",
      "Accuracy 10: 0.6363095238095238\n",
      "Average val loss epoch 10: 1.707971859727259\n",
      "Val_Accuracy 10: 0.4574468085106383\n",
      "Average loss epoch 11: 0.9922200426922159\n",
      "Accuracy 11: 0.6535714285714286\n",
      "Average val loss epoch 11: 1.6746208919526138\n",
      "Val_Accuracy 11: 0.46941489361702127\n",
      "Average loss epoch 12: 0.9644218984049557\n",
      "Accuracy 12: 0.6662202380952381\n",
      "Average val loss epoch 12: 1.5796200959251858\n",
      "Val_Accuracy 12: 0.5186170212765957\n",
      "Average loss epoch 13: 0.8885048819832112\n",
      "Accuracy 13: 0.6821428571428572\n",
      "Average val loss epoch 13: 1.893689290457043\n",
      "Val_Accuracy 13: 0.42021276595744683\n",
      "Average loss epoch 14: 0.8930775551423797\n",
      "Accuracy 14: 0.6895833333333333\n",
      "Average val loss epoch 14: 1.9121428181751268\n",
      "Val_Accuracy 14: 0.4321808510638298\n",
      "Average loss epoch 15: 0.8722206632378797\n",
      "Accuracy 15: 0.6921130952380953\n",
      "Average val loss epoch 15: 1.6976088649139238\n",
      "Val_Accuracy 15: 0.550531914893617\n",
      "Average loss epoch 16: 0.7886554498657001\n",
      "Accuracy 16: 0.7254464285714286\n",
      "Average val loss epoch 16: 1.8419817312795201\n",
      "Val_Accuracy 16: 0.5372340425531915\n",
      "Average loss epoch 17: 0.7476055598505814\n",
      "Accuracy 17: 0.7321428571428571\n",
      "Average val loss epoch 17: 1.4772853725509059\n",
      "Val_Accuracy 17: 0.5638297872340425\n",
      "Average loss epoch 18: 0.7335526872057401\n",
      "Accuracy 18: 0.7366071428571429\n",
      "Average val loss epoch 18: 1.6320331511619413\n",
      "Val_Accuracy 18: 0.5824468085106383\n",
      "Average loss epoch 19: 0.7075916011657607\n",
      "Accuracy 19: 0.7462797619047619\n",
      "Average val loss epoch 19: 1.6074442518896872\n",
      "Val_Accuracy 19: 0.5212765957446809\n",
      "Average loss epoch 20: 0.6873555533814386\n",
      "Accuracy 20: 0.7553571428571428\n",
      "Average val loss epoch 20: 1.6029564420220892\n",
      "Val_Accuracy 20: 0.5704787234042553\n",
      "Average loss epoch 21: 0.6765358001282544\n",
      "Accuracy 21: 0.7616071428571428\n",
      "Average val loss epoch 21: 1.9627861146265608\n",
      "Val_Accuracy 21: 0.4601063829787234\n",
      "Average loss epoch 22: 0.6324192497405274\n",
      "Accuracy 22: 0.7724702380952381\n",
      "Average val loss epoch 22: 1.6106009290336676\n",
      "Val_Accuracy 22: 0.5625\n",
      "Average loss epoch 23: 0.5849605020507415\n",
      "Accuracy 23: 0.7894345238095238\n",
      "Average val loss epoch 23: 1.9492476773702319\n",
      "Val_Accuracy 23: 0.5132978723404256\n",
      "Average loss epoch 24: 0.5763037164617135\n",
      "Accuracy 24: 0.7919642857142857\n",
      "Average val loss epoch 24: 1.5552851085464494\n",
      "Val_Accuracy 24: 0.5611702127659575\n",
      "Average loss epoch 25: 0.5368421904250269\n",
      "Accuracy 25: 0.803422619047619\n",
      "Average val loss epoch 25: 1.7412730244184182\n",
      "Val_Accuracy 25: 0.5412234042553191\n",
      "Average loss epoch 26: 0.5324790225213365\n",
      "Accuracy 26: 0.8071428571428572\n",
      "Average val loss epoch 26: 1.7098891814156818\n",
      "Val_Accuracy 26: 0.5332446808510638\n",
      "Average loss epoch 27: 0.4999586609915432\n",
      "Accuracy 27: 0.8191964285714286\n",
      "Average val loss epoch 27: 1.6794347675421017\n",
      "Val_Accuracy 27: 0.5731382978723404\n",
      "Average loss epoch 28: 0.4860401135725055\n",
      "Accuracy 28: 0.8208333333333333\n",
      "Average val loss epoch 28: 1.8020798307910362\n",
      "Val_Accuracy 28: 0.5212765957446809\n",
      "Average loss epoch 29: 0.48012622827836937\n",
      "Accuracy 29: 0.8251488095238095\n",
      "Average val loss epoch 29: 1.9598258169767044\n",
      "Val_Accuracy 29: 0.5172872340425532\n",
      "Average loss epoch 30: 0.4229080080628151\n",
      "Accuracy 30: 0.844047619047619\n",
      "Average val loss epoch 30: 1.6783697779109703\n",
      "Val_Accuracy 30: 0.5492021276595744\n",
      "Average loss epoch 31: 0.4302869490332322\n",
      "Accuracy 31: 0.8410714285714286\n",
      "Average val loss epoch 31: 2.0021759447514396\n",
      "Val_Accuracy 31: 0.5319148936170213\n",
      "Average loss epoch 32: 0.404646495550695\n",
      "Accuracy 32: 0.8520833333333333\n",
      "Average val loss epoch 32: 1.9514427447689475\n",
      "Val_Accuracy 32: 0.5664893617021277\n",
      "Average loss epoch 33: 0.39182026702259115\n",
      "Accuracy 33: 0.8538690476190476\n",
      "Average val loss epoch 33: 1.929652902009744\n",
      "Val_Accuracy 33: 0.5545212765957447\n",
      "Average loss epoch 34: 0.3938129362099434\n",
      "Accuracy 34: 0.8610119047619048\n",
      "Average val loss epoch 34: 1.7854456305080113\n",
      "Val_Accuracy 34: 0.5638297872340425\n",
      "Average loss epoch 35: 0.3294532448927432\n",
      "Accuracy 35: 0.8819940476190476\n",
      "Average val loss epoch 35: 1.7658960790984277\n",
      "Val_Accuracy 35: 0.5438829787234043\n",
      "Average loss epoch 36: 0.3265267939448328\n",
      "Accuracy 36: 0.8837797619047619\n",
      "Average val loss epoch 36: 1.765053983576289\n",
      "Val_Accuracy 36: 0.5731382978723404\n",
      "Average loss epoch 37: 0.3165159871964179\n",
      "Accuracy 37: 0.8876488095238095\n",
      "Average val loss epoch 37: 1.850464188183833\n",
      "Val_Accuracy 37: 0.5718085106382979\n",
      "Average loss epoch 38: 0.3032997171777289\n",
      "Accuracy 38: 0.8930059523809524\n",
      "Average val loss epoch 38: 1.8294453474936563\n",
      "Val_Accuracy 38: 0.5625\n",
      "Average loss epoch 39: 0.3151113285186737\n",
      "Accuracy 39: 0.8876488095238095\n",
      "Average val loss epoch 39: 2.035564291059581\n",
      "Val_Accuracy 39: 0.5478723404255319\n",
      "Average loss epoch 40: 0.28084690435576476\n",
      "Accuracy 40: 0.9026785714285714\n",
      "Average val loss epoch 40: 1.9140315289649665\n",
      "Val_Accuracy 40: 0.5771276595744681\n",
      "Average loss epoch 41: 0.26120157801373006\n",
      "Accuracy 41: 0.9089285714285714\n",
      "Average val loss epoch 41: 1.837229538131269\n",
      "Val_Accuracy 41: 0.5478723404255319\n",
      "Average loss epoch 42: 0.31483178540555135\n",
      "Accuracy 42: 0.89375\n",
      "Average val loss epoch 42: 2.288782912617955\n",
      "Val_Accuracy 42: 0.5558510638297872\n",
      "Average loss epoch 43: 0.24575014666675823\n",
      "Accuracy 43: 0.9130952380952381\n",
      "Average val loss epoch 43: 2.063640704190577\n",
      "Val_Accuracy 43: 0.5492021276595744\n",
      "Average loss epoch 44: 0.25796859861375004\n",
      "Accuracy 44: 0.9089285714285714\n",
      "Average val loss epoch 44: 1.9097443208995084\n",
      "Val_Accuracy 44: 0.5531914893617021\n",
      "Average loss epoch 45: 0.24837022390795896\n",
      "Accuracy 45: 0.9148809523809524\n",
      "Average val loss epoch 45: 1.9790543628764536\n",
      "Val_Accuracy 45: 0.5279255319148937\n",
      "Average loss epoch 46: 0.2556118323114065\n",
      "Accuracy 46: 0.9095238095238095\n",
      "Average val loss epoch 46: 2.237111587231322\n",
      "Val_Accuracy 46: 0.5478723404255319\n",
      "Average loss epoch 47: 0.2110682140662859\n",
      "Accuracy 47: 0.9272321428571428\n",
      "Average val loss epoch 47: 2.322965735649069\n",
      "Val_Accuracy 47: 0.550531914893617\n",
      "Average loss epoch 48: 0.19975970379958505\n",
      "Accuracy 48: 0.9311011904761904\n",
      "Average val loss epoch 48: 2.0884899757898623\n",
      "Val_Accuracy 48: 0.5585106382978723\n",
      "Average loss epoch 49: 0.20325844226652584\n",
      "Accuracy 49: 0.9313988095238095\n",
      "Average val loss epoch 49: 2.0868600151344783\n",
      "Val_Accuracy 49: 0.5412234042553191\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        totalvalright = 0\n",
    "        nvalbatches = 0\n",
    "        totalvalloss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(val_init)\n",
    "        try:\n",
    "            while True:\n",
    "                accval,valloss = sess.run([accuracy, loss])\n",
    "                totalvalright += accval\n",
    "                nvalbatches += 1\n",
    "                totalvalloss += valloss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "        print('Average val loss epoch {0}: {1}'.format(i, totalvalloss/nvalbatches))\n",
    "        print('Val_Accuracy {0}: {1}'.format(i, totalvalright/nvalbatches)) \n",
    "    prediction = sess.run(preds, feed_dict={txt: valInp})\n",
    "    prediction = np.asarray(prediction)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5403225806451613"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(np.argmax(prediction, 1), labels2.idxmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fooling around with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "#f = np.array([[1,2,3], [1,2,3]])\n",
    "#print(f)\n",
    "g = np.expand_dims(f, axis = 1)\n",
    "g = np.reshape(f, [-1,2])\n",
    "print(g)\n",
    "print(g.shape)\n",
    "print(np.squeeze(g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
