{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Read In "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finnum/train.csv')\n",
    "x = df1.index\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df = pd.concat([df1,df2], ignore_index = True)\n",
    "df['cat_num'] = df['category'].astype('category').cat.codes\n",
    "labels = pd.get_dummies(df['cat_num'][x])\n",
    "labels2 = pd.get_dummies(df['cat_num'][len(x):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInp = np.load('trainChar.npy')\n",
    "valInp = np.load('valChar.npy')\n",
    "embed = np.load('embedChar.npy')\n",
    "embed = embed.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\justi\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "train = (trainInp, labels)\n",
    "val = (valInp, labels2)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create validation Dataset and batch it\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val)\n",
    "val_data = val_data.shuffle(10000) # if you want to shuffle your data\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "val_init = iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 547, 8) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 547, 8, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [3,4,5]\n",
    "embedding_size = embed.shape[1]\n",
    "num_filters1 = 32\n",
    "num_filters2 = 64\n",
    "max_length = 547\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape1 = [filter_size, 1, 1, num_filters1]\n",
    "    W1 = tf.Variable(tf.truncated_normal(filter_shape1, stddev=0.1), name='W1')\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]), name = 'b1')\n",
    "    conv1 = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W1,tf.float32),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='SAME',\n",
    "        name='conv1')\n",
    "    conv1 = tf.layers.batch_normalization(inputs = conv1)\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, tf.cast(b1,tf.float32)), name=\"relu\")\n",
    "    \n",
    "    filter_shape2 = [filter_size, embedding_size, num_filters1, num_filters2]\n",
    "    W2 = tf.Variable(tf.truncated_normal(filter_shape2, stddev=0.1), name='W2')\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]), name = 'b2')    \n",
    "    conv2 = tf.nn.conv2d(\n",
    "        relu1,\n",
    "        tf.cast(W2,tf.float32),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv2')\n",
    "    conv2 = tf.layers.batch_normalization(inputs = conv2)    \n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, tf.cast(b2,tf.float32)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu2,\n",
    "        ksize=[1, max_length - filter_size +1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8, 32, 64]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_shape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pool_3:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'pool_4:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'pool_5:0' shape=(?, 1, 1, 64) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters2 * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 192) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = 800\n",
    "hidden2 = 400\n",
    "out = 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = int(combined_flat.get_shape()[1])\n",
    "w_h = tf.get_variable('weight_h', initializer = tf.random_normal([inp_size,hidden1], mean = 0.0, stddev = .01),dtype = tf.float32)\n",
    "b_h = tf.get_variable('bias_h', initializer = tf.constant(0.0, shape = [1,hidden1]), dtype = tf.float32)\n",
    "\n",
    "w2 = tf.get_variable('weight2', initializer = tf.random_normal([hidden1,hidden2], mean = 0.0, stddev = .01),dtype = tf.float32)\n",
    "b2 = tf.get_variable('bias2', initializer = tf.constant(0.0, shape = [1,hidden2]), dtype = tf.float32)\n",
    "\n",
    "w = tf.get_variable('weight', initializer = tf.random_normal([hidden1,out], mean = 0.0, stddev = .01),dtype = tf.float32)\n",
    "b = tf.get_variable('bias', initializer = tf.constant(0.0, shape = [1,out]), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = tf.nn.leaky_relu(tf.matmul(combined_flat, w_h) + b_h)\n",
    "\n",
    "#h = tf.nn.leaky_relu(tf.matmul(h2, w2) + b2)\n",
    "\n",
    "#drop2 = tf.nn.dropout(h, keep_prob = .6)\n",
    "logits = tf.matmul(h2,w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-9f9d7bb6195e>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = logits)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.3265896566744362\n",
      "Accuracy 0: 0.47619047619047616\n",
      "Average val loss epoch 0: 1.1730959161799004\n",
      "Val_Accuracy 0: 0.5771276595744681\n",
      "Average loss epoch 1: 0.8878861701204663\n",
      "Accuracy 1: 0.6840773809523809\n",
      "Average val loss epoch 1: 0.7851193242884696\n",
      "Val_Accuracy 1: 0.7207446808510638\n",
      "Average loss epoch 2: 0.7244541338866666\n",
      "Accuracy 2: 0.7389880952380953\n",
      "Average val loss epoch 2: 0.7324035446694557\n",
      "Val_Accuracy 2: 0.7313829787234043\n",
      "Average loss epoch 3: 0.5933987545825187\n",
      "Accuracy 3: 0.7879464285714286\n",
      "Average val loss epoch 3: 0.649250141800718\n",
      "Val_Accuracy 3: 0.7792553191489362\n",
      "Average loss epoch 4: 0.5156727180417094\n",
      "Accuracy 4: 0.8136904761904762\n",
      "Average val loss epoch 4: 0.6535254152531319\n",
      "Val_Accuracy 4: 0.7805851063829787\n",
      "Average loss epoch 5: 0.44603047750535463\n",
      "Accuracy 5: 0.8398809523809524\n",
      "Average val loss epoch 5: 0.6237308554192806\n",
      "Val_Accuracy 5: 0.8111702127659575\n",
      "Average loss epoch 6: 0.38383904848312905\n",
      "Accuracy 6: 0.8617559523809524\n",
      "Average val loss epoch 6: 0.6798177088828797\n",
      "Val_Accuracy 6: 0.7898936170212766\n",
      "Average loss epoch 7: 0.32680374954694086\n",
      "Accuracy 7: 0.8831845238095238\n",
      "Average val loss epoch 7: 0.7056358827555433\n",
      "Val_Accuracy 7: 0.8058510638297872\n",
      "Average loss epoch 8: 0.29860085953204407\n",
      "Accuracy 8: 0.8912202380952381\n",
      "Average val loss epoch 8: 0.6776577356330892\n",
      "Val_Accuracy 8: 0.8045212765957447\n",
      "Average loss epoch 9: 0.2556118628165374\n",
      "Accuracy 9: 0.9069940476190477\n",
      "Average val loss epoch 9: 0.8077895256116036\n",
      "Val_Accuracy 9: 0.7872340425531915\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        totalvalright = 0\n",
    "        nvalbatches = 0\n",
    "        totalvalloss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(val_init)\n",
    "        try:\n",
    "            while True:\n",
    "                accval,valloss = sess.run([accuracy, loss])\n",
    "                totalvalright += accval\n",
    "                nvalbatches += 1\n",
    "                totalvalloss += valloss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "        print('Average val loss epoch {0}: {1}'.format(i, totalvalloss/nvalbatches))\n",
    "        print('Val_Accuracy {0}: {1}'.format(i, totalvalright/nvalbatches)) \n",
    "    prediction = sess.run(preds, feed_dict={txt: valInp})\n",
    "    prediction = np.asarray(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7876344086021505"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(np.argmax(prediction, 1), labels2.idxmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "writer1 = tf.summary.FileWriter('./Pgraphs/train', graph = tf.get_default_graph())\n",
    "writer2 = tf.summary.FileWriter('./Pgraphs/val', graph = tf.get_default_graph())\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.80        \n",
    "        train_summary = sess.run(merged_summary_op, feed_dict = {txt:train[0], label:train[1]})\n",
    "        val_summary = sess.run(merged_summary_op, feed_dict= {txt: val[0], label: val[1]})\n",
    "\n",
    "        writer1.add_summary(train_summary, i)\n",
    "        writer1.flush()\n",
    "        writer2.add_summary(val_summary, i)\n",
    "        writer2.flush()\n",
    "        sess.run(val_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
