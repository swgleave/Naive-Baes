{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>idx</th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>target_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98221616</td>\n",
       "      <td>4976</td>\n",
       "      <td>$ARNA APD334 for Amyotrophic Lateral Sclerosis...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82321187</td>\n",
       "      <td>9839</td>\n",
       "      <td>$OCLR Noob investor that i am, put a 7.38 stop...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>stop loss</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103328840</td>\n",
       "      <td>1455</td>\n",
       "      <td>$ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104840294</td>\n",
       "      <td>1111</td>\n",
       "      <td>$TMUS its acquisition of Layer3 TV The purchas...</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>Product Number</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>relative</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69935467</td>\n",
       "      <td>2373</td>\n",
       "      <td>$TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>forecast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94249158</td>\n",
       "      <td>1372</td>\n",
       "      <td>$SEED L2 Capital deal is real savvy. It takes ...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100979260</td>\n",
       "      <td>505</td>\n",
       "      <td>$BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100775772</td>\n",
       "      <td>1210</td>\n",
       "      <td>$WRN My fav $WRN pattern on my watchlist for 1...</td>\n",
       "      <td>Temporal</td>\n",
       "      <td>date</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   idx                                              tweet  \\\n",
       "0   98221616  4976  $ARNA APD334 for Amyotrophic Lateral Sclerosis...   \n",
       "1   82321187  9839  $OCLR Noob investor that i am, put a 7.38 stop...   \n",
       "2  103328840  1455  $ES_F $SPY Bias-2 bearish and the DLT-1 DRR ar...   \n",
       "3  104840294  1111  $TMUS its acquisition of Layer3 TV The purchas...   \n",
       "4   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "5   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "6   69935467  2373  $TWTR ^Buy  $WSTL 68c up 14%  4 time avg vol. ...   \n",
       "7   94249158  1372  $SEED L2 Capital deal is real savvy. It takes ...   \n",
       "8  100979260   505  $BTE $BTE.CA $MEG.CA $CPG $CPG.CA $CJ.CA - 4th...   \n",
       "9  100775772  1210  $WRN My fav $WRN pattern on my watchlist for 1...   \n",
       "\n",
       "         category     subcategory target_num  \n",
       "0  Product Number  Product Number        334  \n",
       "1        Monetary       stop loss       7.38  \n",
       "2  Product Number  Product Number          1  \n",
       "3  Product Number  Product Number          5  \n",
       "4      Percentage        relative         14  \n",
       "5        Quantity        Quantity          4  \n",
       "6        Monetary        forecast          5  \n",
       "7        Temporal            date         33  \n",
       "8        Temporal            date          4  \n",
       "9        Temporal            date         11  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('finnum/train.csv')\n",
    "x = df1.index\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df = pd.concat([df1,df2], ignore_index = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for encoding categories as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_num'] = df['category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load('fastText1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing tweets by lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lower'] = [x.lower() for x in df.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing target with <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to training, making this into a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mod'] = df.apply(lambda x: flagNum(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd <num>  for amyotrophic lateral scler...\n",
       "1    $oclr noob investor that i am, put a  <num>  s...\n",
       "2    $es_f $spy bias-2 bearish and the dlt- <num>  ...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up  <num> %  4 time avg ...\n",
       "Name: mod, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mod'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $arna apd334 for amyotrophic lateral sclerosis...\n",
       "1    $oclr noob investor that i am, put a 7.38 stop...\n",
       "2    $es_f $spy bias-2 bearish and the dlt-1 drr ar...\n",
       "3    $tmus its acquisition of layer3 tv the purchas...\n",
       "4    $twtr ^buy  $wstl 68c up 14%  4 time avg vol. ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out numbers and words less than 3 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped = [textPuncandNum(text) for text in df['mod']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list of unique words from this processed text, excluding <num\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14064"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = list(stripped)\n",
    "low = ' '.join(low)\n",
    "low = list(set(low.split()))\n",
    "low.remove('<num>')\n",
    "len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breakdown...could',\n",
       " 'price...she',\n",
       " 'already.',\n",
       " 'asleep.',\n",
       " 'pylori',\n",
       " 'ago',\n",
       " 'breach',\n",
       " 'least.',\n",
       " 'hrs.',\n",
       " 'somewhere',\n",
       " 'nov.',\n",
       " 'tuck',\n",
       " 'sep',\n",
       " 'zuma',\n",
       " 'sure',\n",
       " 'iffy',\n",
       " 'inside',\n",
       " 'nak',\n",
       " '&quot;crypto&quot;',\n",
       " 'obvsly',\n",
       " 'ideas?',\n",
       " 'bullshjt.',\n",
       " 'spy',\n",
       " 'fack',\n",
       " 'b...bidu',\n",
       " 'sleeping',\n",
       " 'back-',\n",
       " 'certain!',\n",
       " '#markets',\n",
       " 'adami',\n",
       " 'tecfidera',\n",
       " '$jag',\n",
       " 'woes.',\n",
       " 'offering/backstop',\n",
       " 'quarter',\n",
       " 'effect-',\n",
       " 'justification',\n",
       " '$cpg.ca',\n",
       " 'favourite',\n",
       " '$egn',\n",
       " 'finish!!',\n",
       " 'inpx.target',\n",
       " 'cake',\n",
       " 'myself',\n",
       " 'retirement',\n",
       " '$cgnx',\n",
       " 'resumption.',\n",
       " 'tgt..',\n",
       " '$ears',\n",
       " 'prts&#;',\n",
       " 'only!?!',\n",
       " 'shareholders.',\n",
       " 'van',\n",
       " '$biib',\n",
       " 'usually',\n",
       " 'possible.',\n",
       " 'untouched.',\n",
       " 'old?',\n",
       " 'fleas,',\n",
       " 'divs.',\n",
       " 'adjust',\n",
       " 'report.',\n",
       " 'overtook',\n",
       " '$ltc.x',\n",
       " 'nikkei',\n",
       " 'analog',\n",
       " 'stability',\n",
       " 'gdax',\n",
       " '$trx.x',\n",
       " '$inda',\n",
       " 'oooops',\n",
       " 'emerald',\n",
       " '$/per?',\n",
       " 'expected.',\n",
       " 'failed/flopped',\n",
       " 'pixel',\n",
       " 'fcel',\n",
       " 'thumbs',\n",
       " 'chfs',\n",
       " 'tech.',\n",
       " 'moneyflow',\n",
       " 'hand.....lmao!',\n",
       " 'looooooooong!',\n",
       " 'mind',\n",
       " 'shorts.',\n",
       " 'exits',\n",
       " '$deck',\n",
       " 'housing',\n",
       " 'moment,',\n",
       " 'holidays.',\n",
       " 'woken',\n",
       " 'cholangitis',\n",
       " '&quot;premium&quot;',\n",
       " '$nwbo',\n",
       " '#tothemoon',\n",
       " '&quot;bofa',\n",
       " '$jnj',\n",
       " 'dma..',\n",
       " 'underwriter',\n",
       " 'drinking',\n",
       " 'pipelines.',\n",
       " 'fill...',\n",
       " 'smallcap',\n",
       " 'takes,',\n",
       " 'moments',\n",
       " '$cori',\n",
       " '$study',\n",
       " 'normal',\n",
       " 'hence',\n",
       " '#gold.',\n",
       " 'herself...she',\n",
       " 'mins.',\n",
       " 'week*',\n",
       " '$stng',\n",
       " '#eglt',\n",
       " 'existing',\n",
       " 'signal.',\n",
       " 'dut+',\n",
       " '@kgb_general',\n",
       " '$rdd.x',\n",
       " 'enough.',\n",
       " 'margin=b',\n",
       " 'happier',\n",
       " 'it&#;s',\n",
       " 'tank',\n",
       " '---------works',\n",
       " '..i,ll',\n",
       " 'voyage',\n",
       " '$relv',\n",
       " 'impressed.',\n",
       " 'coiled',\n",
       " '$geely',\n",
       " 'light',\n",
       " 'now?',\n",
       " 'wma.',\n",
       " 'world',\n",
       " 'equity,the',\n",
       " 'https://youtu.be/mwbswifoae',\n",
       " 'profit',\n",
       " '$eufn',\n",
       " 'highs.',\n",
       " 'fail.',\n",
       " 'regressive',\n",
       " 'realtime',\n",
       " 'own!',\n",
       " 'rgly',\n",
       " 'infi.',\n",
       " 'aloha',\n",
       " 'portfolio,',\n",
       " 'mega',\n",
       " '$rueif,',\n",
       " 'live',\n",
       " 'institutional.',\n",
       " 'vacuum.',\n",
       " 'losses',\n",
       " 'hqy',\n",
       " 'barrel.',\n",
       " '$billion?',\n",
       " 'bitch!',\n",
       " 'shipper',\n",
       " 'cyclin',\n",
       " 'course:)',\n",
       " 'res.',\n",
       " 'away...',\n",
       " 'from',\n",
       " 'bribes',\n",
       " '$mnga',\n",
       " 'talking',\n",
       " '~dec)',\n",
       " 'golden',\n",
       " 'chartists?',\n",
       " 'her.',\n",
       " 'tbv,',\n",
       " 'gta',\n",
       " 'basically',\n",
       " 'market..',\n",
       " 'goo.gl/iuxpt',\n",
       " 'plunger',\n",
       " '$tti',\n",
       " 'etc,',\n",
       " 'stream',\n",
       " '$wkhs',\n",
       " 'that?',\n",
       " 'doubled',\n",
       " '$mplx',\n",
       " 'throughput..',\n",
       " 'iqiyi',\n",
       " 'designs',\n",
       " 'plant-based',\n",
       " 'server.',\n",
       " 'booze.',\n",
       " 'silly',\n",
       " 'dumb.',\n",
       " '(p&lt;.)',\n",
       " 'http://stockcharts.com/c-sc/sc?s=gld&amp;p=d&amp;yr=&amp;mn=&amp;dy=&amp;id=p&amp;a=&amp;r=',\n",
       " 'setting',\n",
       " 'more...',\n",
       " 'unlikely',\n",
       " 'felt',\n",
       " 'featherflunky',\n",
       " '$hsgx.',\n",
       " 'geniuses',\n",
       " 'companies,',\n",
       " 'london',\n",
       " 'topic',\n",
       " 'expect',\n",
       " '$achc',\n",
       " 'all..',\n",
       " '$btceur',\n",
       " 'journalism',\n",
       " 'convos',\n",
       " 'quick,',\n",
       " 'working...lol',\n",
       " 'fools!',\n",
       " '$intc.',\n",
       " 'might.',\n",
       " 'that,',\n",
       " 'countries',\n",
       " '(emphasis:',\n",
       " '@solarpvinvestor',\n",
       " 'report),',\n",
       " 'jan/dec',\n",
       " 'hands.....evercore',\n",
       " 'ruin',\n",
       " 'ownership;',\n",
       " 'panels',\n",
       " 'white',\n",
       " '$soxl',\n",
       " 'goooooo!',\n",
       " 'azzholes.',\n",
       " 'ago:',\n",
       " '$oild',\n",
       " 'order',\n",
       " 'ride...hopefully',\n",
       " '$vxx',\n",
       " '$adap.',\n",
       " '$ryb',\n",
       " 'dominates',\n",
       " 'hands!!!',\n",
       " 'after,',\n",
       " '$azn',\n",
       " 'http://www.seekingalpha.com/news/http://www.seekingalpha.com/news/',\n",
       " 'cormedix',\n",
       " 'let',\n",
       " 'arc',\n",
       " 'directions.',\n",
       " 'listen.',\n",
       " 'fathom',\n",
       " '$isrg',\n",
       " 'just',\n",
       " 'post',\n",
       " 'partial',\n",
       " '$bhf',\n",
       " 'jokes',\n",
       " '$bbox',\n",
       " 'march',\n",
       " 'kidney',\n",
       " 'points,',\n",
       " 'guarantee',\n",
       " 'decuc.',\n",
       " 'pit.',\n",
       " 'could&#;ve',\n",
       " 'often',\n",
       " '$fhn',\n",
       " ',which',\n",
       " 'math',\n",
       " 'strike',\n",
       " 'slideshow',\n",
       " 'more?',\n",
       " 'saboteurs',\n",
       " 'cheapies.$.,',\n",
       " 'guy.',\n",
       " 'became',\n",
       " 'incoming.',\n",
       " 'rocket,',\n",
       " 'concerned',\n",
       " 'heavy',\n",
       " 'floored.',\n",
       " 'opportunity.',\n",
       " 'having.',\n",
       " 'mdr',\n",
       " '@brnlry',\n",
       " 'putting',\n",
       " '$win',\n",
       " 'you',\n",
       " '$ngas',\n",
       " 'incredible',\n",
       " 'continuation',\n",
       " 'gpu',\n",
       " 'you&#;re',\n",
       " 'https://www.youtube.com/watch?v=phvpabzbti',\n",
       " '$...this',\n",
       " '$crbp',\n",
       " 'pre-mkt',\n",
       " 'own)...by',\n",
       " '$dia...compared',\n",
       " 's,then,it&#;d',\n",
       " 'shook',\n",
       " 'listened',\n",
       " 'obviously',\n",
       " 'qatar',\n",
       " 'underway.',\n",
       " 'minted',\n",
       " 'session,',\n",
       " 'wear',\n",
       " '$slv,',\n",
       " 'ark',\n",
       " 'one!',\n",
       " 'opening-',\n",
       " 'industrials.',\n",
       " '$wing',\n",
       " 'inst.,',\n",
       " 'blood',\n",
       " 'narbor',\n",
       " 'begin',\n",
       " 'here&gt;$.',\n",
       " 'little....one',\n",
       " 'bloat',\n",
       " 'finish',\n",
       " '$lkflf',\n",
       " 'baby',\n",
       " 'http://performance.morningstar.com/stock/performance-return.action?p=dividend_split_page&amp;t=t&amp;region=usa&amp;culture=en-us',\n",
       " '$gfa',\n",
       " 'insulin',\n",
       " 'us..&quot;',\n",
       " 'going,',\n",
       " '$knx',\n",
       " 'minus',\n",
       " 'ali',\n",
       " 'nicely.',\n",
       " 'service',\n",
       " 'claims',\n",
       " '&#;big',\n",
       " 'stealth',\n",
       " '$btcd.x',\n",
       " '-us-ep-outlook/',\n",
       " 'currrently',\n",
       " 'front',\n",
       " '$eca.ca',\n",
       " 'https://weather.com/maps/tendayforecast',\n",
       " 'ipad',\n",
       " 'valuable',\n",
       " 'mat&#;ls',\n",
       " 'bargain!',\n",
       " 'costs.',\n",
       " '$xauusd',\n",
       " 'pennies',\n",
       " 'q-guides-higher/',\n",
       " 'shoppings',\n",
       " 'matrix',\n",
       " '$tpic',\n",
       " '$foxa~x',\n",
       " 'https://www.dispatchtribunal.com////analysts-set-kratos-defense-security-solutions-inc-ktos-target-price-at-',\n",
       " 'm&amp;a',\n",
       " 'holder,',\n",
       " 'myopic',\n",
       " 'brings',\n",
       " 'come',\n",
       " '$oesx',\n",
       " 'watching!',\n",
       " 'fair',\n",
       " 'confident.',\n",
       " '$bjri',\n",
       " '$avgr',\n",
       " '$aker',\n",
       " '$trillion',\n",
       " 'tis',\n",
       " '&quot;s&amp;p',\n",
       " '$sltd',\n",
       " 'xbox',\n",
       " 'earlier.',\n",
       " '$dlg.f',\n",
       " 'signal...',\n",
       " '$data',\n",
       " 'dead.',\n",
       " '#coinmarketcap',\n",
       " '$arna',\n",
       " 'lrcx',\n",
       " 'exercised',\n",
       " 'turkey',\n",
       " 'pricing',\n",
       " 'risk/reward.',\n",
       " 'excluding',\n",
       " 'ideal',\n",
       " 'https://www.youtube.com/watch?v=mnevbuw_kc',\n",
       " '&#;inventory&#;,has',\n",
       " 'buyers?',\n",
       " 'can',\n",
       " 'relief',\n",
       " 'participation',\n",
       " '$cbrl',\n",
       " 'snap.',\n",
       " 'damn',\n",
       " '$upro',\n",
       " 'magnet',\n",
       " 'broken',\n",
       " 'india,',\n",
       " 'madalena&#;s',\n",
       " 'lots',\n",
       " 'reversal!',\n",
       " 'line...short',\n",
       " '$peix',\n",
       " 'boosting',\n",
       " '$tcehy',\n",
       " '@jbrodzz',\n",
       " 'robust',\n",
       " 'nyse',\n",
       " 'sgn-lva',\n",
       " 'climbing.',\n",
       " 'audi',\n",
       " 'at/near',\n",
       " 'uncharacteristically',\n",
       " 'hivinfectious',\n",
       " 'imbalance',\n",
       " 'acct',\n",
       " 'row.',\n",
       " '$agn',\n",
       " '..that',\n",
       " '@mukri',\n",
       " 'twitter',\n",
       " '$xone,',\n",
       " 'accumulating...will',\n",
       " '(closing',\n",
       " 'first!',\n",
       " 'money~didn&#;t',\n",
       " 'consol',\n",
       " 'overbought',\n",
       " '$brzu',\n",
       " '$xbit',\n",
       " 'restruct',\n",
       " 'bots.',\n",
       " 'mazor',\n",
       " 'probe.',\n",
       " 'resist',\n",
       " 'houses',\n",
       " '$pkd',\n",
       " '$bit.x',\n",
       " 'dec:',\n",
       " '#breakoutpotential',\n",
       " 'kim',\n",
       " 'delaying',\n",
       " 'dow',\n",
       " 'today...#wastedmoney',\n",
       " 'innat',\n",
       " 'imagine',\n",
       " 'scientific',\n",
       " 'breakout?',\n",
       " 'starting....',\n",
       " 'totaled',\n",
       " '$sgms',\n",
       " '$cl_f',\n",
       " 'team...promised',\n",
       " 'coolisys',\n",
       " 'importance',\n",
       " 'juno&#;s',\n",
       " 'yeeee',\n",
       " 'jumping',\n",
       " '$fnko',\n",
       " '$vygr',\n",
       " 'converted',\n",
       " 'material',\n",
       " '$btl',\n",
       " 'stronger:https://globenewswire.com/news-release/',\n",
       " '$oas',\n",
       " 'sham',\n",
       " '$pir',\n",
       " 'estimates,',\n",
       " 'target+',\n",
       " '$hao',\n",
       " 'maris',\n",
       " '$gc_f',\n",
       " 'louis',\n",
       " 'street.&quot;',\n",
       " 'macys',\n",
       " 'psychosis,',\n",
       " 'e-commerce.',\n",
       " 'crisis',\n",
       " 'but,',\n",
       " 'natural',\n",
       " 'here&#;s',\n",
       " 'on&quot;...',\n",
       " '$mdr',\n",
       " 'pcg',\n",
       " 'admits',\n",
       " 'benzinga',\n",
       " 'brutal',\n",
       " '$aal',\n",
       " 'tom.',\n",
       " 'ripple',\n",
       " 'thanksgiving.',\n",
       " 'triggers',\n",
       " 'ifs',\n",
       " 'personally',\n",
       " 'much',\n",
       " '$orig',\n",
       " 'mon',\n",
       " 'build',\n",
       " 'yesterday.all',\n",
       " 'https://www.cnbc.com////billionaire-ron-baron-says-any-patient-investor-can-turn--dollars-a-year-into-nearly-',\n",
       " '$wynn',\n",
       " '$bbva',\n",
       " 'size',\n",
       " 'wish',\n",
       " 'difficulty',\n",
       " 'laziness',\n",
       " 'debacle.',\n",
       " 'involvement',\n",
       " 'pullback',\n",
       " 'coincidence?',\n",
       " 'rand',\n",
       " 'karma',\n",
       " 'mos.',\n",
       " 'talks.',\n",
       " 'able',\n",
       " '#aapl',\n",
       " 'cnbc',\n",
       " 'http://pattern-based-trading.com////important-market-update----after-market-close/',\n",
       " 'weekend,',\n",
       " '$usdcad',\n",
       " 'boyhasnoshame',\n",
       " 'place,',\n",
       " 's,than',\n",
       " 'weren&#;t',\n",
       " '$kldx',\n",
       " '&quot;pump',\n",
       " 'ice',\n",
       " 'msft.',\n",
       " '#retail',\n",
       " '$tho,',\n",
       " 'https://www.thestreet.com/story///westfield-being-bought-out-by-unibail-rodamco.html',\n",
       " 'lil',\n",
       " 'point;',\n",
       " 'row',\n",
       " 'etc.',\n",
       " '$amj',\n",
       " 'pls',\n",
       " 'account',\n",
       " 'cold',\n",
       " 'more;',\n",
       " 'square.',\n",
       " '(winter',\n",
       " 'never',\n",
       " 'https://www.washingtonpost.com/news/wonk/wp////cvs-agrees-to-buy-aetna-in-',\n",
       " 'impaired',\n",
       " 'stretch',\n",
       " 'politics..',\n",
       " 'growth.',\n",
       " '$dmm.v',\n",
       " 'group',\n",
       " 'nears',\n",
       " 'spike',\n",
       " 'woodshed',\n",
       " 'physical',\n",
       " '$,others',\n",
       " 'purchased',\n",
       " 'decides',\n",
       " 'courts',\n",
       " 'top-line',\n",
       " ',expect',\n",
       " 'freeze?',\n",
       " 'halt',\n",
       " 'phone',\n",
       " 'small',\n",
       " 'patiently..',\n",
       " 'uniquely-induced',\n",
       " 'back...significantly',\n",
       " 'valued',\n",
       " '$trov,',\n",
       " 'everyone:',\n",
       " 'quality&quot;',\n",
       " 'transactions.',\n",
       " 'equities.',\n",
       " 'stent',\n",
       " 'weight',\n",
       " 'eow',\n",
       " 'stair-stepping',\n",
       " '$ipo',\n",
       " 'drain',\n",
       " 'company?',\n",
       " 'mine.',\n",
       " '&quot;bears',\n",
       " 'garden',\n",
       " 'taken,',\n",
       " 'ins',\n",
       " 'avg..going',\n",
       " '$./share',\n",
       " 'advanced',\n",
       " '$ypf',\n",
       " 'over:',\n",
       " '$symc',\n",
       " '$var',\n",
       " 'dollars',\n",
       " 'plus,',\n",
       " '.,,but',\n",
       " ',,then',\n",
       " 'comparison:',\n",
       " '$rada',\n",
       " 'http://bit.ly/eotcy',\n",
       " 'someone',\n",
       " '$btc.x',\n",
       " '$uslv.',\n",
       " 'leader',\n",
       " '$rcii',\n",
       " 'wreck',\n",
       " 'prepare',\n",
       " 'since.',\n",
       " 'calls)',\n",
       " 'votes',\n",
       " 'flipped',\n",
       " 'wants',\n",
       " 'simultaneously.',\n",
       " 'drr',\n",
       " '$bwld',\n",
       " 'delayed',\n",
       " '(otc,',\n",
       " 'depreciation',\n",
       " 'accumulation.',\n",
       " 'hung',\n",
       " 'successes',\n",
       " '$ivv',\n",
       " 'thanks.',\n",
       " 'fiasco',\n",
       " '$eltk',\n",
       " '$pegi',\n",
       " 'experts',\n",
       " '$has',\n",
       " 'running!!',\n",
       " 'they',\n",
       " '$crm',\n",
       " 'banking!',\n",
       " 'winter.',\n",
       " '$sndx',\n",
       " 'yup!',\n",
       " 'contacting',\n",
       " 'scaremongering',\n",
       " 'physics',\n",
       " 'crooked',\n",
       " 'day..hmm.',\n",
       " 'nevermind,',\n",
       " 'congrats',\n",
       " 'bitfinex:ltcusd',\n",
       " 'chinese',\n",
       " '$rgc',\n",
       " 'cloud',\n",
       " '$spy,',\n",
       " 'single',\n",
       " 'bod?',\n",
       " '(yellow',\n",
       " 'reveal',\n",
       " '$xvg.x',\n",
       " 'stanley&#;s',\n",
       " 'spirit',\n",
       " 'disease',\n",
       " '$bpmp',\n",
       " 'slated',\n",
       " 'today...evercore',\n",
       " '@terribletrader',\n",
       " '(bats)',\n",
       " 'differential',\n",
       " '$gold',\n",
       " 'hard,',\n",
       " '$msft,',\n",
       " '@yen_hee',\n",
       " 'whacko.',\n",
       " '$splk',\n",
       " 'tutes..now',\n",
       " 'cafd',\n",
       " 'idiocracy,',\n",
       " 'thesis',\n",
       " 'today?first',\n",
       " 'o&amp;g',\n",
       " 'valuations:',\n",
       " 'payouts',\n",
       " 'ride!',\n",
       " 'reached',\n",
       " '$jdst.',\n",
       " 'gone-',\n",
       " 'confirmation.',\n",
       " '$twlo',\n",
       " '$snap',\n",
       " 'lit!!',\n",
       " 'pause',\n",
       " '$rovio',\n",
       " 'forbes',\n",
       " 'green,',\n",
       " 'crytocurrency',\n",
       " 'takeout',\n",
       " 'sore....',\n",
       " 'hah.',\n",
       " 'https://youtu.be/hrycwumfc',\n",
       " '-dmas',\n",
       " '$push',\n",
       " 'https://seekingalpha.com/article/-hibbett-reports-strong-q',\n",
       " 'unloaded',\n",
       " 'w/china&#;s',\n",
       " 'believe)',\n",
       " 'forks',\n",
       " '(rate',\n",
       " 'buy,',\n",
       " 'humans',\n",
       " 'manufacturers',\n",
       " 'separate',\n",
       " 'turkey-day',\n",
       " 'mad',\n",
       " 'milly',\n",
       " 'goes.',\n",
       " '-stores-will-have-the-best-cyber-monday-deals-in-/',\n",
       " 'proposition',\n",
       " 'read,',\n",
       " 'performing',\n",
       " 'butts.',\n",
       " 'toe',\n",
       " 'jeans',\n",
       " 'image',\n",
       " 'https://www.youtube.com/watch?v=_tnuslsaaa',\n",
       " 'dropp',\n",
       " 'extension',\n",
       " '&quot;canna',\n",
       " 'status',\n",
       " 'mkt',\n",
       " 'algos',\n",
       " 'juno',\n",
       " 'diversified',\n",
       " '-things-kratos-defense-security-.html',\n",
       " '$sino',\n",
       " 'promotion',\n",
       " 'brother',\n",
       " '@mallet',\n",
       " '$evep',\n",
       " '$pcmi',\n",
       " 'chapman',\n",
       " 'place',\n",
       " '$cphi',\n",
       " 'statement',\n",
       " 'morning!!',\n",
       " '$trmb',\n",
       " 'crash..',\n",
       " 'increasing',\n",
       " '(cenx)',\n",
       " 'divs',\n",
       " 'boatload',\n",
       " '$wnr',\n",
       " 'fill.',\n",
       " '$mzor',\n",
       " 'trucks',\n",
       " '@dblock',\n",
       " 'reaffirms',\n",
       " 'tvix',\n",
       " 'investr',\n",
       " 'quickly!',\n",
       " 'uctt&#;s',\n",
       " 'cell',\n",
       " '%&quot;',\n",
       " '&#;users&#;',\n",
       " '$insg',\n",
       " 'indicator/overlay',\n",
       " 'change',\n",
       " 'review...',\n",
       " 'corner.',\n",
       " 'adios',\n",
       " 'feinberg',\n",
       " 'gregory',\n",
       " 'drop!',\n",
       " 'wife,',\n",
       " '#luckylou',\n",
       " '$fosl',\n",
       " 'trend...',\n",
       " 'winners!!',\n",
       " '@wouter',\n",
       " 'through',\n",
       " 'impulsive',\n",
       " '$dang',\n",
       " 'crashing,',\n",
       " 'casino.',\n",
       " 'dumb!',\n",
       " 'lift',\n",
       " 'citizen',\n",
       " 'undervalued',\n",
       " 'sears',\n",
       " 'large',\n",
       " 'pullbacks',\n",
       " 'yoy.',\n",
       " 'helps',\n",
       " 'minocycline',\n",
       " 'discount..see',\n",
       " 'hod',\n",
       " 'isn&#;t',\n",
       " '$cohu',\n",
       " 'bounces',\n",
       " 'study:',\n",
       " 'followers',\n",
       " '#mgt',\n",
       " '$slb',\n",
       " 'report:',\n",
       " 'ptn',\n",
       " 'crisis.',\n",
       " 'k-cup',\n",
       " '$ebay.',\n",
       " 'shuts',\n",
       " 'sit',\n",
       " '$ath.ca',\n",
       " 'limit',\n",
       " '$hpq',\n",
       " 'rounds.',\n",
       " 'gold,',\n",
       " 'lies',\n",
       " 'proper',\n",
       " 'm.this',\n",
       " 'hikes',\n",
       " 'homemade',\n",
       " 'capitalism.',\n",
       " 'cdna',\n",
       " 'quarter.',\n",
       " '$drna',\n",
       " 'wks',\n",
       " 'invested',\n",
       " 'demand.',\n",
       " 'dips,',\n",
       " 'win!',\n",
       " '#blozf',\n",
       " '$dnn',\n",
       " 'count.',\n",
       " 'close!',\n",
       " 'days.i',\n",
       " '$kbsf',\n",
       " 'waited.',\n",
       " 'surgery.',\n",
       " '&quot;had',\n",
       " 'http://preciousmetalwatch.com/john-kaiser-why-the-gold-price-could-hit--in-the-next-',\n",
       " '@riddthy',\n",
       " 'upsurge',\n",
       " 'similarly,',\n",
       " 'flip,',\n",
       " 'breakout',\n",
       " 'tightening.',\n",
       " 'hod.',\n",
       " '$clr',\n",
       " 'refi&#;d',\n",
       " 'robotics',\n",
       " 'laughing',\n",
       " 'glty.',\n",
       " '$iot.x',\n",
       " 'gonna',\n",
       " 'rapidly.',\n",
       " '$wcps',\n",
       " 'all.',\n",
       " 'excited',\n",
       " 'finance.',\n",
       " '$tops,',\n",
       " 'yeah.',\n",
       " '$symx',\n",
       " 'models.',\n",
       " 'cush',\n",
       " 'eom!',\n",
       " 'too,',\n",
       " 'announced',\n",
       " 'rates',\n",
       " '/was',\n",
       " 'series.',\n",
       " 'end.',\n",
       " '$rnn,',\n",
       " 'susquehanna',\n",
       " 'conf',\n",
       " 'records',\n",
       " 'diluting',\n",
       " 'amd:',\n",
       " 'investment?',\n",
       " 'viacom',\n",
       " 'end...',\n",
       " '$gdot',\n",
       " 'bombing',\n",
       " 'squeeze...',\n",
       " 'although',\n",
       " 'groundbreaking',\n",
       " 'week.exciting',\n",
       " 'term!',\n",
       " 'news?!',\n",
       " 'prts',\n",
       " '$dcth',\n",
       " 'explode',\n",
       " '$bbw',\n",
       " '$dbvt',\n",
       " '$genc',\n",
       " 'corruption!!',\n",
       " 'overvalued,',\n",
       " 'cents?',\n",
       " 'reentering',\n",
       " '$osx',\n",
       " 'yielding',\n",
       " 'lesson',\n",
       " 'gains',\n",
       " 'such',\n",
       " 'wanting',\n",
       " 'doubt)',\n",
       " 'uptrend',\n",
       " 'october',\n",
       " 'bad...',\n",
       " 'doubt.',\n",
       " '$egbn',\n",
       " 'skorney',\n",
       " 'opposite',\n",
       " 'happens,',\n",
       " 'wave',\n",
       " '($mill',\n",
       " '.very',\n",
       " 'stick',\n",
       " 'deal;',\n",
       " 'genius.',\n",
       " 'cibc...lol',\n",
       " '$cot,',\n",
       " 'keller(athlon,ryzen,apple',\n",
       " '$ewh',\n",
       " 'heat!',\n",
       " 'limited',\n",
       " '$frta',\n",
       " '$psti',\n",
       " 'paypal',\n",
       " '$sify',\n",
       " 'solid,',\n",
       " 'data.',\n",
       " 'overdone,',\n",
       " 'juice',\n",
       " '$anth',\n",
       " '(swing)',\n",
       " '$cbli.',\n",
       " 'ticker,',\n",
       " '$googl,',\n",
       " 'checkout',\n",
       " 'aug/early',\n",
       " 'yoy',\n",
       " '$tcs',\n",
       " '$ptla',\n",
       " 'strength',\n",
       " 'sales]',\n",
       " 'wouldnt',\n",
       " '@brianq',\n",
       " '$ewz',\n",
       " 'choice.',\n",
       " 'violent',\n",
       " '$.cents',\n",
       " 'qrts',\n",
       " '$cost)',\n",
       " ',here',\n",
       " 'https://www.lesechos.fr/tech-medias/hightech/-blackberry-senvole-en-bourse-grace-a-ses-logiciels-.php',\n",
       " 'rules',\n",
       " 'failed',\n",
       " 'your',\n",
       " 'bod',\n",
       " 'naeve',\n",
       " '$epix',\n",
       " 'funded.',\n",
       " 'http://stks.co/ixdv',\n",
       " 'effort',\n",
       " 'hodl!!',\n",
       " '$mrdn',\n",
       " 'cake.',\n",
       " 'screws',\n",
       " 'fast!',\n",
       " 'shouting',\n",
       " 'tiny',\n",
       " 'afraid',\n",
       " 'longer,',\n",
       " 'bad',\n",
       " 'great....just',\n",
       " 'divy.',\n",
       " 'unbelievable',\n",
       " 'small.',\n",
       " 'ups',\n",
       " 'iota',\n",
       " 'following:',\n",
       " 'abnormal',\n",
       " 'deep',\n",
       " '#madalena',\n",
       " 'sting',\n",
       " 'approve',\n",
       " 'gtav',\n",
       " '$cmcsa',\n",
       " 'october,',\n",
       " 'presentation',\n",
       " 'promotions',\n",
       " 'last',\n",
       " 'you&#;ve',\n",
       " 'sweet,',\n",
       " 'shannon',\n",
       " 'priority.',\n",
       " 'tax',\n",
       " 'vuzi&#;s',\n",
       " 'whitman',\n",
       " '$msft',\n",
       " 'oopsy!',\n",
       " 'equifax,',\n",
       " 'scalp',\n",
       " 'producer',\n",
       " 'cuts?',\n",
       " '&quot;alibaba',\n",
       " 'brokers,',\n",
       " 'acquisition',\n",
       " '@reformedtrader',\n",
       " 'stream.',\n",
       " 'nvidia',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to get embeddingss for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.wv[low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14064, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing these unqique words in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {k: v for v, k in enumerate(low)}\n",
    "label_dict['<num>'] = len(label_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding <num\\> back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict['<num>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding filler word to keep all tweets the same length. Then replacing all words with their dictionary equivalent. This is for tenssorflows matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufferIndex = len(label_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "modifiedText = [[label_dict[word] for word in text.split()]for text in stripped]\n",
    "maxLen = max(map(len, modifiedText))\n",
    "for item in modifiedText:                # for each item in the list\n",
    "    while len(item) < maxLen:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex) \n",
    "numpyInp = np.asarray(modifiedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  376,  7042, 14064, ..., 14065, 14065, 14065],\n",
       "       [ 5341, 10242,  1935, ..., 14065, 14065, 14065],\n",
       "       [14011,  7228, 12522, ..., 14065, 14065, 14065],\n",
       "       ...,\n",
       "       [ 5686,  6917, 11357, ..., 14065, 14065, 14065],\n",
       "       [10199,  6174, 11531, ..., 14065, 14065, 14065],\n",
       "       [ 8362,  8574,  5462, ..., 14065, 14065, 14065]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpyInp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in unique embeddings for <num\\> and filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.vstack((embed, np.zeros(100)+20, np.zeros(100)+0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14066, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = numpyInp[x,:]\n",
    "val = numpyInp[len(x):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7450"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape[0] + train.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#do again for validation set\n",
    "\n",
    "df2 = pd.read_csv('finnum/dev.csv')\n",
    "df2['cat_num'] = df2['category'].astype('category').cat.codes\n",
    "model2 = FastText.load('Gensim FastText Vectors/fastText1')\n",
    "df2['lower'] = [x.lower() for x in df2.tweet]\n",
    "import re\n",
    "def flagNum(x):\n",
    "    text = x.lower\n",
    "    outNum = str(x.target_num)\n",
    "    text_out = re.sub(r'(?<=\\D)'+outNum+'(?=\\D)', ' <num> ', text)\n",
    "    #text_out = text.replace('\\D('+outNum+')\\D', ' <num> ')\n",
    "    return text_out\n",
    "df2['mod'] = df2.apply(lambda x: flagNum(x), axis = 1)\n",
    "import string\n",
    "def textPuncandNum(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if len(word.translate(table))>2]\n",
    "    return ' '.join(text)\n",
    "stripped2 = [textPuncandNum(text) for text in df2['mod']]\n",
    "low2 = list(stripped2)\n",
    "low2 = ' '.join(low2)\n",
    "low2 = list(set(low2.split()))\n",
    "low2.remove('<num>')\n",
    "embed2 = model2.wv[low2]\n",
    "label_dict2 = {k: v for v, k in enumerate(low2)}\n",
    "label_dict2['<num>'] = len(label_dict2) \n",
    "label_dict2['<num>']\n",
    "bufferIndex2 = len(label_dict2) \n",
    "import numpy as np\n",
    "modifiedText2 = [[label_dict2[word] for word in text.split()]for text in stripped2]\n",
    "maxLen2 = max(map(len, modifiedText2))\n",
    "for item in modifiedText2:                # for each item in the list\n",
    "    while len(item) < maxLen2:            # while the item length is smaller than maxLen\n",
    "        item.append(bufferIndex2) \n",
    "numpyInp2 = np.asarray(modifiedText2)\n",
    "embed2 = np.vstack((embed2, np.zeros(100)+20, np.zeros(100)+25))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embed2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the embedding matrix for any input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInp = numpyInp[x,:]\n",
    "valInp = numpyInp[len(x):,:]\n",
    "\n",
    "labels = pd.get_dummies(df['cat_num'][x])\n",
    "labels2 = pd.get_dummies(df['cat_num'][len(x):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and creating iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (trainInp, labels)\n",
    "val = (valInp, labels2)\n",
    "\n",
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# create validation Dataset and batch it\n",
    "val_data = tf.data.Dataset.from_tensor_slices(val)\n",
    "val_data = val_data.shuffle(10000) # if you want to shuffle your data\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "txt, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "val_init = iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.nn.embedding_lookup(embed, txt, partition_strategy='mod', name=None)\n",
    "embedded_chars_expanded = tf.expand_dims(embedding, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does enumerate do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 3)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [0,3,4]\n",
    "for i in enumerate(filter_sizes):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed attempt using predefined filter\n",
    "Update: Works now, not using tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWINDOW_SIZE = 100\\nSTRIDE = int(WINDOW_SIZE/2)\\n#embedding2 = tf.expand_dims(embedding, axis = 1)\\nconv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \\n               strides=1, padding='SAME') \\nconv = tf.nn.relu(conv)   \\nwords = flatten(conv)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "WINDOW_SIZE = 100\n",
    "STRIDE = int(WINDOW_SIZE/2)\n",
    "#embedding2 = tf.expand_dims(embedding, axis = 1)\n",
    "conv = tf.layers.conv2d(embedded_chars_expanded, 2, [2,WINDOW_SIZE], \n",
    "               strides=1, padding='SAME') \n",
    "conv = tf.nn.relu(conv)   \n",
    "words = flatten(conv)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 26, 100) dtype=float64>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 26, 100, 1) dtype=float64>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing looped convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "filter_sizes = [2,3,4,5]\n",
    "embedding_size = 100\n",
    "num_filters = 64\n",
    "max_length = 26\n",
    "for filter_size in filter_sizes:\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name = 'b')\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        tf.cast(W,tf.float64),\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, tf.cast(b,tf.float64)), name=\"relu\")\n",
    "    pooled = tf.nn.max_pool(\n",
    "        relu,\n",
    "        ksize=[1, max_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pool:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_1:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_2:0' shape=(?, 1, 1, 64) dtype=float64>,\n",
       " <tf.Tensor 'pool_3:0' shape=(?, 1, 1, 64) dtype=float64>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining separate convolutional layers into 1 feed forward input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "combined = tf.concat(pooled_outputs, 3)\n",
    "combined_flat = tf.reshape(combined, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 256) dtype=float64>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tf.layers.dense(combined_flat, 100, activation = 'relu')\n",
    "conn2 = tf.layers.dense(conn, len(set(df.cat_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Relu:0' shape=(?, 100) dtype=float64>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross entropy, loss, and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-8cbb1315fd70>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = conn2)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(conn2)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 3.118664245119096\n",
      "Accuracy 0: 0.359375\n",
      "Average val loss epoch 0: 2.043885842384472\n",
      "Val_Accuracy 0: 0.4654255319148936\n",
      "Average loss epoch 1: 1.7088354852774321\n",
      "Accuracy 1: 0.47961309523809526\n",
      "Average val loss epoch 1: 1.8009645185219545\n",
      "Val_Accuracy 1: 0.4601063829787234\n",
      "Average loss epoch 2: 1.4524899560169464\n",
      "Accuracy 2: 0.5269345238095238\n",
      "Average val loss epoch 2: 2.4353898536801566\n",
      "Val_Accuracy 2: 0.48138297872340424\n",
      "Average loss epoch 3: 1.2880040743546988\n",
      "Accuracy 3: 0.5671130952380953\n",
      "Average val loss epoch 3: 1.5661094941794618\n",
      "Val_Accuracy 3: 0.5106382978723404\n",
      "Average loss epoch 4: 1.1893291239231198\n",
      "Accuracy 4: 0.5869047619047619\n",
      "Average val loss epoch 4: 1.8681147581604634\n",
      "Val_Accuracy 4: 0.40558510638297873\n",
      "Average loss epoch 5: 1.053281803121968\n",
      "Accuracy 5: 0.6291666666666667\n",
      "Average val loss epoch 5: 1.4070641662613803\n",
      "Val_Accuracy 5: 0.5212765957446809\n",
      "Average loss epoch 6: 0.9791611565878499\n",
      "Accuracy 6: 0.6498511904761904\n",
      "Average val loss epoch 6: 1.5336857229731506\n",
      "Val_Accuracy 6: 0.5452127659574468\n",
      "Average loss epoch 7: 0.9121652578877909\n",
      "Accuracy 7: 0.6700892857142857\n",
      "Average val loss epoch 7: 1.4389891408432602\n",
      "Val_Accuracy 7: 0.5797872340425532\n",
      "Average loss epoch 8: 0.8684333149527348\n",
      "Accuracy 8: 0.6796130952380952\n",
      "Average val loss epoch 8: 1.695104127349295\n",
      "Val_Accuracy 8: 0.550531914893617\n",
      "Average loss epoch 9: 0.837657253188287\n",
      "Accuracy 9: 0.6979166666666666\n",
      "Average val loss epoch 9: 1.3293541487620884\n",
      "Val_Accuracy 9: 0.586436170212766\n",
      "Average loss epoch 10: 0.7877978517470443\n",
      "Accuracy 10: 0.7069940476190476\n",
      "Average val loss epoch 10: 1.3929751378889574\n",
      "Val_Accuracy 10: 0.5704787234042553\n",
      "Average loss epoch 11: 0.7554918729466557\n",
      "Accuracy 11: 0.7196428571428571\n",
      "Average val loss epoch 11: 1.3720562644080077\n",
      "Val_Accuracy 11: 0.5757978723404256\n",
      "Average loss epoch 12: 0.6836168043573433\n",
      "Accuracy 12: 0.7526785714285714\n",
      "Average val loss epoch 12: 1.5457015110234504\n",
      "Val_Accuracy 12: 0.5558510638297872\n",
      "Average loss epoch 13: 0.6876980979238709\n",
      "Accuracy 13: 0.752827380952381\n",
      "Average val loss epoch 13: 1.5861885030013014\n",
      "Val_Accuracy 13: 0.5079787234042553\n",
      "Average loss epoch 14: 0.6240385101920176\n",
      "Accuracy 14: 0.7706845238095238\n",
      "Average val loss epoch 14: 1.4146519603470307\n",
      "Val_Accuracy 14: 0.5771276595744681\n",
      "Average loss epoch 15: 0.602860111031992\n",
      "Accuracy 15: 0.7811011904761904\n",
      "Average val loss epoch 15: 1.4071376230517154\n",
      "Val_Accuracy 15: 0.5904255319148937\n",
      "Average loss epoch 16: 0.5533247273033768\n",
      "Accuracy 16: 0.8008928571428572\n",
      "Average val loss epoch 16: 1.3704801233384403\n",
      "Val_Accuracy 16: 0.601063829787234\n",
      "Average loss epoch 17: 0.5381860297248681\n",
      "Accuracy 17: 0.8081845238095238\n",
      "Average val loss epoch 17: 1.388815819777035\n",
      "Val_Accuracy 17: 0.5877659574468085\n",
      "Average loss epoch 18: 0.5141520440682054\n",
      "Accuracy 18: 0.8144345238095239\n",
      "Average val loss epoch 18: 1.316953428718981\n",
      "Val_Accuracy 18: 0.598404255319149\n",
      "Average loss epoch 19: 0.48299795369841847\n",
      "Accuracy 19: 0.8319940476190476\n",
      "Average val loss epoch 19: 1.3825867797512685\n",
      "Val_Accuracy 19: 0.5837765957446809\n",
      "Average loss epoch 20: 0.45937055097915813\n",
      "Accuracy 20: 0.8336309523809524\n",
      "Average val loss epoch 20: 1.4299219002069599\n",
      "Val_Accuracy 20: 0.5957446808510638\n",
      "Average loss epoch 21: 0.4575877868045787\n",
      "Accuracy 21: 0.8361607142857143\n",
      "Average val loss epoch 21: 1.572584978463532\n",
      "Val_Accuracy 21: 0.589095744680851\n",
      "Average loss epoch 22: 0.41352512867914737\n",
      "Accuracy 22: 0.8546130952380953\n",
      "Average val loss epoch 22: 1.6203040453504038\n",
      "Val_Accuracy 22: 0.5757978723404256\n",
      "Average loss epoch 23: 0.4091534435476085\n",
      "Accuracy 23: 0.8550595238095238\n",
      "Average val loss epoch 23: 1.389289482666221\n",
      "Val_Accuracy 23: 0.589095744680851\n",
      "Average loss epoch 24: 0.37189775440210615\n",
      "Accuracy 24: 0.8766369047619048\n",
      "Average val loss epoch 24: 1.7022264253539519\n",
      "Val_Accuracy 24: 0.5811170212765957\n",
      "Average loss epoch 25: 0.35302076356951856\n",
      "Accuracy 25: 0.8791666666666667\n",
      "Average val loss epoch 25: 1.5006805197161244\n",
      "Val_Accuracy 25: 0.5691489361702128\n",
      "Average loss epoch 26: 0.3409679297902654\n",
      "Accuracy 26: 0.8852678571428572\n",
      "Average val loss epoch 26: 1.4676094165874025\n",
      "Val_Accuracy 26: 0.5997340425531915\n",
      "Average loss epoch 27: 0.35382790653977647\n",
      "Accuracy 27: 0.8761904761904762\n",
      "Average val loss epoch 27: 1.4696751218971047\n",
      "Val_Accuracy 27: 0.5997340425531915\n",
      "Average loss epoch 28: 0.3031122870891419\n",
      "Accuracy 28: 0.8995535714285714\n",
      "Average val loss epoch 28: 1.442569785944319\n",
      "Val_Accuracy 28: 0.5970744680851063\n",
      "Average loss epoch 29: 0.2848246514009466\n",
      "Accuracy 29: 0.9102678571428572\n",
      "Average val loss epoch 29: 1.5996790677680817\n",
      "Val_Accuracy 29: 0.5997340425531915\n",
      "Average loss epoch 30: 0.27572486284871794\n",
      "Accuracy 30: 0.9138392857142857\n",
      "Average val loss epoch 30: 1.5332991384490027\n",
      "Val_Accuracy 30: 0.6037234042553191\n",
      "Average loss epoch 31: 0.26564947301853803\n",
      "Accuracy 31: 0.9122023809523809\n",
      "Average val loss epoch 31: 1.5644074401578885\n",
      "Val_Accuracy 31: 0.5638297872340425\n",
      "Average loss epoch 32: 0.2468695033470077\n",
      "Accuracy 32: 0.9169642857142857\n",
      "Average val loss epoch 32: 1.59439773658533\n",
      "Val_Accuracy 32: 0.5970744680851063\n",
      "Average loss epoch 33: 0.22587888803118772\n",
      "Accuracy 33: 0.9294642857142857\n",
      "Average val loss epoch 33: 1.5517391221445085\n",
      "Val_Accuracy 33: 0.574468085106383\n",
      "Average loss epoch 34: 0.22339885602972234\n",
      "Accuracy 34: 0.9296130952380952\n",
      "Average val loss epoch 34: 1.5533345083029453\n",
      "Val_Accuracy 34: 0.5944148936170213\n",
      "Average loss epoch 35: 0.20155838531680892\n",
      "Accuracy 35: 0.9383928571428571\n",
      "Average val loss epoch 35: 1.6941846284792672\n",
      "Val_Accuracy 35: 0.589095744680851\n",
      "Average loss epoch 36: 0.19836184692298403\n",
      "Accuracy 36: 0.940327380952381\n",
      "Average val loss epoch 36: 1.6877996930126555\n",
      "Val_Accuracy 36: 0.5917553191489362\n",
      "Average loss epoch 37: 0.20159134774100673\n",
      "Accuracy 37: 0.937797619047619\n",
      "Average val loss epoch 37: 1.74059022776109\n",
      "Val_Accuracy 37: 0.5904255319148937\n",
      "Average loss epoch 38: 0.18696882463140205\n",
      "Accuracy 38: 0.9438988095238096\n",
      "Average val loss epoch 38: 2.0347869609346345\n",
      "Val_Accuracy 38: 0.5585106382978723\n",
      "Average loss epoch 39: 0.17861413678208812\n",
      "Accuracy 39: 0.9447916666666667\n",
      "Average val loss epoch 39: 1.8012822465854994\n",
      "Val_Accuracy 39: 0.5957446808510638\n",
      "Average loss epoch 40: 0.18046249881838153\n",
      "Accuracy 40: 0.944047619047619\n",
      "Average val loss epoch 40: 1.6816816147849727\n",
      "Val_Accuracy 40: 0.5997340425531915\n",
      "Average loss epoch 41: 0.16092050424122284\n",
      "Accuracy 41: 0.950297619047619\n",
      "Average val loss epoch 41: 1.7349848734737858\n",
      "Val_Accuracy 41: 0.5997340425531915\n",
      "Average loss epoch 42: 0.14786459056808232\n",
      "Accuracy 42: 0.9608630952380952\n",
      "Average val loss epoch 42: 1.7662645854697943\n",
      "Val_Accuracy 42: 0.5970744680851063\n",
      "Average loss epoch 43: 0.1530431202008834\n",
      "Accuracy 43: 0.9565476190476191\n",
      "Average val loss epoch 43: 1.8712609524380364\n",
      "Val_Accuracy 43: 0.6103723404255319\n",
      "Average loss epoch 44: 0.15068039377224415\n",
      "Accuracy 44: 0.9546130952380952\n",
      "Average val loss epoch 44: 1.851921243178867\n",
      "Val_Accuracy 44: 0.5930851063829787\n",
      "Average loss epoch 45: 0.13463720548083655\n",
      "Accuracy 45: 0.9614583333333333\n",
      "Average val loss epoch 45: 2.004430896676179\n",
      "Val_Accuracy 45: 0.5877659574468085\n",
      "Average loss epoch 46: 0.1384296920034223\n",
      "Accuracy 46: 0.9595238095238096\n",
      "Average val loss epoch 46: 1.8272965546292226\n",
      "Val_Accuracy 46: 0.5811170212765957\n",
      "Average loss epoch 47: 0.12480386131829244\n",
      "Accuracy 47: 0.9644345238095238\n",
      "Average val loss epoch 47: 1.850559637757998\n",
      "Val_Accuracy 47: 0.5877659574468085\n",
      "Average loss epoch 48: 0.13595864633192423\n",
      "Accuracy 48: 0.9581845238095238\n",
      "Average val loss epoch 48: 1.938086644064753\n",
      "Val_Accuracy 48: 0.5877659574468085\n",
      "Average loss epoch 49: 0.11952994437306604\n",
      "Accuracy 49: 0.965625\n",
      "Average val loss epoch 49: 1.8641898319000052\n",
      "Val_Accuracy 49: 0.586436170212766\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train the model n_epochs times\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        \n",
    "        sess.run(train_init)# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        total_right = 0\n",
    "        n_batches = 0\n",
    "        totalright = 0\n",
    "        totalvalright = 0\n",
    "        nvalbatches = 0\n",
    "        totalvalloss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                #summary,acc,_, l = sess.run([summary_op,accuracy,optimizer, loss]) #use with scalar summary\n",
    "                acc,_, l = sess.run([accuracy, optimizer, loss])                \n",
    "                total_loss += l\n",
    "                total_right += acc\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        sess.run(val_init)\n",
    "        try:\n",
    "            while True:\n",
    "                accval,valloss = sess.run([accuracy, loss])\n",
    "                totalvalright += accval\n",
    "                nvalbatches += 1\n",
    "                totalvalloss += valloss\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        print('Accuracy {0}: {1}'.format(i, total_right/n_batches))    \n",
    "        print('Average val loss epoch {0}: {1}'.format(i, totalvalloss/nvalbatches))\n",
    "        print('Val_Accuracy {0}: {1}'.format(i, totalvalright/nvalbatches)) \n",
    "    prediction = sess.run(preds, feed_dict={txt: valInp})\n",
    "    prediction = np.asarray(prediction)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5873655913978495"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(np.argmax(prediction, 1), labels2.idxmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fooling around with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n",
      "(9, 2)\n",
      "[[1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "f = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "#f = np.array([[1,2,3], [1,2,3]])\n",
    "#print(f)\n",
    "g = np.expand_dims(f, axis = 1)\n",
    "g = np.reshape(f, [-1,2])\n",
    "print(g)\n",
    "print(g.shape)\n",
    "print(np.squeeze(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
